{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6a260c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Connection Established\n",
      "     data_category data_source model_name travel_date  year quarter month  \\\n",
      "0           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "1           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "2           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "3           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "4           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "...            ...         ...        ...         ...   ...     ...   ...   \n",
      "2399      FORECAST       MODEL         NA   326202212  2022      Q1   MAR   \n",
      "2400      FORECAST       MODEL         NA   328202218  2022      Q1   MAR   \n",
      "2401      FORECAST       MODEL         NA   328202218  2022      Q1   MAR   \n",
      "2402      FORECAST       MODEL         NA   329202224  2022      Q1   MAR   \n",
      "2403      FORECAST       MODEL         NA   329202224  2022      Q1   MAR   \n",
      "\n",
      "     week       day  hour  ... source_wind source_humidity  \\\n",
      "0       1    SUNDAY     6  ...    0.45 MPS             30%   \n",
      "1       1    SUNDAY     6  ...    0.45 MPS             30%   \n",
      "2       1    SUNDAY     6  ...    0.45 MPS             30%   \n",
      "3       1    SUNDAY     6  ...    0.45 MPS             30%   \n",
      "4       1    SUNDAY     6  ...    0.45 MPS             30%   \n",
      "...   ...       ...   ...  ...         ...             ...   \n",
      "2399    4   TUESDAY    12  ...    0.45 MPS             30%   \n",
      "2400    4  THURSDAY    18  ...    0.45 MPS             30%   \n",
      "2401    4  THURSDAY    18  ...    0.45 MPS             30%   \n",
      "2402    4    FRIDAY    24  ...    0.45 MPS             30%   \n",
      "2403    4    FRIDAY    24  ...    0.45 MPS             30%   \n",
      "\n",
      "     source_precipitation destination_wind  destination_humidity  \\\n",
      "0                    RAIN         0.52 MPS                   20%   \n",
      "1                    RAIN         0.52 MPS                   20%   \n",
      "2                    RAIN         0.52 MPS                   20%   \n",
      "3                    RAIN         0.52 MPS                   20%   \n",
      "4                    RAIN         0.52 MPS                   20%   \n",
      "...                   ...              ...                   ...   \n",
      "2399                 RAIN         0.52 MPS                   20%   \n",
      "2400                 RAIN         0.52 MPS                   20%   \n",
      "2401                 RAIN         0.52 MPS                   20%   \n",
      "2402                 RAIN         0.52 MPS                   20%   \n",
      "2403                 RAIN         0.52 MPS                   20%   \n",
      "\n",
      "     destination_precipitation number_of_booking                date  \\\n",
      "0                      DRIZZLE              10.0 2021-01-03 06:00:00   \n",
      "1                      DRIZZLE               5.0 2021-01-03 06:00:00   \n",
      "2                      DRIZZLE              10.0 2021-01-03 06:00:00   \n",
      "3                      DRIZZLE              20.0 2021-01-03 06:00:00   \n",
      "4                      DRIZZLE              20.0 2021-01-03 06:00:00   \n",
      "...                        ...               ...                 ...   \n",
      "2399                   DRIZZLE               NaN 2022-03-26 12:00:00   \n",
      "2400                   DRIZZLE               NaN 2022-03-28 18:00:00   \n",
      "2401                   DRIZZLE               NaN 2022-03-28 18:00:00   \n",
      "2402                   DRIZZLE               NaN 2022-03-30 00:00:00   \n",
      "2403                   DRIZZLE               NaN 2022-03-30 00:00:00   \n",
      "\n",
      "     model_accuracy accuracy_probability  \n",
      "0                 0                   0%  \n",
      "1                 0                   0%  \n",
      "2                 0                   0%  \n",
      "3                 0                   0%  \n",
      "4                 0                   0%  \n",
      "...             ...                  ...  \n",
      "2399              0                   0%  \n",
      "2400              0                   0%  \n",
      "2401              0                   0%  \n",
      "2402              0                   0%  \n",
      "2403              0                   0%  \n",
      "\n",
      "[2404 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sqlalchemy import create_engine\n",
    "from sagemaker import get_execution_role\n",
    "np.random.seed(1)\n",
    "\n",
    "from redshift import RedshiftConnection,get_secret\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "secret = get_secret()\n",
    "username = secret['username']\n",
    "password = secret['password']\n",
    "engine = secret['engine']\n",
    "host = secret['host']\n",
    "port = secret['port']\n",
    "redshift = RedshiftConnection(username,password,engine,host,port)\n",
    "print(\"Redshift Connection Established\")\n",
    "#Truncate table with datasource as model\n",
    "data = RedshiftConnection.read_redshift_data_booking(redshift)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bb8f7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-03 06:00:00</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-03 06:00:00</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03 06:00:00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-03 06:00:00</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05 12:00:00</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2021-03-28 18:00:00</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2021-03-29 00:00:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2021-03-29 00:00:00</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2021-03-29 00:00:00</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2021-03-29 00:00:00</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  start  target\n",
       "0   2021-01-03 06:00:00      25\n",
       "1   2021-01-03 06:00:00      50\n",
       "2   2021-01-03 06:00:00     100\n",
       "3   2021-01-03 06:00:00     292\n",
       "4   2021-01-05 12:00:00      30\n",
       "..                  ...     ...\n",
       "187 2021-03-28 18:00:00     289\n",
       "188 2021-03-29 00:00:00      28\n",
       "189 2021-03-29 00:00:00      57\n",
       "190 2021-03-29 00:00:00      93\n",
       "191 2021-03-29 00:00:00     289\n",
       "\n",
       "[192 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data[['date', 'number_of_booking']]\n",
    "data2.columns = ['start', 'target']\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5f67929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-05 12:00:00</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-28 18:00:00</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-29 00:00:00</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-29 00:00:00</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-29 00:00:00</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-29 00:00:00</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     target\n",
       "start                      \n",
       "2021-01-03 06:00:00      25\n",
       "2021-01-03 06:00:00      50\n",
       "2021-01-03 06:00:00     100\n",
       "2021-01-03 06:00:00     292\n",
       "2021-01-05 12:00:00      30\n",
       "...                     ...\n",
       "2021-03-28 18:00:00     289\n",
       "2021-03-29 00:00:00      28\n",
       "2021-03-29 00:00:00      57\n",
       "2021-03-29 00:00:00      93\n",
       "2021-03-29 00:00:00     289\n",
       "\n",
       "[192 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.set_index('start', inplace=True)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5d2a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2021-01-03 06:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2021-03-08 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3ae27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ceaf8da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\":1609653600000,\"target\":25}\n",
      "{\"start\":1609653600000,\"target\":50}\n",
      "{\"start\":1609653600000,\"target\":100}\n",
      "{\"start\":1609653600000,\"target\":292}\n",
      "{\"start\":1609848000000,\"target\":30}\n",
      "{\"start\":1609848000000,\"target\":45}\n",
      "{\"start\":1609848000000,\"target\":150}\n",
      "{\"start\":1609848000000,\"target\":242}\n",
      "{\"start\":1610042400000,\"target\":24}\n",
      "{\"start\":1610042400000,\"target\":58}\n",
      "{\"start\":1610042400000,\"target\":125}\n",
      "{\"start\":1610042400000,\"target\":260}\n",
      "{\"start\":1610064000000,\"target\":25}\n",
      "{\"start\":1610064000000,\"target\":50}\n",
      "{\"start\":1610064000000,\"target\":100}\n",
      "{\"start\":1610064000000,\"target\":292}\n",
      "{\"start\":1610258400000,\"target\":25}\n",
      "{\"start\":1610258400000,\"target\":50}\n",
      "{\"start\":1610258400000,\"target\":100}\n",
      "{\"start\":1610258400000,\"target\":292}\n",
      "{\"start\":1610452800000,\"target\":30}\n",
      "{\"start\":1610452800000,\"target\":45}\n",
      "{\"start\":1610452800000,\"target\":150}\n",
      "{\"start\":1610452800000,\"target\":242}\n",
      "{\"start\":1610647200000,\"target\":26}\n",
      "{\"start\":1610647200000,\"target\":55}\n",
      "{\"start\":1610647200000,\"target\":126}\n",
      "{\"start\":1610647200000,\"target\":260}\n",
      "{\"start\":1610668800000,\"target\":25}\n",
      "{\"start\":1610668800000,\"target\":50}\n",
      "{\"start\":1610668800000,\"target\":100}\n",
      "{\"start\":1610668800000,\"target\":292}\n",
      "{\"start\":1610863200000,\"target\":25}\n",
      "{\"start\":1610863200000,\"target\":50}\n",
      "{\"start\":1610863200000,\"target\":100}\n",
      "{\"start\":1610863200000,\"target\":292}\n",
      "{\"start\":1611057600000,\"target\":30}\n",
      "{\"start\":1611057600000,\"target\":45}\n",
      "{\"start\":1611057600000,\"target\":150}\n",
      "{\"start\":1611057600000,\"target\":242}\n",
      "{\"start\":1611252000000,\"target\":27}\n",
      "{\"start\":1611252000000,\"target\":55}\n",
      "{\"start\":1611252000000,\"target\":125}\n",
      "{\"start\":1611252000000,\"target\":260}\n",
      "{\"start\":1611273600000,\"target\":25}\n",
      "{\"start\":1611273600000,\"target\":50}\n",
      "{\"start\":1611273600000,\"target\":100}\n",
      "{\"start\":1611273600000,\"target\":292}\n",
      "{\"start\":1611468000000,\"target\":25}\n",
      "{\"start\":1611468000000,\"target\":50}\n",
      "{\"start\":1611468000000,\"target\":100}\n",
      "{\"start\":1611468000000,\"target\":292}\n",
      "{\"start\":1611662400000,\"target\":30}\n",
      "{\"start\":1611662400000,\"target\":45}\n",
      "{\"start\":1611662400000,\"target\":150}\n",
      "{\"start\":1611662400000,\"target\":242}\n",
      "{\"start\":1611856800000,\"target\":27}\n",
      "{\"start\":1611856800000,\"target\":55}\n",
      "{\"start\":1611856800000,\"target\":125}\n",
      "{\"start\":1611856800000,\"target\":260}\n",
      "{\"start\":1611878400000,\"target\":25}\n",
      "{\"start\":1611878400000,\"target\":50}\n",
      "{\"start\":1611878400000,\"target\":100}\n",
      "{\"start\":1611878400000,\"target\":292}\n",
      "{\"start\":1612332000000,\"target\":25}\n",
      "{\"start\":1612332000000,\"target\":48}\n",
      "{\"start\":1612332000000,\"target\":101}\n",
      "{\"start\":1612332000000,\"target\":293}\n",
      "{\"start\":1612526400000,\"target\":25}\n",
      "{\"start\":1612526400000,\"target\":51}\n",
      "{\"start\":1612526400000,\"target\":102}\n",
      "{\"start\":1612526400000,\"target\":289}\n",
      "{\"start\":1612720800000,\"target\":23}\n",
      "{\"start\":1612720800000,\"target\":53}\n",
      "{\"start\":1612720800000,\"target\":108}\n",
      "{\"start\":1612720800000,\"target\":283}\n",
      "{\"start\":1612742400000,\"target\":26}\n",
      "{\"start\":1612742400000,\"target\":53}\n",
      "{\"start\":1612742400000,\"target\":93}\n",
      "{\"start\":1612742400000,\"target\":295}\n",
      "{\"start\":1612936800000,\"target\":23}\n",
      "{\"start\":1612936800000,\"target\":56}\n",
      "{\"start\":1612936800000,\"target\":100}\n",
      "{\"start\":1612936800000,\"target\":288}\n",
      "{\"start\":1613131200000,\"target\":29}\n",
      "{\"start\":1613131200000,\"target\":53}\n",
      "{\"start\":1613131200000,\"target\":100}\n",
      "{\"start\":1613131200000,\"target\":285}\n",
      "{\"start\":1613325600000,\"target\":24}\n",
      "{\"start\":1613325600000,\"target\":55}\n",
      "{\"start\":1613325600000,\"target\":102}\n",
      "{\"start\":1613325600000,\"target\":286}\n",
      "{\"start\":1613347200000,\"target\":23}\n",
      "{\"start\":1613347200000,\"target\":54}\n",
      "{\"start\":1613347200000,\"target\":99}\n",
      "{\"start\":1613347200000,\"target\":291}\n",
      "{\"start\":1613541600000,\"target\":26}\n",
      "{\"start\":1613541600000,\"target\":58}\n",
      "{\"start\":1613541600000,\"target\":101}\n",
      "{\"start\":1613541600000,\"target\":282}\n",
      "{\"start\":1613736000000,\"target\":27}\n",
      "{\"start\":1613736000000,\"target\":58}\n",
      "{\"start\":1613736000000,\"target\":99}\n",
      "{\"start\":1613736000000,\"target\":283}\n",
      "{\"start\":1613930400000,\"target\":23}\n",
      "{\"start\":1613930400000,\"target\":55}\n",
      "{\"start\":1613930400000,\"target\":103}\n",
      "{\"start\":1613930400000,\"target\":286}\n",
      "{\"start\":1613952000000,\"target\":22}\n",
      "{\"start\":1613952000000,\"target\":57}\n",
      "{\"start\":1613952000000,\"target\":104}\n",
      "{\"start\":1613952000000,\"target\":284}\n",
      "{\"start\":1614146400000,\"target\":25}\n",
      "{\"start\":1614146400000,\"target\":57}\n",
      "{\"start\":1614146400000,\"target\":101}\n",
      "{\"start\":1614146400000,\"target\":284}\n",
      "{\"start\":1614340800000,\"target\":27}\n",
      "{\"start\":1614340800000,\"target\":58}\n",
      "{\"start\":1614340800000,\"target\":95}\n",
      "{\"start\":1614340800000,\"target\":287}\n",
      "{\"start\":1614535200000,\"target\":26}\n",
      "{\"start\":1614535200000,\"target\":53}\n",
      "{\"start\":1614535200000,\"target\":103}\n",
      "{\"start\":1614535200000,\"target\":285}\n",
      "{\"start\":1614556800000,\"target\":30}\n",
      "{\"start\":1614556800000,\"target\":47}\n",
      "{\"start\":1614556800000,\"target\":103}\n",
      "{\"start\":1614556800000,\"target\":287}\n",
      "{\"start\":1614751200000,\"target\":25}\n",
      "{\"start\":1614751200000,\"target\":58}\n",
      "{\"start\":1614751200000,\"target\":95}\n",
      "{\"start\":1614751200000,\"target\":289}\n",
      "{\"start\":1614945600000,\"target\":32}\n",
      "{\"start\":1614945600000,\"target\":49}\n",
      "{\"start\":1614945600000,\"target\":100}\n",
      "{\"start\":1614945600000,\"target\":286}\n",
      "{\"start\":1615140000000,\"target\":28}\n",
      "{\"start\":1615140000000,\"target\":57}\n",
      "{\"start\":1615140000000,\"target\":95}\n",
      "{\"start\":1615140000000,\"target\":287}\n",
      "{\"start\":1615161600000,\"target\":28}\n",
      "{\"start\":1615161600000,\"target\":52}\n",
      "{\"start\":1615161600000,\"target\":101}\n",
      "{\"start\":1615161600000,\"target\":286}\n",
      "{\"start\":1615356000000,\"target\":28}\n",
      "{\"start\":1615356000000,\"target\":56}\n",
      "{\"start\":1615356000000,\"target\":99}\n",
      "{\"start\":1615356000000,\"target\":284}\n",
      "{\"start\":1615550400000,\"target\":29}\n",
      "{\"start\":1615550400000,\"target\":57}\n",
      "{\"start\":1615550400000,\"target\":98}\n",
      "{\"start\":1615550400000,\"target\":283}\n",
      "{\"start\":1615744800000,\"target\":27}\n",
      "{\"start\":1615744800000,\"target\":56}\n",
      "{\"start\":1615744800000,\"target\":97}\n",
      "{\"start\":1615744800000,\"target\":287}\n",
      "{\"start\":1615766400000,\"target\":29}\n",
      "{\"start\":1615766400000,\"target\":54}\n",
      "{\"start\":1615766400000,\"target\":92}\n",
      "{\"start\":1615766400000,\"target\":292}\n",
      "{\"start\":1615960800000,\"target\":29}\n",
      "{\"start\":1615960800000,\"target\":54}\n",
      "{\"start\":1615960800000,\"target\":98}\n",
      "{\"start\":1615960800000,\"target\":286}\n",
      "{\"start\":1616155200000,\"target\":30}\n",
      "{\"start\":1616155200000,\"target\":58}\n",
      "{\"start\":1616155200000,\"target\":97}\n",
      "{\"start\":1616155200000,\"target\":282}\n",
      "{\"start\":1616349600000,\"target\":29}\n",
      "{\"start\":1616349600000,\"target\":57}\n",
      "{\"start\":1616349600000,\"target\":99}\n",
      "{\"start\":1616349600000,\"target\":282}\n",
      "{\"start\":1616371200000,\"target\":20}\n",
      "{\"start\":1616371200000,\"target\":56}\n",
      "{\"start\":1616371200000,\"target\":102}\n",
      "{\"start\":1616371200000,\"target\":289}\n",
      "{\"start\":1616565600000,\"target\":28}\n",
      "{\"start\":1616565600000,\"target\":56}\n",
      "{\"start\":1616565600000,\"target\":93}\n",
      "{\"start\":1616565600000,\"target\":290}\n",
      "{\"start\":1616760000000,\"target\":27}\n",
      "{\"start\":1616760000000,\"target\":58}\n",
      "{\"start\":1616760000000,\"target\":96}\n",
      "{\"start\":1616760000000,\"target\":286}\n",
      "{\"start\":1616954400000,\"target\":28}\n",
      "{\"start\":1616954400000,\"target\":57}\n",
      "{\"start\":1616954400000,\"target\":93}\n",
      "{\"start\":1616954400000,\"target\":289}\n",
      "{\"start\":1616976000000,\"target\":28}\n",
      "{\"start\":1616976000000,\"target\":57}\n",
      "{\"start\":1616976000000,\"target\":93}\n",
      "{\"start\":1616976000000,\"target\":289}\n"
     ]
    }
   ],
   "source": [
    "train_data = data2[:len(data2)-50]\n",
    "test_data = data2[len(data2)-50:]\n",
    "\n",
    "train_json = train_data.to_json(orient='records', lines=True)\n",
    "print(train_json)\n",
    "test_json = test_data.to_json(orient='records', lines=True)\n",
    "print(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b41be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image_uri = get_image_uri(boto3.Session().region_name, \"forecasting-deepar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98d8868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "s3_output_path = \"airline-test/output/DeepAR_output/\"\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "region = session.boto_region_name\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=session,\n",
    "    image_uri=image_uri,\n",
    "    image_name=sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\"),\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-poc1',\n",
    "    output_path=f\"s3://{s3_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e24c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"H\"\n",
    "prediction_length = 48\n",
    "context_length = 72\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ce4c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"airline-test\"\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "FILE_TRAIN = \"train.json\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(FILE_TRAIN, bucket,\"/data/train/\" + FILE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(FILE_TRAIN, bucket,\"/data/train/\" + FILE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78efa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_data_path = \"airline-test/train-data.jsonl\"\n",
    "\n",
    "data_channels = {\"train\": f\"s3://{s3_data_path}/train/\", \"test\": f\"s3://{s3_data_path}/test/\"}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbc9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aab18aa2",
   "metadata": {},
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd65d2",
   "metadata": {},
   "source": [
    "*************************First Approach****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbfdc2",
   "metadata": {},
   "source": [
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e071b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7176a228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>flight</th>\n",
       "      <th>capacity</th>\n",
       "      <th>...</th>\n",
       "      <th>location_economical_status</th>\n",
       "      <th>location_employment_status</th>\n",
       "      <th>location_event</th>\n",
       "      <th>source_wind</th>\n",
       "      <th>source_humidity</th>\n",
       "      <th>source_precipitation</th>\n",
       "      <th>destination_wind</th>\n",
       "      <th>destination_humidity</th>\n",
       "      <th>destination_precipitation</th>\n",
       "      <th>number_of_booking</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-26 12:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-28 18:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-28 18:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>24</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>24</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2404 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     year quarter month week       day  hour origin  \\\n",
       "date                                                                  \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "...                   ...     ...   ...  ...       ...   ...    ...   \n",
       "2022-03-26 12:00:00  2022      Q1   MAR    4   TUESDAY    12    MAA   \n",
       "2022-03-28 18:00:00  2022      Q1   MAR    4  THURSDAY    18    MAA   \n",
       "2022-03-28 18:00:00  2022      Q1   MAR    4  THURSDAY    18    MAA   \n",
       "2022-03-30 00:00:00  2022      Q1   MAR    4    FRIDAY    24    MAA   \n",
       "2022-03-30 00:00:00  2022      Q1   MAR    4    FRIDAY    24    MAA   \n",
       "\n",
       "                    destination flight  capacity  ...  \\\n",
       "date                                              ...   \n",
       "2021-01-03 06:00:00         DXB  FD101       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD101       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD102       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD102       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD101       467  ...   \n",
       "...                         ...    ...       ...  ...   \n",
       "2022-03-26 12:00:00         DXB  FD102       467  ...   \n",
       "2022-03-28 18:00:00         DXB  FD101       467  ...   \n",
       "2022-03-28 18:00:00         DXB  FD102       467  ...   \n",
       "2022-03-30 00:00:00         DXB  FD102       467  ...   \n",
       "2022-03-30 00:00:00         DXB  FD102       467  ...   \n",
       "\n",
       "                    location_economical_status location_employment_status  \\\n",
       "date                                                                        \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "...                                        ...                        ...   \n",
       "2022-03-26 12:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-28 18:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-28 18:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-30 00:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-30 00:00:00                       HIGH                   EMPLOYED   \n",
       "\n",
       "                    location_event source_wind source_humidity  \\\n",
       "date                                                             \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "...                            ...         ...             ...   \n",
       "2022-03-26 12:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-28 18:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-28 18:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-30 00:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-30 00:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "\n",
       "                    source_precipitation destination_wind  \\\n",
       "date                                                        \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "...                                  ...              ...   \n",
       "2022-03-26 12:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-28 18:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-28 18:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-30 00:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-30 00:00:00                 RAIN         0.52 MPS   \n",
       "\n",
       "                    destination_humidity destination_precipitation  \\\n",
       "date                                                                 \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "...                                  ...                       ...   \n",
       "2022-03-26 12:00:00                  20%                   DRIZZLE   \n",
       "2022-03-28 18:00:00                  20%                   DRIZZLE   \n",
       "2022-03-28 18:00:00                  20%                   DRIZZLE   \n",
       "2022-03-30 00:00:00                  20%                   DRIZZLE   \n",
       "2022-03-30 00:00:00                  20%                   DRIZZLE   \n",
       "\n",
       "                    number_of_booking  \n",
       "date                                   \n",
       "2021-01-03 06:00:00              10.0  \n",
       "2021-01-03 06:00:00               5.0  \n",
       "2021-01-03 06:00:00              10.0  \n",
       "2021-01-03 06:00:00              20.0  \n",
       "2021-01-03 06:00:00              20.0  \n",
       "...                               ...  \n",
       "2022-03-26 12:00:00               NaN  \n",
       "2022-03-28 18:00:00               NaN  \n",
       "2022-03-28 18:00:00               NaN  \n",
       "2022-03-30 00:00:00               NaN  \n",
       "2022-03-30 00:00:00               NaN  \n",
       "\n",
       "[2404 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data.set_index('date')\n",
    "data1 = data1[['year', 'quarter',\n",
    "       'month', 'week', 'day', 'hour', 'origin', 'destination',\n",
    "       'flight', 'capacity', 'price_type', 'promotion', 'roundtrip_or_oneway',\n",
    "       'customer_type', 'product_type', 'location_lifestyle',\n",
    "       'location_economical_status', 'location_employment_status',\n",
    "       'location_event', 'source_wind', 'source_humidity',\n",
    "       'source_precipitation', 'destination_wind', 'destination_humidity',\n",
    "       'destination_precipitation', 'number_of_booking']]\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2422b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                          object\n",
       "quarter                       object\n",
       "month                         object\n",
       "week                          object\n",
       "day                           object\n",
       "hour                           int64\n",
       "origin                        object\n",
       "destination                   object\n",
       "flight                        object\n",
       "capacity                       int64\n",
       "price_type                    object\n",
       "promotion                     object\n",
       "roundtrip_or_oneway           object\n",
       "customer_type                 object\n",
       "product_type                  object\n",
       "location_lifestyle            object\n",
       "location_economical_status    object\n",
       "location_employment_status    object\n",
       "location_event                object\n",
       "source_wind                   object\n",
       "source_humidity               object\n",
       "source_precipitation          object\n",
       "destination_wind              object\n",
       "destination_humidity          object\n",
       "destination_precipitation     object\n",
       "number_of_booking              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['year'] = data1['year'].astype(int)\n",
    "data1['quarter'] = data1['quarter'].astype('category')\n",
    "data1['month'] = data1['month'].astype('category')\n",
    "data1['week'] = data1['week'].astype(int)\n",
    "data1['day'] = data1['day'].astype('category')\n",
    "data1['origin'] = data1['origin'].astype('category')\n",
    "data1['destination'] = data1['destination'].astype('category')\n",
    "data1['flight'] = data1['flight'].astype('category')\n",
    "data1['price_type'] = data1['price_type'].astype('category')\n",
    "data1['promotion'] = data1['promotion'].astype('category')\n",
    "data1['roundtrip_or_oneway'] = data1['roundtrip_or_oneway'].astype('category')\n",
    "data1['customer_type'] = data1['customer_type'].astype('category')\n",
    "data1['product_type'] = data1['product_type'].astype('category')\n",
    "data1['location_lifestyle'] = data1['location_lifestyle'].astype('category')\n",
    "data1['location_economical_status'] = data1['location_economical_status'].astype('category')\n",
    "data1['location_employment_status'] = data1['location_employment_status'].astype('category')\n",
    "data1['location_event'] = data1['location_event'].astype('category')\n",
    "data1['day'] = data1['day'].astype('category')\n",
    "cat_columns = df.select_dtypes(['category']).columns\n",
    "df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307425fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.45', 'MPS']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['source_wind'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267d4d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['data_category', 'data_source', 'model_name', 'year', 'quarter',\n",
       "       'month', 'week', 'day', 'hour', 'region', 'origin', 'destination',\n",
       "       'flight', 'capacity', 'price_type', 'promotion', 'roundtrip_or_oneway',\n",
       "       'customer_type', 'product_type', 'location_lifestyle',\n",
       "       'location_economical_status', 'location_employment_status',\n",
       "       'location_event', 'source_wind', 'source_humidity',\n",
       "       'source_precipitation', 'destination_wind', 'destination_humidity',\n",
       "       'destination_precipitation', 'number_of_booking'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518814d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7e949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "878a6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0b12047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8120beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "266778c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"deepar-demo-notebook\"  # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79dedbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6dc521b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOST = \"sagemaker-sample-files\"\n",
    "DATA_PATH = \"datasets/timeseries/uci_electricity/\"\n",
    "ARCHIVE_NAME = \"LD2011_2014.txt.zip\"\n",
    "FILE_NAME = ARCHIVE_NAME[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "if not os.path.isfile(FILE_NAME):\n",
    "    print(\"downloading dataset (258MB), can take a few minutes depending on your connection\")\n",
    "    s3_client.download_file(DATA_HOST, DATA_PATH + ARCHIVE_NAME, ARCHIVE_NAME)\n",
    "\n",
    "    print(\"\\nextracting data archive\")\n",
    "    zip_ref = zipfile.ZipFile(ARCHIVE_NAME, \"r\")\n",
    "    zip_ref.extractall(\"./\")\n",
    "    zip_ref.close()\n",
    "else:\n",
    "    print(\"File found skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f482b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4115f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5873b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6f80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa41d901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Connection Established\n",
      "     data_category data_source model_name travel_date  year quarter month  \\\n",
      "0           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "1           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "2           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "3           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "4           ACTUAL         ETL         NA   103202106  2021      Q1   JAN   \n",
      "...            ...         ...        ...         ...   ...     ...   ...   \n",
      "2299      FORECAST       MODEL         NA   329202224  2022      Q1   MAR   \n",
      "2300      FORECAST       MODEL         NA   329202224  2022      Q1   MAR   \n",
      "2301      FORECAST       MODEL         NA   329202224  2022      Q1   MAR   \n",
      "2302      FORECAST       MODEL         NA   329202224  2022      Q1   MAR   \n",
      "2303      FORECAST       MODEL         NA   329202224  2022      Q1   MAR   \n",
      "\n",
      "     week     day  hour  ... source_wind source_humidity source_precipitation  \\\n",
      "0       1  SUNDAY     6  ...    0.45 MPS             30%                 RAIN   \n",
      "1       1  SUNDAY     6  ...    0.45 MPS             30%                 RAIN   \n",
      "2       1  SUNDAY     6  ...    0.45 MPS             30%                 RAIN   \n",
      "3       1  SUNDAY     6  ...    0.45 MPS             30%                 RAIN   \n",
      "4       1  SUNDAY     6  ...    0.45 MPS             30%                 RAIN   \n",
      "...   ...     ...   ...  ...         ...             ...                  ...   \n",
      "2299    4  FRIDAY    24  ...    0.45 MPS             30%                 RAIN   \n",
      "2300    4  FRIDAY    24  ...    0.45 MPS             30%                 RAIN   \n",
      "2301    4  FRIDAY    24  ...    0.45 MPS             30%                 RAIN   \n",
      "2302    4  FRIDAY    24  ...    0.45 MPS             30%                 RAIN   \n",
      "2303    4  FRIDAY    24  ...    0.45 MPS             30%                 RAIN   \n",
      "\n",
      "     destination_wind  destination_humidity destination_precipitation  \\\n",
      "0            0.52 MPS                   20%                   DRIZZLE   \n",
      "1            0.52 MPS                   20%                   DRIZZLE   \n",
      "2            0.52 MPS                   20%                   DRIZZLE   \n",
      "3            0.52 MPS                   20%                   DRIZZLE   \n",
      "4            0.52 MPS                   20%                   DRIZZLE   \n",
      "...               ...                   ...                       ...   \n",
      "2299         0.52 MPS                   20%                   DRIZZLE   \n",
      "2300         0.52 MPS                   20%                   DRIZZLE   \n",
      "2301         0.52 MPS                   20%                   DRIZZLE   \n",
      "2302         0.52 MPS                   20%                   DRIZZLE   \n",
      "2303         0.52 MPS                   20%                   DRIZZLE   \n",
      "\n",
      "     number_of_booking                date model_accuracy accuracy_probability  \n",
      "0                   10 2021-01-03 06:00:00              0                   0%  \n",
      "1                    5 2021-01-03 06:00:00              0                   0%  \n",
      "2                   10 2021-01-03 06:00:00              0                   0%  \n",
      "3                   20 2021-01-03 06:00:00              0                   0%  \n",
      "4                   20 2021-01-03 06:00:00              0                   0%  \n",
      "...                ...                 ...            ...                  ...  \n",
      "2299                10 2022-03-30 00:00:00              0                   0%  \n",
      "2300                18 2022-03-30 00:00:00              0                   0%  \n",
      "2301                19 2022-03-30 00:00:00              0                   0%  \n",
      "2302                60 2022-03-30 00:00:00              0                   0%  \n",
      "2303                50 2022-03-30 00:00:00              0                   0%  \n",
      "\n",
      "[2304 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sqlalchemy import create_engine\n",
    "from sagemaker import get_execution_role\n",
    "np.random.seed(1)\n",
    "\n",
    "from redshift import RedshiftConnection,get_secret\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "secret = get_secret()\n",
    "username = secret['username']\n",
    "password = secret['password']\n",
    "engine = secret['engine']\n",
    "host = secret['host']\n",
    "port = secret['port']\n",
    "redshift = RedshiftConnection(username,password,engine,host,port)\n",
    "print(\"Redshift Connection Established\")\n",
    "#Truncate table with datasource as model\n",
    "data = RedshiftConnection.read_redshift_data_booking(redshift)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0ff811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['data_category', 'data_source', 'model_name', 'travel_date', 'year',\n",
       "       'quarter', 'month', 'week', 'day', 'hour', 'region', 'origin',\n",
       "       'destination', 'flight', 'capacity', 'price_type', 'promotion',\n",
       "       'roundtrip_or_oneway', 'customer_type', 'product_type',\n",
       "       'location_lifestyle', 'location_economical_status',\n",
       "       'location_employment_status', 'location_event', 'source_wind',\n",
       "       'source_humidity', 'source_precipitation', 'destination_wind',\n",
       "       'destination_humidity', 'destination_precipitation',\n",
       "       'number_of_booking', 'date', 'model_accuracy', 'accuracy_probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100282df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9241eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a00809",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"deepar-demo-notebook\"  # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9730a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5bcb149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae78aa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['data_category', 'data_source', 'model_name', 'travel_date', 'year',\n",
       "       'quarter', 'month', 'week', 'day', 'hour', 'region', 'origin',\n",
       "       'destination', 'flight', 'capacity', 'price_type', 'promotion',\n",
       "       'roundtrip_or_oneway', 'customer_type', 'product_type',\n",
       "       'location_lifestyle', 'location_economical_status',\n",
       "       'location_employment_status', 'location_event', 'source_wind',\n",
       "       'source_humidity', 'source_precipitation', 'destination_wind',\n",
       "       'destination_humidity', 'destination_precipitation',\n",
       "       'number_of_booking', 'date', 'model_accuracy', 'accuracy_probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a930fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                          datetime64[ns]\n",
       "year                                  object\n",
       "quarter                               object\n",
       "month                                 object\n",
       "week                                  object\n",
       "day                                   object\n",
       "hour                                   int64\n",
       "origin                                object\n",
       "destination                           object\n",
       "flight                                object\n",
       "capacity                               int64\n",
       "price_type                            object\n",
       "promotion                             object\n",
       "roundtrip_or_oneway                   object\n",
       "customer_type                         object\n",
       "product_type                          object\n",
       "location_lifestyle                    object\n",
       "location_economical_status            object\n",
       "location_employment_status            object\n",
       "location_event                        object\n",
       "source_wind                           object\n",
       "source_humidity                       object\n",
       "source_precipitation                  object\n",
       "destination_wind                      object\n",
       "destination_humidity                  object\n",
       "destination_precipitation             object\n",
       "number_of_booking                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data[['date', 'year',\n",
    "       'quarter', 'month', 'week', 'day', 'hour', 'origin',\n",
    "       'destination', 'flight', 'capacity', 'price_type', 'promotion',\n",
    "       'roundtrip_or_oneway', 'customer_type', 'product_type',\n",
    "       'location_lifestyle', 'location_economical_status',\n",
    "       'location_employment_status', 'location_event', 'source_wind',\n",
    "       'source_humidity', 'source_precipitation', 'destination_wind',\n",
    "       'destination_humidity', 'destination_precipitation',\n",
    "       'number_of_booking']]\n",
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1f91159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date                          datetime64[ns]\n",
       "year                                   int64\n",
       "quarter                                 int8\n",
       "month                                   int8\n",
       "week                                   int64\n",
       "day                                     int8\n",
       "hour                                   int64\n",
       "origin                                  int8\n",
       "destination                             int8\n",
       "flight                                  int8\n",
       "capacity                               int64\n",
       "price_type                              int8\n",
       "promotion                               int8\n",
       "roundtrip_or_oneway                     int8\n",
       "customer_type                           int8\n",
       "product_type                            int8\n",
       "location_lifestyle                      int8\n",
       "location_economical_status              int8\n",
       "location_employment_status              int8\n",
       "location_event                          int8\n",
       "source_wind                             int8\n",
       "source_humidity                         int8\n",
       "source_precipitation                    int8\n",
       "destination_wind                        int8\n",
       "destination_humidity                    int8\n",
       "destination_precipitation               int8\n",
       "number_of_booking                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['year'] = data1['year'].astype(int)\n",
    "data1['quarter'] = data1['quarter'].astype('category')\n",
    "data1['month'] = data1['month'].astype('category')\n",
    "data1['week'] = data1['week'].astype(int)\n",
    "data1['day'] = data1['day'].astype('category')\n",
    "data1['origin'] = data1['origin'].astype('category')\n",
    "data1['destination'] = data1['destination'].astype('category')\n",
    "data1['flight'] = data1['flight'].astype('category')\n",
    "data1['price_type'] = data1['price_type'].astype('category')\n",
    "data1['promotion'] = data1['promotion'].astype('category')\n",
    "data1['roundtrip_or_oneway'] = data1['roundtrip_or_oneway'].astype('category')\n",
    "data1['customer_type'] = data1['customer_type'].astype('category')\n",
    "data1['product_type'] = data1['product_type'].astype('category')\n",
    "data1['location_lifestyle'] = data1['location_lifestyle'].astype('category')\n",
    "data1['location_economical_status'] = data1['location_economical_status'].astype('category')\n",
    "data1['location_employment_status'] = data1['location_employment_status'].astype('category')\n",
    "data1['location_event'] = data1['location_event'].astype('category')\n",
    "data1['source_wind'] = data1['source_wind'].astype('category')\n",
    "data1['source_humidity'] = data1['source_humidity'].astype('category')\n",
    "data1['source_precipitation'] = data1['source_precipitation'].astype('category')\n",
    "data1['destination_wind'] = data1['destination_wind'].astype('category')\n",
    "data1['destination_humidity'] = data1['destination_humidity'].astype('category')\n",
    "data1['destination_precipitation'] = data1['destination_precipitation'].astype('category')\n",
    "cat_columns = data1.select_dtypes(['category']).columns\n",
    "data1[cat_columns] = data1[cat_columns].apply(lambda x: x.cat.codes)\n",
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa97ec83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>flight</th>\n",
       "      <th>capacity</th>\n",
       "      <th>...</th>\n",
       "      <th>location_economical_status</th>\n",
       "      <th>location_employment_status</th>\n",
       "      <th>location_event</th>\n",
       "      <th>source_wind</th>\n",
       "      <th>source_humidity</th>\n",
       "      <th>source_precipitation</th>\n",
       "      <th>destination_wind</th>\n",
       "      <th>destination_humidity</th>\n",
       "      <th>destination_precipitation</th>\n",
       "      <th>number_of_booking</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2304 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     year  quarter  month  week  day  hour  origin  \\\n",
       "date                                                                 \n",
       "2021-01-03 06:00:00  2021        0      1     1    1     6       0   \n",
       "2021-01-03 06:00:00  2021        0      1     1    1     6       0   \n",
       "2021-01-03 06:00:00  2021        0      1     1    1     6       0   \n",
       "2021-01-03 06:00:00  2021        0      1     1    1     6       0   \n",
       "2021-01-03 06:00:00  2021        0      1     1    1     6       0   \n",
       "...                   ...      ...    ...   ...  ...   ...     ...   \n",
       "2022-03-30 00:00:00  2022        0      2     4    0    24       0   \n",
       "2022-03-30 00:00:00  2022        0      2     4    0    24       0   \n",
       "2022-03-30 00:00:00  2022        0      2     4    0    24       0   \n",
       "2022-03-30 00:00:00  2022        0      2     4    0    24       0   \n",
       "2022-03-30 00:00:00  2022        0      2     4    0    24       0   \n",
       "\n",
       "                     destination  flight  capacity  ...  \\\n",
       "date                                                ...   \n",
       "2021-01-03 06:00:00            0       0       467  ...   \n",
       "2021-01-03 06:00:00            0       0       467  ...   \n",
       "2021-01-03 06:00:00            0       1       467  ...   \n",
       "2021-01-03 06:00:00            0       1       467  ...   \n",
       "2021-01-03 06:00:00            0       0       467  ...   \n",
       "...                          ...     ...       ...  ...   \n",
       "2022-03-30 00:00:00            0       1       467  ...   \n",
       "2022-03-30 00:00:00            0       0       467  ...   \n",
       "2022-03-30 00:00:00            0       0       467  ...   \n",
       "2022-03-30 00:00:00            0       1       467  ...   \n",
       "2022-03-30 00:00:00            0       1       467  ...   \n",
       "\n",
       "                     location_economical_status  location_employment_status  \\\n",
       "date                                                                          \n",
       "2021-01-03 06:00:00                           0                           0   \n",
       "2021-01-03 06:00:00                           0                           0   \n",
       "2021-01-03 06:00:00                           0                           0   \n",
       "2021-01-03 06:00:00                           0                           0   \n",
       "2021-01-03 06:00:00                           0                           0   \n",
       "...                                         ...                         ...   \n",
       "2022-03-30 00:00:00                           0                           0   \n",
       "2022-03-30 00:00:00                           0                           0   \n",
       "2022-03-30 00:00:00                           0                           0   \n",
       "2022-03-30 00:00:00                           0                           0   \n",
       "2022-03-30 00:00:00                           0                           0   \n",
       "\n",
       "                     location_event  source_wind  source_humidity  \\\n",
       "date                                                                \n",
       "2021-01-03 06:00:00               0            0                0   \n",
       "2021-01-03 06:00:00               0            0                0   \n",
       "2021-01-03 06:00:00               0            0                0   \n",
       "2021-01-03 06:00:00               0            0                0   \n",
       "2021-01-03 06:00:00               0            0                0   \n",
       "...                             ...          ...              ...   \n",
       "2022-03-30 00:00:00               0            0                0   \n",
       "2022-03-30 00:00:00               0            0                0   \n",
       "2022-03-30 00:00:00               0            0                0   \n",
       "2022-03-30 00:00:00               0            0                0   \n",
       "2022-03-30 00:00:00               0            0                0   \n",
       "\n",
       "                     source_precipitation  destination_wind  \\\n",
       "date                                                          \n",
       "2021-01-03 06:00:00                     0                 0   \n",
       "2021-01-03 06:00:00                     0                 0   \n",
       "2021-01-03 06:00:00                     0                 0   \n",
       "2021-01-03 06:00:00                     0                 0   \n",
       "2021-01-03 06:00:00                     0                 0   \n",
       "...                                   ...               ...   \n",
       "2022-03-30 00:00:00                     0                 0   \n",
       "2022-03-30 00:00:00                     0                 0   \n",
       "2022-03-30 00:00:00                     0                 0   \n",
       "2022-03-30 00:00:00                     0                 0   \n",
       "2022-03-30 00:00:00                     0                 0   \n",
       "\n",
       "                     destination_humidity  destination_precipitation  \\\n",
       "date                                                                   \n",
       "2021-01-03 06:00:00                     0                          0   \n",
       "2021-01-03 06:00:00                     0                          0   \n",
       "2021-01-03 06:00:00                     0                          0   \n",
       "2021-01-03 06:00:00                     0                          0   \n",
       "2021-01-03 06:00:00                     0                          0   \n",
       "...                                   ...                        ...   \n",
       "2022-03-30 00:00:00                     0                          0   \n",
       "2022-03-30 00:00:00                     0                          0   \n",
       "2022-03-30 00:00:00                     0                          0   \n",
       "2022-03-30 00:00:00                     0                          0   \n",
       "2022-03-30 00:00:00                     0                          0   \n",
       "\n",
       "                     number_of_booking  \n",
       "date                                    \n",
       "2021-01-03 06:00:00                 10  \n",
       "2021-01-03 06:00:00                  5  \n",
       "2021-01-03 06:00:00                 10  \n",
       "2021-01-03 06:00:00                 20  \n",
       "2021-01-03 06:00:00                 20  \n",
       "...                                ...  \n",
       "2022-03-30 00:00:00                 10  \n",
       "2022-03-30 00:00:00                 18  \n",
       "2022-03-30 00:00:00                 19  \n",
       "2022-03-30 00:00:00                 60  \n",
       "2022-03-30 00:00:00                 50  \n",
       "\n",
       "[2304 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.set_index('date', inplace=True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6740a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = data1.shape[1]\n",
    "data_kw = data1.resample(\"H\").sum() / 8\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_kw.iloc[:, i], trim=\"f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ca939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = \"H\"\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 24\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97309912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2021-01-03T06:00:00.000000000', '2021-01-05T12:00:00.000000000',\n",
       "       '2021-01-07T18:00:00.000000000', '2021-01-09T00:00:00.000000000',\n",
       "       '2021-01-10T06:00:00.000000000', '2021-01-12T12:00:00.000000000',\n",
       "       '2021-01-14T18:00:00.000000000', '2021-01-16T00:00:00.000000000',\n",
       "       '2021-01-17T06:00:00.000000000', '2021-01-19T12:00:00.000000000',\n",
       "       '2021-01-21T18:00:00.000000000', '2021-01-23T00:00:00.000000000',\n",
       "       '2021-01-24T06:00:00.000000000', '2021-01-26T12:00:00.000000000',\n",
       "       '2021-01-28T18:00:00.000000000', '2021-01-30T00:00:00.000000000',\n",
       "       '2021-02-03T06:00:00.000000000', '2021-02-05T12:00:00.000000000',\n",
       "       '2021-02-07T18:00:00.000000000', '2021-02-09T00:00:00.000000000',\n",
       "       '2021-02-10T06:00:00.000000000', '2021-02-12T12:00:00.000000000',\n",
       "       '2021-02-14T18:00:00.000000000', '2021-02-16T00:00:00.000000000',\n",
       "       '2021-02-17T06:00:00.000000000', '2021-02-19T12:00:00.000000000',\n",
       "       '2021-02-21T18:00:00.000000000', '2021-02-23T00:00:00.000000000',\n",
       "       '2021-02-24T06:00:00.000000000', '2021-02-26T12:00:00.000000000',\n",
       "       '2021-02-28T18:00:00.000000000', '2021-03-02T00:00:00.000000000',\n",
       "       '2021-03-03T06:00:00.000000000', '2021-03-05T12:00:00.000000000',\n",
       "       '2021-03-07T18:00:00.000000000', '2021-03-09T00:00:00.000000000',\n",
       "       '2021-03-10T06:00:00.000000000', '2021-03-12T12:00:00.000000000',\n",
       "       '2021-03-14T18:00:00.000000000', '2021-03-16T00:00:00.000000000',\n",
       "       '2021-03-17T06:00:00.000000000', '2021-03-19T12:00:00.000000000',\n",
       "       '2021-03-21T18:00:00.000000000', '2021-03-23T00:00:00.000000000',\n",
       "       '2021-03-24T06:00:00.000000000', '2021-03-26T12:00:00.000000000',\n",
       "       '2021-03-28T18:00:00.000000000', '2021-03-30T00:00:00.000000000',\n",
       "       '2022-01-03T06:00:00.000000000', '2022-01-05T12:00:00.000000000',\n",
       "       '2022-01-07T18:00:00.000000000', '2022-01-09T00:00:00.000000000',\n",
       "       '2022-01-10T06:00:00.000000000', '2022-01-12T12:00:00.000000000',\n",
       "       '2022-01-14T18:00:00.000000000', '2022-01-16T00:00:00.000000000',\n",
       "       '2022-01-17T06:00:00.000000000', '2022-01-19T12:00:00.000000000',\n",
       "       '2022-01-21T18:00:00.000000000', '2022-01-23T00:00:00.000000000',\n",
       "       '2022-01-24T06:00:00.000000000', '2022-01-26T12:00:00.000000000',\n",
       "       '2022-01-28T18:00:00.000000000', '2022-01-30T00:00:00.000000000',\n",
       "       '2022-02-03T06:00:00.000000000', '2022-02-05T12:00:00.000000000',\n",
       "       '2022-02-07T18:00:00.000000000', '2022-02-09T00:00:00.000000000',\n",
       "       '2022-02-10T06:00:00.000000000', '2022-02-12T12:00:00.000000000',\n",
       "       '2022-02-14T18:00:00.000000000', '2022-02-16T00:00:00.000000000',\n",
       "       '2022-02-17T06:00:00.000000000', '2022-02-19T12:00:00.000000000',\n",
       "       '2022-02-21T18:00:00.000000000', '2022-02-23T00:00:00.000000000',\n",
       "       '2022-02-24T06:00:00.000000000', '2022-02-26T12:00:00.000000000',\n",
       "       '2022-02-28T18:00:00.000000000', '2022-03-02T00:00:00.000000000',\n",
       "       '2022-03-03T06:00:00.000000000', '2022-03-05T12:00:00.000000000',\n",
       "       '2022-03-07T18:00:00.000000000', '2022-03-09T00:00:00.000000000',\n",
       "       '2022-03-10T06:00:00.000000000', '2022-03-12T12:00:00.000000000',\n",
       "       '2022-03-14T18:00:00.000000000', '2022-03-16T00:00:00.000000000',\n",
       "       '2022-03-17T06:00:00.000000000', '2022-03-19T12:00:00.000000000',\n",
       "       '2022-03-21T18:00:00.000000000', '2022-03-23T00:00:00.000000000',\n",
       "       '2022-03-24T06:00:00.000000000', '2022-03-26T12:00:00.000000000',\n",
       "       '2022-03-28T18:00:00.000000000', '2022-03-30T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6520187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58ca03ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2021-01-03 06:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2021-03-30 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "853fe0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[\n",
    "            start_dataset : end_training - timedelta(days=1)\n",
    "        ].tolist(),  # We use -1, because pandas indexing includes the upper bound\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e94f5d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1)\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "977a6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data1):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data1:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4d7871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.7 ms, sys: 6.54 ms, total: 60.2 ms\n",
      "Wall time: 85.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2064f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data1:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c2ec932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-2-363247502029/deepar-demo-notebook/data/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-2-363247502029/deepar-demo-notebook/data/test/test.json\n",
      "CPU times: user 41.8 ms, sys: 7.34 ms, total: 49.2 ms\n",
      "Wall time: 221 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdd6845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2021-01-03 06:00:00\", \"target\": [4042.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", \"rb\") as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4a7087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.2xlarge\",\n",
    "    base_job_name=\"deepar-electricity-demo\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ddedf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3a35312",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ee8fc58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-30 06:38:03 Starting - Starting the training job...\n",
      "2021-12-30 06:38:27 Starting - Launching requested ML instancesProfilerReport-1640846283: InProgress\n",
      "...\n",
      "2021-12-30 06:39:00 Starting - Preparing the instances for training.........\n",
      "2021-12-30 06:40:28 Downloading - Downloading input data\n",
      "2021-12-30 06:40:28 Training - Downloading the training image.....\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '168', 'time_freq': 'H', 'context_length': '168', 'epochs': '400', 'learning_rate': '5E-4', 'early_stopping_patience': '40', 'mini_batch_size': '64'}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '168', 'time_freq': 'H', 'context_length': '168', 'epochs': '400'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] random_seed is None\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Training set statistics:\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Real time series\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] number of time series: 26\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] number of observations: 20350\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] mean target length: 782.6923076923077\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] min/mean/max target: 0.0/11.734871007371007/4042.0\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] mean abs(target): 11.734871007371007\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Small number of time series. Doing 25 passes over dataset with prob 0.9846153846153847 per epoch.\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Test set statistics:\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Real time series\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] number of time series: 104\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] number of observations: 378520\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] mean target length: 3639.6153846153848\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] min/mean/max target: 0.0/5.9886214731057805/8088.0\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] mean abs(target): 5.9886214731057805\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] #memory_usage::<batchbuffer> = 18.48388671875 mb\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] nvidia-smi took: 0.025301456451416016 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:09 INFO 140511108945280] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846469.5372922, \"EndTime\": 1640846470.840273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 1300.5092144012451, \"count\": 1, \"min\": 1300.5092144012451, \"max\": 1300.5092144012451}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:10 INFO 140511108945280] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:12 INFO 140511108945280] #memory_usage::<model> = 109 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846470.8403628, \"EndTime\": 1640846472.282531, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 2745.1255321502686, \"count\": 1, \"min\": 2745.1255321502686, \"max\": 2745.1255321502686}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:13 INFO 140511108945280] Epoch[0] Batch[0] avg_epoch_loss=-0.118814\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=-0.11881403625011444\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:15 INFO 140511108945280] Epoch[0] Batch[5] avg_epoch_loss=0.102772\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=0.10277203346292178\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:15 INFO 140511108945280] Epoch[0] Batch [5]#011Speed: 161.70 samples/sec#011loss=0.102772\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] Epoch[0] Batch[10] avg_epoch_loss=0.032796\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=-0.05117523819208145\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] Epoch[0] Batch [10]#011Speed: 147.71 samples/sec#011loss=-0.051175\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] processed a total of 736 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846472.282602, \"EndTime\": 1640846477.6734095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 5390.711069107056, \"count\": 1, \"min\": 5390.711069107056, \"max\": 5390.711069107056}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=136.5271560765953 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=0, train loss <loss>=0.25915826112031937\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:17 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_933b90c6-f7db-4ceb-8dbd-d9ac1acc52b0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846477.6735244, \"EndTime\": 1640846477.7737267, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 99.55692291259766, \"count\": 1, \"min\": 99.55692291259766, \"max\": 99.55692291259766}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:18 INFO 140511108945280] Epoch[1] Batch[0] avg_epoch_loss=-0.599932\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=-0.5999319553375244\u001b[0m\n",
      "\n",
      "2021-12-30 06:41:28 Training - Training image download completed. Training in progress.\u001b[34m[12/30/2021 06:41:20 INFO 140511108945280] Epoch[1] Batch[5] avg_epoch_loss=-0.018966\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=-0.018965885353585083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:20 INFO 140511108945280] Epoch[1] Batch [5]#011Speed: 169.44 samples/sec#011loss=-0.018966\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] Epoch[1] Batch[10] avg_epoch_loss=-0.104491\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=-0.20712085589766502\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] Epoch[1] Batch [10]#011Speed: 157.66 samples/sec#011loss=-0.207121\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846477.7738168, \"EndTime\": 1640846482.6842809, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4910.39514541626, \"count\": 1, \"min\": 4910.39514541626, \"max\": 4910.39514541626}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.4382217238362 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=1, train loss <loss>=0.12722692281628648\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:22 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_d1d2e8e1-5a32-46c8-9258-413dc3c712ca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846482.6843777, \"EndTime\": 1640846482.847098, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 162.18948364257812, \"count\": 1, \"min\": 162.18948364257812, \"max\": 162.18948364257812}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:23 INFO 140511108945280] Epoch[2] Batch[0] avg_epoch_loss=-0.670853\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=-0.670853316783905\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:25 INFO 140511108945280] Epoch[2] Batch[5] avg_epoch_loss=-0.694447\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=-0.694447303811709\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:25 INFO 140511108945280] Epoch[2] Batch [5]#011Speed: 171.64 samples/sec#011loss=-0.694447\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] Epoch[2] Batch[10] avg_epoch_loss=-0.803857\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=-0.9351482629776001\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] Epoch[2] Batch [10]#011Speed: 163.54 samples/sec#011loss=-0.935148\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] processed a total of 758 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846482.8471856, \"EndTime\": 1640846487.691917, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4844.659805297852, \"count\": 1, \"min\": 4844.659805297852, \"max\": 4844.659805297852}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.45658410374014 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=2, train loss <loss>=-0.8480354969700178\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:27 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_eb4fa259-3002-4ef8-9876-4ffae6244146-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846487.6920133, \"EndTime\": 1640846487.8000476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 107.42926597595215, \"count\": 1, \"min\": 107.42926597595215, \"max\": 107.42926597595215}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:28 INFO 140511108945280] Epoch[3] Batch[0] avg_epoch_loss=-1.911227\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=-1.9112268686294556\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:30 INFO 140511108945280] Epoch[3] Batch[5] avg_epoch_loss=-1.242933\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=-1.242933412392934\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:30 INFO 140511108945280] Epoch[3] Batch [5]#011Speed: 172.38 samples/sec#011loss=-1.242933\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] Epoch[3] Batch[10] avg_epoch_loss=-1.304753\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=-1.3789372444152832\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] Epoch[3] Batch [10]#011Speed: 163.64 samples/sec#011loss=-1.378937\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846487.8001626, \"EndTime\": 1640846492.6472478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4847.019195556641, \"count\": 1, \"min\": 4847.019195556641, \"max\": 4847.019195556641}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=146.89016357181114 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=3, train loss <loss>=-1.4128418664137523\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:32 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_24a65bca-2a96-4906-8f1c-be41030fe724-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846492.6473467, \"EndTime\": 1640846492.75675, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 108.81638526916504, \"count\": 1, \"min\": 108.81638526916504, \"max\": 108.81638526916504}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:33 INFO 140511108945280] Epoch[4] Batch[0] avg_epoch_loss=-2.097276\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=-2.097276210784912\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:35 INFO 140511108945280] Epoch[4] Batch[5] avg_epoch_loss=-1.703692\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=-1.7036919991175334\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:35 INFO 140511108945280] Epoch[4] Batch [5]#011Speed: 169.10 samples/sec#011loss=-1.703692\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] Epoch[4] Batch[10] avg_epoch_loss=-1.728703\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=-1.7587170124053955\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] Epoch[4] Batch [10]#011Speed: 166.92 samples/sec#011loss=-1.758717\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] processed a total of 733 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846492.7568467, \"EndTime\": 1640846497.5735383, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4816.617012023926, \"count\": 1, \"min\": 4816.617012023926, \"max\": 4816.617012023926}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.17716082002354 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=4, train loss <loss>=-1.5590845222274463\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:37 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_3ccce703-eeef-41ee-9df1-5b89b792d442-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846497.573636, \"EndTime\": 1640846497.6876528, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 113.47842216491699, \"count\": 1, \"min\": 113.47842216491699, \"max\": 113.47842216491699}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:38 INFO 140511108945280] Epoch[5] Batch[0] avg_epoch_loss=-1.813110\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=-1.8131103515625\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:40 INFO 140511108945280] Epoch[5] Batch[5] avg_epoch_loss=-1.750121\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=-1.7501207788785298\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:40 INFO 140511108945280] Epoch[5] Batch [5]#011Speed: 171.13 samples/sec#011loss=-1.750121\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] Epoch[5] Batch[10] avg_epoch_loss=-1.836640\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=-1.9404640436172484\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] Epoch[5] Batch [10]#011Speed: 166.21 samples/sec#011loss=-1.940464\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] processed a total of 762 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846497.6877184, \"EndTime\": 1640846502.4768496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4789.05463218689, \"count\": 1, \"min\": 4789.05463218689, \"max\": 4789.05463218689}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=159.10829965288553 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=5, train loss <loss>=-1.849064310391744\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:42 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_a5cb0c13-1222-4b3f-a5b4-0ad3b3387576-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846502.4769456, \"EndTime\": 1640846502.6085713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 131.09803199768066, \"count\": 1, \"min\": 131.09803199768066, \"max\": 131.09803199768066}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:43 INFO 140511108945280] Epoch[6] Batch[0] avg_epoch_loss=-2.243200\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=-2.2432000637054443\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:45 INFO 140511108945280] Epoch[6] Batch[5] avg_epoch_loss=-2.118323\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=-2.118323187033335\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:45 INFO 140511108945280] Epoch[6] Batch [5]#011Speed: 171.53 samples/sec#011loss=-2.118323\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] Epoch[6] Batch[10] avg_epoch_loss=-2.162948\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=-2.216496706008911\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] Epoch[6] Batch [10]#011Speed: 166.99 samples/sec#011loss=-2.216497\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] processed a total of 762 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846502.6086304, \"EndTime\": 1640846507.3813443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4772.652149200439, \"count\": 1, \"min\": 4772.652149200439, \"max\": 4772.652149200439}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=159.6551576894953 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=6, train loss <loss>=-2.174340377251307\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:47 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_a7168542-62a2-4d9f-ab83-cccd91a5c72f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846507.38144, \"EndTime\": 1640846507.5533352, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 171.31352424621582, \"count\": 1, \"min\": 171.31352424621582, \"max\": 171.31352424621582}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:48 INFO 140511108945280] Epoch[7] Batch[0] avg_epoch_loss=-2.053421\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=-2.0534214973449707\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:50 INFO 140511108945280] Epoch[7] Batch[5] avg_epoch_loss=-2.140523\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=-2.1405226985613504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:50 INFO 140511108945280] Epoch[7] Batch [5]#011Speed: 171.00 samples/sec#011loss=-2.140523\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] Epoch[7] Batch[10] avg_epoch_loss=-2.075687\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=-1.9978849172592164\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] Epoch[7] Batch [10]#011Speed: 159.52 samples/sec#011loss=-1.997885\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] processed a total of 745 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846507.5534225, \"EndTime\": 1640846512.4234908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4869.993448257446, \"count\": 1, \"min\": 4869.993448257446, \"max\": 4869.993448257446}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.97313259344375 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=7, train loss <loss>=-2.155207018057505\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:52 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:53 INFO 140511108945280] Epoch[8] Batch[0] avg_epoch_loss=-2.002454\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=-2.0024538040161133\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:54 INFO 140511108945280] Epoch[8] Batch[5] avg_epoch_loss=-2.378440\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=-2.378439744313558\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:54 INFO 140511108945280] Epoch[8] Batch [5]#011Speed: 169.01 samples/sec#011loss=-2.378440\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:56 INFO 140511108945280] Epoch[8] Batch[10] avg_epoch_loss=-2.520862\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=-2.6917676448822023\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:56 INFO 140511108945280] Epoch[8] Batch [10]#011Speed: 167.48 samples/sec#011loss=-2.691768\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] processed a total of 722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846512.4235895, \"EndTime\": 1640846517.2474465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4823.354005813599, \"count\": 1, \"min\": 4823.354005813599, \"max\": 4823.354005813599}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.6848261082496 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=8, train loss <loss>=-2.6519135435422263\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_dd1a3105-e9fc-484d-87f1-528aeeab8fd4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846517.247526, \"EndTime\": 1640846517.348509, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 100.33822059631348, \"count\": 1, \"min\": 100.33822059631348, \"max\": 100.33822059631348}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] Epoch[9] Batch[0] avg_epoch_loss=-3.497024\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=-3.497023582458496\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:59 INFO 140511108945280] Epoch[9] Batch[5] avg_epoch_loss=-2.698255\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=-2.6982551415761313\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:41:59 INFO 140511108945280] Epoch[9] Batch [5]#011Speed: 163.94 samples/sec#011loss=-2.698255\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:01 INFO 140511108945280] Epoch[9] Batch[10] avg_epoch_loss=-2.708319\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=-2.720395231246948\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:01 INFO 140511108945280] Epoch[9] Batch [10]#011Speed: 156.69 samples/sec#011loss=-2.720395\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:02 INFO 140511108945280] processed a total of 778 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846517.3485897, \"EndTime\": 1640846522.7254603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5376.792669296265, \"count\": 1, \"min\": 5376.792669296265, \"max\": 5376.792669296265}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:02 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=144.69193635745887 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:02 INFO 140511108945280] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=9, train loss <loss>=-2.609816367809589\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:02 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:03 INFO 140511108945280] Epoch[10] Batch[0] avg_epoch_loss=-2.241661\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=-2.2416605949401855\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:05 INFO 140511108945280] Epoch[10] Batch[5] avg_epoch_loss=-2.582420\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=-2.5824203888575235\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:05 INFO 140511108945280] Epoch[10] Batch [5]#011Speed: 151.83 samples/sec#011loss=-2.582420\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:07 INFO 140511108945280] Epoch[10] Batch[10] avg_epoch_loss=-2.713258\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=-2.8702628135681154\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:07 INFO 140511108945280] Epoch[10] Batch [10]#011Speed: 147.52 samples/sec#011loss=-2.870263\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] processed a total of 735 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846522.7255623, \"EndTime\": 1640846528.03509, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5308.855056762695, \"count\": 1, \"min\": 5308.855056762695, \"max\": 5308.855056762695}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=138.44432512772408 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=10, train loss <loss>=-2.831999719142914\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_40f9db80-929c-4343-9c5b-bfe690d9308d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846528.0351892, \"EndTime\": 1640846528.1545305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 118.78657341003418, \"count\": 1, \"min\": 118.78657341003418, \"max\": 118.78657341003418}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] Epoch[11] Batch[0] avg_epoch_loss=-2.598128\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=-2.598128318786621\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:10 INFO 140511108945280] Epoch[11] Batch[5] avg_epoch_loss=-2.842705\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=-2.8427048921585083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:10 INFO 140511108945280] Epoch[11] Batch [5]#011Speed: 171.98 samples/sec#011loss=-2.842705\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:12 INFO 140511108945280] Epoch[11] Batch[10] avg_epoch_loss=-2.857927\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=-2.876193904876709\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:12 INFO 140511108945280] Epoch[11] Batch [10]#011Speed: 164.97 samples/sec#011loss=-2.876194\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:13 INFO 140511108945280] processed a total of 789 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846528.154614, \"EndTime\": 1640846533.3096874, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5154.998064041138, \"count\": 1, \"min\": 5154.998064041138, \"max\": 5154.998064041138}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:13 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.05127340395086 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:13 INFO 140511108945280] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=11, train loss <loss>=-2.9560083059164195\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:13 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:13 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_dcb12d2b-4ebd-4305-baa2-19ab4843e8ac-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846533.3097844, \"EndTime\": 1640846533.4114182, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 101.0894775390625, \"count\": 1, \"min\": 101.0894775390625, \"max\": 101.0894775390625}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:14 INFO 140511108945280] Epoch[12] Batch[0] avg_epoch_loss=-2.967087\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=-2.9670872688293457\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:15 INFO 140511108945280] Epoch[12] Batch[5] avg_epoch_loss=-2.877966\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=-2.877966046333313\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:15 INFO 140511108945280] Epoch[12] Batch [5]#011Speed: 172.89 samples/sec#011loss=-2.877966\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:17 INFO 140511108945280] Epoch[12] Batch[10] avg_epoch_loss=-2.836645\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=-2.787060546875\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:17 INFO 140511108945280] Epoch[12] Batch [10]#011Speed: 166.79 samples/sec#011loss=-2.787061\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:18 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846533.411482, \"EndTime\": 1640846538.1870093, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4775.458574295044, \"count\": 1, \"min\": 4775.458574295044, \"max\": 4775.458574295044}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.46733038140434 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=12, train loss <loss>=-2.8842612703641257\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:18 INFO 140511108945280] Epoch[13] Batch[0] avg_epoch_loss=-3.102853\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=-3.102853298187256\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:20 INFO 140511108945280] Epoch[13] Batch[5] avg_epoch_loss=-2.932404\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=-2.9324042002360025\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:20 INFO 140511108945280] Epoch[13] Batch [5]#011Speed: 169.53 samples/sec#011loss=-2.932404\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:22 INFO 140511108945280] Epoch[13] Batch[10] avg_epoch_loss=-3.025565\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=-3.1373570919036866\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:22 INFO 140511108945280] Epoch[13] Batch [10]#011Speed: 160.77 samples/sec#011loss=-3.137357\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:23 INFO 140511108945280] processed a total of 731 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846538.1871064, \"EndTime\": 1640846543.074361, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4886.5392208099365, \"count\": 1, \"min\": 4886.5392208099365, \"max\": 4886.5392208099365}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:23 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.59048619651313 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:23 INFO 140511108945280] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=13, train loss <loss>=-2.9343391259511313\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:23 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:23 INFO 140511108945280] Epoch[14] Batch[0] avg_epoch_loss=-2.904508\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=-2.904508352279663\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:25 INFO 140511108945280] Epoch[14] Batch[5] avg_epoch_loss=-3.010932\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=-3.010932286580404\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:25 INFO 140511108945280] Epoch[14] Batch [5]#011Speed: 171.70 samples/sec#011loss=-3.010932\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] Epoch[14] Batch[10] avg_epoch_loss=-3.005542\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=-2.999072790145874\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] Epoch[14] Batch [10]#011Speed: 164.86 samples/sec#011loss=-2.999073\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] processed a total of 765 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846543.0744576, \"EndTime\": 1640846547.8485317, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4773.523330688477, \"count\": 1, \"min\": 4773.523330688477, \"max\": 4773.523330688477}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=160.25440586389695 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=14, train loss <loss>=-3.013683299223582\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:27 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_a0fef283-27a6-49be-864a-d92ca1c277b1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846547.848629, \"EndTime\": 1640846547.9603615, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 111.19961738586426, \"count\": 1, \"min\": 111.19961738586426, \"max\": 111.19961738586426}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:28 INFO 140511108945280] Epoch[15] Batch[0] avg_epoch_loss=-3.162515\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=-3.1625149250030518\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:30 INFO 140511108945280] Epoch[15] Batch[5] avg_epoch_loss=-3.025267\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=-3.025267481803894\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:30 INFO 140511108945280] Epoch[15] Batch [5]#011Speed: 171.42 samples/sec#011loss=-3.025267\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] Epoch[15] Batch[10] avg_epoch_loss=-3.213436\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=-3.4392385482788086\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] Epoch[15] Batch [10]#011Speed: 165.94 samples/sec#011loss=-3.439239\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] processed a total of 760 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846547.9604557, \"EndTime\": 1640846552.7405522, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4780.0233364105225, \"count\": 1, \"min\": 4780.0233364105225, \"max\": 4780.0233364105225}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.98936172600463 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=15, train loss <loss>=-3.210995316505432\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:32 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_412cce5b-b829-418e-8e55-f9c3541d6841-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846552.7406826, \"EndTime\": 1640846552.8492944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 108.05749893188477, \"count\": 1, \"min\": 108.05749893188477, \"max\": 108.05749893188477}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:33 INFO 140511108945280] Epoch[16] Batch[0] avg_epoch_loss=-3.289302\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=-3.289302110671997\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:35 INFO 140511108945280] Epoch[16] Batch[5] avg_epoch_loss=-3.295822\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=-3.295821746190389\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:35 INFO 140511108945280] Epoch[16] Batch [5]#011Speed: 170.81 samples/sec#011loss=-3.295822\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] Epoch[16] Batch[10] avg_epoch_loss=-3.243554\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=-3.180832624435425\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] Epoch[16] Batch [10]#011Speed: 167.83 samples/sec#011loss=-3.180833\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] processed a total of 764 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846552.8493893, \"EndTime\": 1640846557.688949, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4839.478015899658, \"count\": 1, \"min\": 4839.478015899658, \"max\": 4839.478015899658}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.86422905319196 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=16, train loss <loss>=-3.2198647061983743\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:37 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_0f45e87f-163c-41ee-babe-cfbf94cb5320-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846557.6890233, \"EndTime\": 1640846557.7894988, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 99.93362426757812, \"count\": 1, \"min\": 99.93362426757812, \"max\": 99.93362426757812}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:38 INFO 140511108945280] Epoch[17] Batch[0] avg_epoch_loss=-3.840123\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=-3.840123414993286\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:40 INFO 140511108945280] Epoch[17] Batch[5] avg_epoch_loss=-3.262999\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=-3.262999176979065\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:40 INFO 140511108945280] Epoch[17] Batch [5]#011Speed: 169.66 samples/sec#011loss=-3.262999\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] Epoch[17] Batch[10] avg_epoch_loss=-3.191450\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=-3.1055906295776365\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] Epoch[17] Batch [10]#011Speed: 162.67 samples/sec#011loss=-3.105591\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] processed a total of 796 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846557.789568, \"EndTime\": 1640846562.9874246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5197.779178619385, \"count\": 1, \"min\": 5197.779178619385, \"max\": 5197.779178619385}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.1382250426 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=17, train loss <loss>=-2.9727326941031675\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:42 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:43 INFO 140511108945280] Epoch[18] Batch[0] avg_epoch_loss=-3.350877\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=-3.350876808166504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:45 INFO 140511108945280] Epoch[18] Batch[5] avg_epoch_loss=-3.261302\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=-3.261301835378011\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:45 INFO 140511108945280] Epoch[18] Batch [5]#011Speed: 171.09 samples/sec#011loss=-3.261302\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] Epoch[18] Batch[10] avg_epoch_loss=-3.253744\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=-3.2446737766265867\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] Epoch[18] Batch [10]#011Speed: 168.64 samples/sec#011loss=-3.244674\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] processed a total of 717 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846562.9875226, \"EndTime\": 1640846567.7737339, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4785.57276725769, \"count\": 1, \"min\": 4785.57276725769, \"max\": 4785.57276725769}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.82063855147084 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=18, train loss <loss>=-3.0044840425252914\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:47 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:48 INFO 140511108945280] Epoch[19] Batch[0] avg_epoch_loss=-3.789887\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=-3.789886951446533\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:50 INFO 140511108945280] Epoch[19] Batch[5] avg_epoch_loss=-3.436036\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=-3.4360358317693076\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:50 INFO 140511108945280] Epoch[19] Batch [5]#011Speed: 166.08 samples/sec#011loss=-3.436036\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] Epoch[19] Batch[10] avg_epoch_loss=-3.396542\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=-3.3491489410400392\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] Epoch[19] Batch [10]#011Speed: 168.66 samples/sec#011loss=-3.349149\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] processed a total of 704 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846567.773822, \"EndTime\": 1640846572.2341897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4459.795713424683, \"count\": 1, \"min\": 4459.795713424683, \"max\": 4459.795713424683}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.85019875678114 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=19, train loss <loss>=-3.396541790528731\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_bc28ae81-60eb-492f-b812-606dd161ee5f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846572.2342799, \"EndTime\": 1640846572.3454986, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 110.68415641784668, \"count\": 1, \"min\": 110.68415641784668, \"max\": 110.68415641784668}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] Epoch[20] Batch[0] avg_epoch_loss=-3.614414\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=-3.6144139766693115\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:54 INFO 140511108945280] Epoch[20] Batch[5] avg_epoch_loss=-3.456843\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=-3.456843376159668\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:54 INFO 140511108945280] Epoch[20] Batch [5]#011Speed: 166.14 samples/sec#011loss=-3.456843\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:56 INFO 140511108945280] Epoch[20] Batch[10] avg_epoch_loss=-3.384031\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=-3.296657133102417\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:56 INFO 140511108945280] Epoch[20] Batch [10]#011Speed: 166.89 samples/sec#011loss=-3.296657\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] processed a total of 722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846572.3455915, \"EndTime\": 1640846577.2176883, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4872.016429901123, \"count\": 1, \"min\": 4872.016429901123, \"max\": 4872.016429901123}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.18913421671053 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=20, train loss <loss>=-3.522143224875132\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_334bbf26-2cc7-47c1-b8d5-e2dcf9b8b3ff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846577.21778, \"EndTime\": 1640846577.3247085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 106.37617111206055, \"count\": 1, \"min\": 106.37617111206055, \"max\": 106.37617111206055}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] Epoch[21] Batch[0] avg_epoch_loss=-3.232554\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=-3.2325544357299805\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:59 INFO 140511108945280] Epoch[21] Batch[5] avg_epoch_loss=-3.411774\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=-3.4117740392684937\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:42:59 INFO 140511108945280] Epoch[21] Batch [5]#011Speed: 168.21 samples/sec#011loss=-3.411774\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:01 INFO 140511108945280] Epoch[21] Batch[10] avg_epoch_loss=-3.476383\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=-3.5539129734039308\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:01 INFO 140511108945280] Epoch[21] Batch [10]#011Speed: 157.48 samples/sec#011loss=-3.553913\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:02 INFO 140511108945280] processed a total of 769 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846577.3247921, \"EndTime\": 1640846582.6672902, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5342.423677444458, \"count\": 1, \"min\": 5342.423677444458, \"max\": 5342.423677444458}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:02 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=143.93840791280635 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:02 INFO 140511108945280] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=21, train loss <loss>=-3.1852928698062897\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:02 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:03 INFO 140511108945280] Epoch[22] Batch[0] avg_epoch_loss=-3.526431\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=-3.5264313220977783\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:05 INFO 140511108945280] Epoch[22] Batch[5] avg_epoch_loss=-3.421866\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=-3.4218660593032837\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:05 INFO 140511108945280] Epoch[22] Batch [5]#011Speed: 152.50 samples/sec#011loss=-3.421866\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:07 INFO 140511108945280] Epoch[22] Batch[10] avg_epoch_loss=-3.446023\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=-3.4750123023986816\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:07 INFO 140511108945280] Epoch[22] Batch [10]#011Speed: 143.70 samples/sec#011loss=-3.475012\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:08 INFO 140511108945280] processed a total of 793 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846582.6673892, \"EndTime\": 1640846588.3886354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5720.698833465576, \"count\": 1, \"min\": 5720.698833465576, \"max\": 5720.698833465576}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=138.61613508192568 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=22, train loss <loss>=-3.4808413065396824\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:08 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:09 INFO 140511108945280] Epoch[23] Batch[0] avg_epoch_loss=-3.439308\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=-3.4393081665039062\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:10 INFO 140511108945280] Epoch[23] Batch[5] avg_epoch_loss=-3.779198\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=-3.7791976928710938\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:10 INFO 140511108945280] Epoch[23] Batch [5]#011Speed: 170.82 samples/sec#011loss=-3.779198\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:12 INFO 140511108945280] Epoch[23] Batch[10] avg_epoch_loss=-3.697335\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=-3.5991003036499025\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:12 INFO 140511108945280] Epoch[23] Batch [10]#011Speed: 166.35 samples/sec#011loss=-3.599100\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] processed a total of 746 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846588.388732, \"EndTime\": 1640846593.230164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4840.87872505188, \"count\": 1, \"min\": 4840.87872505188, \"max\": 4840.87872505188}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.10047341454148 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=23, train loss <loss>=-3.729977567990621\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_d5dbb805-88fc-4483-801c-7e6b1b86b4b2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846593.2302413, \"EndTime\": 1640846593.3368316, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 105.96561431884766, \"count\": 1, \"min\": 105.96561431884766, \"max\": 105.96561431884766}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] Epoch[24] Batch[0] avg_epoch_loss=-3.337112\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=-3.3371121883392334\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:15 INFO 140511108945280] Epoch[24] Batch[5] avg_epoch_loss=-3.576730\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=-3.576730211575826\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:15 INFO 140511108945280] Epoch[24] Batch [5]#011Speed: 169.51 samples/sec#011loss=-3.576730\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:17 INFO 140511108945280] Epoch[24] Batch[10] avg_epoch_loss=-3.462579\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=-3.325597953796387\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:17 INFO 140511108945280] Epoch[24] Batch [10]#011Speed: 167.23 samples/sec#011loss=-3.325598\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:18 INFO 140511108945280] processed a total of 717 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846593.3369303, \"EndTime\": 1640846598.1369367, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4799.934387207031, \"count\": 1, \"min\": 4799.934387207031, \"max\": 4799.934387207031}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.37274597882092 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=24, train loss <loss>=-3.6456807255744934\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:18 INFO 140511108945280] Epoch[25] Batch[0] avg_epoch_loss=-3.884407\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=-3.884406805038452\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:20 INFO 140511108945280] Epoch[25] Batch[5] avg_epoch_loss=-3.671536\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=-3.6715360085169473\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:20 INFO 140511108945280] Epoch[25] Batch [5]#011Speed: 167.73 samples/sec#011loss=-3.671536\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] Epoch[25] Batch[10] avg_epoch_loss=-3.600928\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=-3.516198492050171\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] Epoch[25] Batch [10]#011Speed: 165.96 samples/sec#011loss=-3.516198\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] processed a total of 723 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846598.1370342, \"EndTime\": 1640846602.979617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4842.050075531006, \"count\": 1, \"min\": 4842.050075531006, \"max\": 4842.050075531006}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.3125080072852 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=25, train loss <loss>=-3.585009535153707\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:22 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:23 INFO 140511108945280] Epoch[26] Batch[0] avg_epoch_loss=-3.769179\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=-3.769178867340088\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:25 INFO 140511108945280] Epoch[26] Batch[5] avg_epoch_loss=-3.670307\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=-3.670306841532389\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:25 INFO 140511108945280] Epoch[26] Batch [5]#011Speed: 169.46 samples/sec#011loss=-3.670307\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] Epoch[26] Batch[10] avg_epoch_loss=-3.699857\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=-3.7353166580200194\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] Epoch[26] Batch [10]#011Speed: 167.05 samples/sec#011loss=-3.735317\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] processed a total of 743 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846602.9797177, \"EndTime\": 1640846607.8301373, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4849.820852279663, \"count\": 1, \"min\": 4849.820852279663, \"max\": 4849.820852279663}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.19757376666547 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=26, train loss <loss>=-3.754784186681112\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:27 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_93b3edc7-2a63-41a4-8594-b600738d9fd2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846607.830221, \"EndTime\": 1640846607.9708903, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 140.0620937347412, \"count\": 1, \"min\": 140.0620937347412, \"max\": 140.0620937347412}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:28 INFO 140511108945280] Epoch[27] Batch[0] avg_epoch_loss=-4.114257\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=-4.114256858825684\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:30 INFO 140511108945280] Epoch[27] Batch[5] avg_epoch_loss=-3.641269\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=-3.6412694851557412\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:30 INFO 140511108945280] Epoch[27] Batch [5]#011Speed: 169.38 samples/sec#011loss=-3.641269\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] Epoch[27] Batch[10] avg_epoch_loss=-3.774917\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=-3.9352947235107423\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] Epoch[27] Batch [10]#011Speed: 167.63 samples/sec#011loss=-3.935295\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] processed a total of 742 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846607.970947, \"EndTime\": 1640846612.7509506, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4779.944658279419, \"count\": 1, \"min\": 4779.944658279419, \"max\": 4779.944658279419}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.22783466903127 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=27, train loss <loss>=-3.5603981216748557\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:32 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:33 INFO 140511108945280] Epoch[28] Batch[0] avg_epoch_loss=-4.137409\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=-4.13740873336792\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:35 INFO 140511108945280] Epoch[28] Batch[5] avg_epoch_loss=-3.866250\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=-3.8662495215733848\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:35 INFO 140511108945280] Epoch[28] Batch [5]#011Speed: 168.09 samples/sec#011loss=-3.866250\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] Epoch[28] Batch[10] avg_epoch_loss=-3.845331\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=-3.820228910446167\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] Epoch[28] Batch [10]#011Speed: 166.80 samples/sec#011loss=-3.820229\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846612.7510395, \"EndTime\": 1640846617.5525477, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4801.002025604248, \"count\": 1, \"min\": 4801.002025604248, \"max\": 4801.002025604248}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.588207824041 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=28, train loss <loss>=-3.850779036680857\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:37 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_d770b105-4371-421e-8f44-fe42298ecd05-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846617.552643, \"EndTime\": 1640846617.672014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 118.83115768432617, \"count\": 1, \"min\": 118.83115768432617, \"max\": 118.83115768432617}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:38 INFO 140511108945280] Epoch[29] Batch[0] avg_epoch_loss=-4.224466\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=-4.224466323852539\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:40 INFO 140511108945280] Epoch[29] Batch[5] avg_epoch_loss=-3.800912\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=-3.800912062327067\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:40 INFO 140511108945280] Epoch[29] Batch [5]#011Speed: 167.41 samples/sec#011loss=-3.800912\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] Epoch[29] Batch[10] avg_epoch_loss=-3.794763\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=-3.787383031845093\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] Epoch[29] Batch [10]#011Speed: 167.71 samples/sec#011loss=-3.787383\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] processed a total of 743 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846617.6720936, \"EndTime\": 1640846622.4484367, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4776.240825653076, \"count\": 1, \"min\": 4776.240825653076, \"max\": 4776.240825653076}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.5570577429918 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=29, train loss <loss>=-3.8290159900983176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:42 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:43 INFO 140511108945280] Epoch[30] Batch[0] avg_epoch_loss=-3.417307\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=-3.417306900024414\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:44 INFO 140511108945280] Epoch[30] Batch[5] avg_epoch_loss=-3.657894\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=-3.6578943729400635\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:44 INFO 140511108945280] Epoch[30] Batch [5]#011Speed: 165.38 samples/sec#011loss=-3.657894\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:46 INFO 140511108945280] Epoch[30] Batch[10] avg_epoch_loss=-3.678621\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=-3.7034931659698485\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:46 INFO 140511108945280] Epoch[30] Batch [10]#011Speed: 168.10 samples/sec#011loss=-3.703493\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:47 INFO 140511108945280] processed a total of 715 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846622.4485333, \"EndTime\": 1640846627.2573864, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4808.335781097412, \"count\": 1, \"min\": 4808.335781097412, \"max\": 4808.335781097412}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:47 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.69585813744283 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:47 INFO 140511108945280] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=30, train loss <loss>=-3.659001588821411\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:47 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:47 INFO 140511108945280] Epoch[31] Batch[0] avg_epoch_loss=-4.028144\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=-4.028144359588623\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:49 INFO 140511108945280] Epoch[31] Batch[5] avg_epoch_loss=-3.970153\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=-3.9701528946558633\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:49 INFO 140511108945280] Epoch[31] Batch [5]#011Speed: 169.62 samples/sec#011loss=-3.970153\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:51 INFO 140511108945280] Epoch[31] Batch[10] avg_epoch_loss=-3.934861\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=-3.8925106525421143\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:51 INFO 140511108945280] Epoch[31] Batch [10]#011Speed: 166.51 samples/sec#011loss=-3.892511\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] processed a total of 737 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846627.257484, \"EndTime\": 1640846632.057551, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4799.574136734009, \"count\": 1, \"min\": 4799.574136734009, \"max\": 4799.574136734009}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.55114088590983 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=31, train loss <loss>=-3.8965375820795694\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_f464d78e-604b-4e63-ad60-1c1cd2b846f7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846632.0576432, \"EndTime\": 1640846632.1636548, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 105.48734664916992, \"count\": 1, \"min\": 105.48734664916992, \"max\": 105.48734664916992}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] Epoch[32] Batch[0] avg_epoch_loss=-3.674813\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=-3.6748125553131104\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:54 INFO 140511108945280] Epoch[32] Batch[5] avg_epoch_loss=-4.058984\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=-4.058983683586121\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:54 INFO 140511108945280] Epoch[32] Batch [5]#011Speed: 157.82 samples/sec#011loss=-4.058984\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:56 INFO 140511108945280] Epoch[32] Batch[10] avg_epoch_loss=-3.961995\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=-3.8456085205078123\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:56 INFO 140511108945280] Epoch[32] Batch [10]#011Speed: 167.85 samples/sec#011loss=-3.845609\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] processed a total of 709 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846632.163737, \"EndTime\": 1640846637.0838864, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4920.078277587891, \"count\": 1, \"min\": 4920.078277587891, \"max\": 4920.078277587891}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=144.09909695826704 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=32, train loss <loss>=-4.19158262014389\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_88904557-42ec-47dc-b4f7-c0e21d899201-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846637.0839884, \"EndTime\": 1640846637.184198, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 99.51114654541016, \"count\": 1, \"min\": 99.51114654541016, \"max\": 99.51114654541016}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] Epoch[33] Batch[0] avg_epoch_loss=-4.109134\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=-4.109134197235107\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:59 INFO 140511108945280] Epoch[33] Batch[5] avg_epoch_loss=-3.883246\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=-3.8832459449768066\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:43:59 INFO 140511108945280] Epoch[33] Batch [5]#011Speed: 169.64 samples/sec#011loss=-3.883246\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:01 INFO 140511108945280] Epoch[33] Batch[10] avg_epoch_loss=-3.898528\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=-3.9168668746948243\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:01 INFO 140511108945280] Epoch[33] Batch [10]#011Speed: 160.43 samples/sec#011loss=-3.916867\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:02 INFO 140511108945280] processed a total of 772 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846637.184283, \"EndTime\": 1640846642.4968839, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5312.527418136597, \"count\": 1, \"min\": 5312.527418136597, \"max\": 5312.527418136597}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:02 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=145.31327766436252 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:02 INFO 140511108945280] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=33, train loss <loss>=-4.045637075717632\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:02 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:03 INFO 140511108945280] Epoch[34] Batch[0] avg_epoch_loss=-4.321345\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=-4.321345329284668\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:05 INFO 140511108945280] Epoch[34] Batch[5] avg_epoch_loss=-3.853720\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=-3.8537203868230185\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:05 INFO 140511108945280] Epoch[34] Batch [5]#011Speed: 151.10 samples/sec#011loss=-3.853720\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] Epoch[34] Batch[10] avg_epoch_loss=-3.769199\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=-3.6677725315093994\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] Epoch[34] Batch [10]#011Speed: 146.32 samples/sec#011loss=-3.667773\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] processed a total of 757 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846642.4969776, \"EndTime\": 1640846647.7799535, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5282.432079315186, \"count\": 1, \"min\": 5282.432079315186, \"max\": 5282.432079315186}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=143.30154194497987 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=34, train loss <loss>=-3.816480120023092\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:07 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:08 INFO 140511108945280] Epoch[35] Batch[0] avg_epoch_loss=-4.102574\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=-4.102573871612549\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:10 INFO 140511108945280] Epoch[35] Batch[5] avg_epoch_loss=-3.785488\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=-3.785487731297811\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:10 INFO 140511108945280] Epoch[35] Batch [5]#011Speed: 168.21 samples/sec#011loss=-3.785488\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] Epoch[35] Batch[10] avg_epoch_loss=-3.851863\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=-3.9315125942230225\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] Epoch[35] Batch [10]#011Speed: 165.55 samples/sec#011loss=-3.931513\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] processed a total of 755 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846647.7800493, \"EndTime\": 1640846652.6216216, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4841.028690338135, \"count\": 1, \"min\": 4841.028690338135, \"max\": 4841.028690338135}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.95418717627632 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=35, train loss <loss>=-3.8931891719500222\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:12 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:13 INFO 140511108945280] Epoch[36] Batch[0] avg_epoch_loss=-3.906758\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=-3.9067583084106445\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:15 INFO 140511108945280] Epoch[36] Batch[5] avg_epoch_loss=-3.954301\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=-3.954301198323568\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:15 INFO 140511108945280] Epoch[36] Batch [5]#011Speed: 165.60 samples/sec#011loss=-3.954301\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] Epoch[36] Batch[10] avg_epoch_loss=-4.043157\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=-4.149784755706787\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] Epoch[36] Batch [10]#011Speed: 166.64 samples/sec#011loss=-4.149785\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] processed a total of 719 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846652.6217182, \"EndTime\": 1640846657.475342, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4853.097915649414, \"count\": 1, \"min\": 4853.097915649414, \"max\": 4853.097915649414}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.14832200185674 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=36, train loss <loss>=-3.8853854139645896\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:17 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:18 INFO 140511108945280] Epoch[37] Batch[0] avg_epoch_loss=-2.209886\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=-2.209885597229004\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:19 INFO 140511108945280] Epoch[37] Batch[5] avg_epoch_loss=-3.099823\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=-3.099823276201884\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:19 INFO 140511108945280] Epoch[37] Batch [5]#011Speed: 166.29 samples/sec#011loss=-3.099823\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:21 INFO 140511108945280] Epoch[37] Batch[10] avg_epoch_loss=-3.059706\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=-3.011565589904785\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:21 INFO 140511108945280] Epoch[37] Batch [10]#011Speed: 161.72 samples/sec#011loss=-3.011566\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:22 INFO 140511108945280] processed a total of 735 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846657.4754462, \"EndTime\": 1640846662.3586528, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4882.542848587036, \"count\": 1, \"min\": 4882.542848587036, \"max\": 4882.542848587036}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:22 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.53268209562964 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:22 INFO 140511108945280] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=37, train loss <loss>=-2.9805779655774436\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:22 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:22 INFO 140511108945280] Epoch[38] Batch[0] avg_epoch_loss=-4.113028\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=-4.113028049468994\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:24 INFO 140511108945280] Epoch[38] Batch[5] avg_epoch_loss=-3.922926\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=-3.922926425933838\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:24 INFO 140511108945280] Epoch[38] Batch [5]#011Speed: 162.90 samples/sec#011loss=-3.922926\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:26 INFO 140511108945280] Epoch[38] Batch[10] avg_epoch_loss=-3.884554\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=-3.838506555557251\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:26 INFO 140511108945280] Epoch[38] Batch [10]#011Speed: 166.77 samples/sec#011loss=-3.838507\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:27 INFO 140511108945280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846662.3587344, \"EndTime\": 1640846667.2148662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4855.499982833862, \"count\": 1, \"min\": 4855.499982833862, \"max\": 4855.499982833862}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:27 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.10585356665658 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:27 INFO 140511108945280] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=38, train loss <loss>=-4.001642485459645\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:27 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:27 INFO 140511108945280] Epoch[39] Batch[0] avg_epoch_loss=-3.494032\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=-3.4940319061279297\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:29 INFO 140511108945280] Epoch[39] Batch[5] avg_epoch_loss=-4.036641\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=-4.036640604337056\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:29 INFO 140511108945280] Epoch[39] Batch [5]#011Speed: 170.49 samples/sec#011loss=-4.036641\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:31 INFO 140511108945280] Epoch[39] Batch[10] avg_epoch_loss=-3.958175\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=-3.864017105102539\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:31 INFO 140511108945280] Epoch[39] Batch [10]#011Speed: 163.74 samples/sec#011loss=-3.864017\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:32 INFO 140511108945280] processed a total of 784 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846667.2149382, \"EndTime\": 1640846672.3684514, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5152.930974960327, \"count\": 1, \"min\": 5152.930974960327, \"max\": 5152.930974960327}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:32 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.14232300019697 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:32 INFO 140511108945280] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=39, train loss <loss>=-4.080400668657743\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:32 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:33 INFO 140511108945280] Epoch[40] Batch[0] avg_epoch_loss=-3.273446\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=-3.2734458446502686\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:34 INFO 140511108945280] Epoch[40] Batch[5] avg_epoch_loss=-3.738923\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=-3.738922675450643\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:34 INFO 140511108945280] Epoch[40] Batch [5]#011Speed: 170.87 samples/sec#011loss=-3.738923\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:36 INFO 140511108945280] Epoch[40] Batch[10] avg_epoch_loss=-3.879749\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=-4.048740196228027\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:36 INFO 140511108945280] Epoch[40] Batch [10]#011Speed: 166.57 samples/sec#011loss=-4.048740\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:37 INFO 140511108945280] processed a total of 713 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846672.3685489, \"EndTime\": 1640846677.1829793, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4813.908576965332, \"count\": 1, \"min\": 4813.908576965332, \"max\": 4813.908576965332}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:37 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.1082237356585 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:37 INFO 140511108945280] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=40, train loss <loss>=-4.017126242319743\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:37 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:37 INFO 140511108945280] Epoch[41] Batch[0] avg_epoch_loss=-3.962915\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=-3.9629149436950684\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:39 INFO 140511108945280] Epoch[41] Batch[5] avg_epoch_loss=-3.979064\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=-3.979064106941223\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:39 INFO 140511108945280] Epoch[41] Batch [5]#011Speed: 165.87 samples/sec#011loss=-3.979064\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:41 INFO 140511108945280] Epoch[41] Batch[10] avg_epoch_loss=-3.947277\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=-3.909131669998169\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:41 INFO 140511108945280] Epoch[41] Batch [10]#011Speed: 165.70 samples/sec#011loss=-3.909132\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:42 INFO 140511108945280] processed a total of 793 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846677.1830745, \"EndTime\": 1640846682.3721864, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5188.524484634399, \"count\": 1, \"min\": 5188.524484634399, \"max\": 5188.524484634399}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:42 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.83314240389316 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:42 INFO 140511108945280] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=41, train loss <loss>=-3.919324654799241\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:42 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:42 INFO 140511108945280] Epoch[42] Batch[0] avg_epoch_loss=-3.886386\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=-3.8863861560821533\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:44 INFO 140511108945280] Epoch[42] Batch[5] avg_epoch_loss=-3.837062\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=-3.837061802546183\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:44 INFO 140511108945280] Epoch[42] Batch [5]#011Speed: 168.52 samples/sec#011loss=-3.837062\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:46 INFO 140511108945280] Epoch[42] Batch[10] avg_epoch_loss=-3.845727\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=-3.8561256885528565\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:46 INFO 140511108945280] Epoch[42] Batch [10]#011Speed: 164.68 samples/sec#011loss=-3.856126\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:47 INFO 140511108945280] processed a total of 773 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846682.372285, \"EndTime\": 1640846687.5353105, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5162.282466888428, \"count\": 1, \"min\": 5162.282466888428, \"max\": 5162.282466888428}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:47 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.73594787879958 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:47 INFO 140511108945280] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=42, train loss <loss>=-3.814106280987079\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:47 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:48 INFO 140511108945280] Epoch[43] Batch[0] avg_epoch_loss=-4.249148\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=-4.249148368835449\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:50 INFO 140511108945280] Epoch[43] Batch[5] avg_epoch_loss=-4.035752\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=-4.035752336184184\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:50 INFO 140511108945280] Epoch[43] Batch [5]#011Speed: 170.93 samples/sec#011loss=-4.035752\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:51 INFO 140511108945280] Epoch[43] Batch[10] avg_epoch_loss=-3.944713\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=-3.835466003417969\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:51 INFO 140511108945280] Epoch[43] Batch [10]#011Speed: 165.54 samples/sec#011loss=-3.835466\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:52 INFO 140511108945280] processed a total of 761 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846687.535407, \"EndTime\": 1640846692.3116937, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4775.76208114624, \"count\": 1, \"min\": 4775.76208114624, \"max\": 4775.76208114624}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:52 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=159.3417462535388 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:52 INFO 140511108945280] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=43, train loss <loss>=-3.984656552473704\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:52 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:52 INFO 140511108945280] Epoch[44] Batch[0] avg_epoch_loss=-2.898637\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=-2.898637056350708\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:54 INFO 140511108945280] Epoch[44] Batch[5] avg_epoch_loss=-3.820353\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=-3.8203527132670083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:54 INFO 140511108945280] Epoch[44] Batch [5]#011Speed: 160.94 samples/sec#011loss=-3.820353\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:56 INFO 140511108945280] Epoch[44] Batch[10] avg_epoch_loss=-3.794949\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=-3.76446418762207\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:56 INFO 140511108945280] Epoch[44] Batch [10]#011Speed: 165.89 samples/sec#011loss=-3.764464\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:57 INFO 140511108945280] processed a total of 748 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846692.311789, \"EndTime\": 1640846697.210852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4898.492813110352, \"count\": 1, \"min\": 4898.492813110352, \"max\": 4898.492813110352}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:57 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.69573441428466 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:57 INFO 140511108945280] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=44, train loss <loss>=-3.830969293912252\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:57 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:57 INFO 140511108945280] Epoch[45] Batch[0] avg_epoch_loss=-3.543616\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=-3.54361629486084\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:59 INFO 140511108945280] Epoch[45] Batch[5] avg_epoch_loss=-4.138503\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=-4.138503392537435\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:44:59 INFO 140511108945280] Epoch[45] Batch [5]#011Speed: 170.71 samples/sec#011loss=-4.138503\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:01 INFO 140511108945280] Epoch[45] Batch[10] avg_epoch_loss=-4.182511\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=-4.235319471359253\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:01 INFO 140511108945280] Epoch[45] Batch [10]#011Speed: 162.19 samples/sec#011loss=-4.235319\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:02 INFO 140511108945280] processed a total of 782 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846697.2109485, \"EndTime\": 1640846702.499796, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5288.2726192474365, \"count\": 1, \"min\": 5288.2726192474365, \"max\": 5288.2726192474365}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:02 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.8703400911798 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:02 INFO 140511108945280] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=45, train loss <loss>=-3.8945781680253835\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:02 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:03 INFO 140511108945280] Epoch[46] Batch[0] avg_epoch_loss=-3.331218\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=-3.3312184810638428\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:05 INFO 140511108945280] Epoch[46] Batch[5] avg_epoch_loss=-3.807227\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=-3.8072270154953003\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:05 INFO 140511108945280] Epoch[46] Batch [5]#011Speed: 158.64 samples/sec#011loss=-3.807227\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] Epoch[46] Batch[10] avg_epoch_loss=-3.951248\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=-4.1240729808807375\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] Epoch[46] Batch [10]#011Speed: 144.32 samples/sec#011loss=-4.124073\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] processed a total of 762 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846702.4998984, \"EndTime\": 1640846707.6911976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5190.647602081299, \"count\": 1, \"min\": 5190.647602081299, \"max\": 5190.647602081299}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=146.79852664063353 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=46, train loss <loss>=-3.987544298171997\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:07 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:08 INFO 140511108945280] Epoch[47] Batch[0] avg_epoch_loss=-4.893562\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=-4.893562316894531\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:10 INFO 140511108945280] Epoch[47] Batch[5] avg_epoch_loss=-4.043662\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=-4.043661793073018\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:10 INFO 140511108945280] Epoch[47] Batch [5]#011Speed: 170.69 samples/sec#011loss=-4.043662\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] Epoch[47] Batch[10] avg_epoch_loss=-3.942273\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=-3.820607233047485\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] Epoch[47] Batch [10]#011Speed: 164.66 samples/sec#011loss=-3.820607\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] processed a total of 743 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846707.6912973, \"EndTime\": 1640846712.4755378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4783.6925983428955, \"count\": 1, \"min\": 4783.6925983428955, \"max\": 4783.6925983428955}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.31462285503855 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=47, train loss <loss>=-4.035321354866028\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:12 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:13 INFO 140511108945280] Epoch[48] Batch[0] avg_epoch_loss=-3.931463\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=-3.9314632415771484\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:14 INFO 140511108945280] Epoch[48] Batch[5] avg_epoch_loss=-3.770583\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=-3.770583430926005\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:14 INFO 140511108945280] Epoch[48] Batch [5]#011Speed: 171.29 samples/sec#011loss=-3.770583\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:16 INFO 140511108945280] Epoch[48] Batch[10] avg_epoch_loss=-3.890040\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=-4.033387279510498\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:16 INFO 140511108945280] Epoch[48] Batch [10]#011Speed: 169.06 samples/sec#011loss=-4.033387\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:17 INFO 140511108945280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846712.4756422, \"EndTime\": 1640846717.236074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4759.847640991211, \"count\": 1, \"min\": 4759.847640991211, \"max\": 4759.847640991211}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:17 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.10029211680325 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:17 INFO 140511108945280] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=48, train loss <loss>=-3.7117345730463662\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:17 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:17 INFO 140511108945280] Epoch[49] Batch[0] avg_epoch_loss=-4.151198\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=-4.151197910308838\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:19 INFO 140511108945280] Epoch[49] Batch[5] avg_epoch_loss=-4.146262\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=-4.1462617715199785\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:19 INFO 140511108945280] Epoch[49] Batch [5]#011Speed: 170.53 samples/sec#011loss=-4.146262\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:21 INFO 140511108945280] Epoch[49] Batch[10] avg_epoch_loss=-4.071075\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=-3.9808505535125733\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:21 INFO 140511108945280] Epoch[49] Batch [10]#011Speed: 165.44 samples/sec#011loss=-3.980851\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:22 INFO 140511108945280] processed a total of 783 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846717.236203, \"EndTime\": 1640846722.3745508, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5137.778043746948, \"count\": 1, \"min\": 5137.778043746948, \"max\": 5137.778043746948}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:22 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.3963457619845 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:22 INFO 140511108945280] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=49, train loss <loss>=-4.069415037448589\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:22 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:22 INFO 140511108945280] Epoch[50] Batch[0] avg_epoch_loss=-3.836856\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=-3.8368563652038574\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:24 INFO 140511108945280] Epoch[50] Batch[5] avg_epoch_loss=-4.044215\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=-4.044215242067973\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:24 INFO 140511108945280] Epoch[50] Batch [5]#011Speed: 164.81 samples/sec#011loss=-4.044215\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:26 INFO 140511108945280] Epoch[50] Batch[10] avg_epoch_loss=-4.051648\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=-4.060567092895508\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:26 INFO 140511108945280] Epoch[50] Batch [10]#011Speed: 163.32 samples/sec#011loss=-4.060567\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846722.3746495, \"EndTime\": 1640846727.2852566, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4910.073041915894, \"count\": 1, \"min\": 4910.073041915894, \"max\": 4910.073041915894}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=144.18939750492683 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=50, train loss <loss>=-4.1992771824200945\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_85a8ab51-189c-46f2-8f1b-18e10a72316f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846727.285353, \"EndTime\": 1640846727.3933177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 107.35940933227539, \"count\": 1, \"min\": 107.35940933227539, \"max\": 107.35940933227539}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] Epoch[51] Batch[0] avg_epoch_loss=-3.767240\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=-3.767240285873413\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:29 INFO 140511108945280] Epoch[51] Batch[5] avg_epoch_loss=-3.989567\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=-3.989567438761393\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:29 INFO 140511108945280] Epoch[51] Batch [5]#011Speed: 171.94 samples/sec#011loss=-3.989567\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:31 INFO 140511108945280] Epoch[51] Batch[10] avg_epoch_loss=-4.048931\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=-4.120167779922485\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:31 INFO 140511108945280] Epoch[51] Batch [10]#011Speed: 168.61 samples/sec#011loss=-4.120168\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:32 INFO 140511108945280] processed a total of 740 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846727.3934047, \"EndTime\": 1640846732.1250806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4731.605529785156, \"count\": 1, \"min\": 4731.605529785156, \"max\": 4731.605529785156}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:32 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.3904439020003 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:32 INFO 140511108945280] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=51, train loss <loss>=-3.855414221684138\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:32 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:32 INFO 140511108945280] Epoch[52] Batch[0] avg_epoch_loss=-4.183028\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=-4.183028221130371\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:34 INFO 140511108945280] Epoch[52] Batch[5] avg_epoch_loss=-4.132929\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=-4.13292920589447\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:34 INFO 140511108945280] Epoch[52] Batch [5]#011Speed: 169.92 samples/sec#011loss=-4.132929\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] Epoch[52] Batch[10] avg_epoch_loss=-4.088807\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=-4.035861253738403\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] Epoch[52] Batch [10]#011Speed: 168.33 samples/sec#011loss=-4.035861\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] processed a total of 726 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846732.1251812, \"EndTime\": 1640846736.8726988, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4746.891021728516, \"count\": 1, \"min\": 4746.891021728516, \"max\": 4746.891021728516}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.93833777790994 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=52, train loss <loss>=-4.030739446481069\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:36 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:37 INFO 140511108945280] Epoch[53] Batch[0] avg_epoch_loss=-3.516459\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=-3.5164589881896973\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:39 INFO 140511108945280] Epoch[53] Batch[5] avg_epoch_loss=-4.209212\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=-4.209211508433024\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:39 INFO 140511108945280] Epoch[53] Batch [5]#011Speed: 171.18 samples/sec#011loss=-4.209212\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] Epoch[53] Batch[10] avg_epoch_loss=-4.137948\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=-4.052431678771972\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] Epoch[53] Batch [10]#011Speed: 167.97 samples/sec#011loss=-4.052432\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] processed a total of 716 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846736.8727875, \"EndTime\": 1640846741.613524, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4740.183353424072, \"count\": 1, \"min\": 4740.183353424072, \"max\": 4740.183353424072}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.04473222533795 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=53, train loss <loss>=-3.751444031794866\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:41 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:42 INFO 140511108945280] Epoch[54] Batch[0] avg_epoch_loss=-3.738864\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=-3.738863945007324\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:44 INFO 140511108945280] Epoch[54] Batch[5] avg_epoch_loss=-4.067014\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=-4.067013581593831\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:44 INFO 140511108945280] Epoch[54] Batch [5]#011Speed: 171.53 samples/sec#011loss=-4.067014\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] Epoch[54] Batch[10] avg_epoch_loss=-4.061673\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=-4.055264616012574\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] Epoch[54] Batch [10]#011Speed: 165.26 samples/sec#011loss=-4.055265\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] processed a total of 738 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846741.6136186, \"EndTime\": 1640846746.396672, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4782.522201538086, \"count\": 1, \"min\": 4782.522201538086, \"max\": 4782.522201538086}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.30768921694863 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=54, train loss <loss>=-4.193610688050588\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] Epoch[55] Batch[0] avg_epoch_loss=-4.088096\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=-4.088095664978027\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:48 INFO 140511108945280] Epoch[55] Batch[5] avg_epoch_loss=-4.275784\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=-4.275784492492676\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:48 INFO 140511108945280] Epoch[55] Batch [5]#011Speed: 169.31 samples/sec#011loss=-4.275784\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:50 INFO 140511108945280] Epoch[55] Batch[10] avg_epoch_loss=-4.204134\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=-4.118153667449951\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:50 INFO 140511108945280] Epoch[55] Batch [10]#011Speed: 165.75 samples/sec#011loss=-4.118154\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:51 INFO 140511108945280] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846746.39676, \"EndTime\": 1640846751.180825, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4783.486366271973, \"count\": 1, \"min\": 4783.486366271973, \"max\": 4783.486366271973}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:51 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.0049972173996 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:51 INFO 140511108945280] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=55, train loss <loss>=-4.153571466604869\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:51 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:51 INFO 140511108945280] Epoch[56] Batch[0] avg_epoch_loss=-4.128858\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=-4.1288580894470215\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:53 INFO 140511108945280] Epoch[56] Batch[5] avg_epoch_loss=-4.345399\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=-4.345398664474487\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:53 INFO 140511108945280] Epoch[56] Batch [5]#011Speed: 172.30 samples/sec#011loss=-4.345399\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:55 INFO 140511108945280] Epoch[56] Batch[10] avg_epoch_loss=-4.225095\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=-4.080730009078979\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:55 INFO 140511108945280] Epoch[56] Batch [10]#011Speed: 157.65 samples/sec#011loss=-4.080730\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:56 INFO 140511108945280] processed a total of 717 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846751.1809218, \"EndTime\": 1640846756.0581908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4876.765012741089, \"count\": 1, \"min\": 4876.765012741089, \"max\": 4876.765012741089}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:56 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.01959564814922 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:56 INFO 140511108945280] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=56, train loss <loss>=-4.158087432384491\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:56 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:56 INFO 140511108945280] Epoch[57] Batch[0] avg_epoch_loss=-3.800290\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=-3.800290107727051\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:58 INFO 140511108945280] Epoch[57] Batch[5] avg_epoch_loss=-3.896987\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=-3.8969868421554565\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:45:58 INFO 140511108945280] Epoch[57] Batch [5]#011Speed: 172.40 samples/sec#011loss=-3.896987\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:00 INFO 140511108945280] Epoch[57] Batch[10] avg_epoch_loss=-3.896532\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=-3.895985507965088\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:00 INFO 140511108945280] Epoch[57] Batch [10]#011Speed: 163.24 samples/sec#011loss=-3.895986\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:01 INFO 140511108945280] processed a total of 792 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846756.058288, \"EndTime\": 1640846761.2303429, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5171.534061431885, \"count\": 1, \"min\": 5171.534061431885, \"max\": 5171.534061431885}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:01 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.14141585039988 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:01 INFO 140511108945280] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=57, train loss <loss>=-3.941595572691697\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:01 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:01 INFO 140511108945280] Epoch[58] Batch[0] avg_epoch_loss=-4.473746\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=-4.473746299743652\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:03 INFO 140511108945280] Epoch[58] Batch[5] avg_epoch_loss=-4.220282\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=-4.220281879107158\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:03 INFO 140511108945280] Epoch[58] Batch [5]#011Speed: 163.18 samples/sec#011loss=-4.220282\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] Epoch[58] Batch[10] avg_epoch_loss=-4.247187\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=-4.279473400115966\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] Epoch[58] Batch [10]#011Speed: 140.81 samples/sec#011loss=-4.279473\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] processed a total of 730 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846761.2304592, \"EndTime\": 1640846766.68138, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5450.393915176392, \"count\": 1, \"min\": 5450.393915176392, \"max\": 5450.393915176392}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=133.93196342410062 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=58, train loss <loss>=-4.0252178609371185\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:06 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:07 INFO 140511108945280] Epoch[59] Batch[0] avg_epoch_loss=-3.750746\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=-3.750746488571167\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:09 INFO 140511108945280] Epoch[59] Batch[5] avg_epoch_loss=-4.241781\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=-4.241780996322632\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:09 INFO 140511108945280] Epoch[59] Batch [5]#011Speed: 172.67 samples/sec#011loss=-4.241781\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] Epoch[59] Batch[10] avg_epoch_loss=-4.286702\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=-4.3406072616577145\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] Epoch[59] Batch [10]#011Speed: 163.80 samples/sec#011loss=-4.340607\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] processed a total of 803 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846766.6814766, \"EndTime\": 1640846771.9161832, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5234.126091003418, \"count\": 1, \"min\": 5234.126091003418, \"max\": 5234.126091003418}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.4117782528283 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=59, train loss <loss>=-4.037819477228018\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:11 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:12 INFO 140511108945280] Epoch[60] Batch[0] avg_epoch_loss=-4.246492\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=-4.246492385864258\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:14 INFO 140511108945280] Epoch[60] Batch[5] avg_epoch_loss=-4.096944\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=-4.096943775812785\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:14 INFO 140511108945280] Epoch[60] Batch [5]#011Speed: 172.55 samples/sec#011loss=-4.096944\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] Epoch[60] Batch[10] avg_epoch_loss=-4.108203\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=-4.121715021133423\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] Epoch[60] Batch [10]#011Speed: 164.13 samples/sec#011loss=-4.121715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846771.9162955, \"EndTime\": 1640846776.7072341, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4790.311574935913, \"count\": 1, \"min\": 4790.311574935913, \"max\": 4790.311574935913}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.97910348935122 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=60, train loss <loss>=-4.15693614880244\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:16 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:17 INFO 140511108945280] Epoch[61] Batch[0] avg_epoch_loss=-4.120121\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=-4.120121002197266\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:19 INFO 140511108945280] Epoch[61] Batch[5] avg_epoch_loss=-4.149360\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=-4.149359623591105\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:19 INFO 140511108945280] Epoch[61] Batch [5]#011Speed: 169.21 samples/sec#011loss=-4.149360\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] Epoch[61] Batch[10] avg_epoch_loss=-4.207960\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=-4.278280258178711\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] Epoch[61] Batch [10]#011Speed: 165.50 samples/sec#011loss=-4.278280\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] processed a total of 734 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846776.707327, \"EndTime\": 1640846781.515814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4807.804822921753, \"count\": 1, \"min\": 4807.804822921753, \"max\": 4807.804822921753}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.66409669320595 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=61, train loss <loss>=-4.157056093215942\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:21 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:22 INFO 140511108945280] Epoch[62] Batch[0] avg_epoch_loss=-4.318873\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=-4.318873405456543\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:23 INFO 140511108945280] Epoch[62] Batch[5] avg_epoch_loss=-4.072528\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=-4.072527686754863\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:23 INFO 140511108945280] Epoch[62] Batch [5]#011Speed: 170.83 samples/sec#011loss=-4.072528\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:25 INFO 140511108945280] Epoch[62] Batch[10] avg_epoch_loss=-4.108279\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=-4.151180791854858\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:25 INFO 140511108945280] Epoch[62] Batch [10]#011Speed: 161.31 samples/sec#011loss=-4.151181\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:26 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846781.515911, \"EndTime\": 1640846786.4037206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4887.250185012817, \"count\": 1, \"min\": 4887.250185012817, \"max\": 4887.250185012817}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:26 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.8652536563233 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:26 INFO 140511108945280] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=62, train loss <loss>=-4.114424029986064\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:26 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:26 INFO 140511108945280] Epoch[63] Batch[0] avg_epoch_loss=-4.304457\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=-4.30445671081543\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:28 INFO 140511108945280] Epoch[63] Batch[5] avg_epoch_loss=-4.050496\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=-4.050495823224385\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:28 INFO 140511108945280] Epoch[63] Batch [5]#011Speed: 171.32 samples/sec#011loss=-4.050496\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:30 INFO 140511108945280] Epoch[63] Batch[10] avg_epoch_loss=-4.188061\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=-4.353139114379883\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:30 INFO 140511108945280] Epoch[63] Batch [10]#011Speed: 166.71 samples/sec#011loss=-4.353139\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] processed a total of 729 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846786.403821, \"EndTime\": 1640846791.163053, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4758.5954666137695, \"count\": 1, \"min\": 4758.5954666137695, \"max\": 4758.5954666137695}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.19193574455903 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=63, train loss <loss>=-4.283316830794017\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_86c9d87a-51df-406b-9f8a-72475216305d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846791.1631513, \"EndTime\": 1640846791.269247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 105.41605949401855, \"count\": 1, \"min\": 105.41605949401855, \"max\": 105.41605949401855}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] Epoch[64] Batch[0] avg_epoch_loss=-4.315003\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=-4.315003395080566\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:33 INFO 140511108945280] Epoch[64] Batch[5] avg_epoch_loss=-4.220263\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=-4.220263361930847\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:33 INFO 140511108945280] Epoch[64] Batch [5]#011Speed: 171.76 samples/sec#011loss=-4.220263\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:35 INFO 140511108945280] Epoch[64] Batch[10] avg_epoch_loss=-4.236245\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=-4.255422115325928\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:35 INFO 140511108945280] Epoch[64] Batch [10]#011Speed: 166.62 samples/sec#011loss=-4.255422\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:36 INFO 140511108945280] processed a total of 765 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846791.2693036, \"EndTime\": 1640846796.029263, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4759.886980056763, \"count\": 1, \"min\": 4759.886980056763, \"max\": 4759.886980056763}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:36 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=160.7134569063145 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:36 INFO 140511108945280] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=64, train loss <loss>=-4.2372503479321795\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:36 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:36 INFO 140511108945280] Epoch[65] Batch[0] avg_epoch_loss=-3.760823\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=-3.7608234882354736\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:38 INFO 140511108945280] Epoch[65] Batch[5] avg_epoch_loss=-3.963469\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=-3.963468909263611\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:38 INFO 140511108945280] Epoch[65] Batch [5]#011Speed: 172.04 samples/sec#011loss=-3.963469\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:40 INFO 140511108945280] Epoch[65] Batch[10] avg_epoch_loss=-4.187135\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=-4.455534648895264\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:40 INFO 140511108945280] Epoch[65] Batch [10]#011Speed: 164.70 samples/sec#011loss=-4.455535\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] processed a total of 778 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846796.0293615, \"EndTime\": 1640846801.2042315, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5174.358367919922, \"count\": 1, \"min\": 5174.358367919922, \"max\": 5174.358367919922}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.35280918311037 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=65, train loss <loss>=-4.3037378787994385\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_cca9b383-9f90-4c1e-b546-d652d4e4a8c7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846801.2043278, \"EndTime\": 1640846801.3106897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 105.76868057250977, \"count\": 1, \"min\": 105.76868057250977, \"max\": 105.76868057250977}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] Epoch[66] Batch[0] avg_epoch_loss=-3.882066\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=-3.882066488265991\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:43 INFO 140511108945280] Epoch[66] Batch[5] avg_epoch_loss=-4.001854\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=-4.001853624979655\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:43 INFO 140511108945280] Epoch[66] Batch [5]#011Speed: 171.50 samples/sec#011loss=-4.001854\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:45 INFO 140511108945280] Epoch[66] Batch[10] avg_epoch_loss=-3.996044\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=-3.989072561264038\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:45 INFO 140511108945280] Epoch[66] Batch [10]#011Speed: 162.01 samples/sec#011loss=-3.989073\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:46 INFO 140511108945280] processed a total of 781 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846801.3107457, \"EndTime\": 1640846806.5200846, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5209.265470504761, \"count\": 1, \"min\": 5209.265470504761, \"max\": 5209.265470504761}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:46 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.9203647350476 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:46 INFO 140511108945280] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=66, train loss <loss>=-3.6531532315107493\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:46 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:47 INFO 140511108945280] Epoch[67] Batch[0] avg_epoch_loss=-3.943699\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=-3.9436991214752197\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:49 INFO 140511108945280] Epoch[67] Batch[5] avg_epoch_loss=-4.353057\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=-4.353057185808818\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:49 INFO 140511108945280] Epoch[67] Batch [5]#011Speed: 170.73 samples/sec#011loss=-4.353057\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:50 INFO 140511108945280] Epoch[67] Batch[10] avg_epoch_loss=-4.197991\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=-4.011912631988525\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:50 INFO 140511108945280] Epoch[67] Batch [10]#011Speed: 167.56 samples/sec#011loss=-4.011913\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:51 INFO 140511108945280] processed a total of 717 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846806.5202093, \"EndTime\": 1640846811.322099, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4801.308393478394, \"count\": 1, \"min\": 4801.308393478394, \"max\": 4801.308393478394}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:51 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.3301490175188 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:51 INFO 140511108945280] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=67, train loss <loss>=-4.347811162471771\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:51 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:51 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_67be48a8-8543-498c-878a-be4d53ff464d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846811.3221934, \"EndTime\": 1640846811.4275713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 104.83908653259277, \"count\": 1, \"min\": 104.83908653259277, \"max\": 104.83908653259277}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:52 INFO 140511108945280] Epoch[68] Batch[0] avg_epoch_loss=-4.302362\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=-4.302362442016602\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:53 INFO 140511108945280] Epoch[68] Batch[5] avg_epoch_loss=-4.337329\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=-4.337329188982646\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:53 INFO 140511108945280] Epoch[68] Batch [5]#011Speed: 170.85 samples/sec#011loss=-4.337329\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:55 INFO 140511108945280] Epoch[68] Batch[10] avg_epoch_loss=-4.163280\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=-3.9544201374053953\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:55 INFO 140511108945280] Epoch[68] Batch [10]#011Speed: 165.10 samples/sec#011loss=-3.954420\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:56 INFO 140511108945280] processed a total of 749 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846811.4276683, \"EndTime\": 1640846816.2787097, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4850.970506668091, \"count\": 1, \"min\": 4850.970506668091, \"max\": 4850.970506668091}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:56 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.39791949622045 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:56 INFO 140511108945280] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=68, train loss <loss>=-4.156175057093303\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:56 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:56 INFO 140511108945280] Epoch[69] Batch[0] avg_epoch_loss=-3.917323\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=-3.9173226356506348\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:58 INFO 140511108945280] Epoch[69] Batch[5] avg_epoch_loss=-4.264001\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=-4.264001290003459\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:46:58 INFO 140511108945280] Epoch[69] Batch [5]#011Speed: 172.28 samples/sec#011loss=-4.264001\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:00 INFO 140511108945280] Epoch[69] Batch[10] avg_epoch_loss=-4.087094\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=-3.874805450439453\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:00 INFO 140511108945280] Epoch[69] Batch [10]#011Speed: 166.63 samples/sec#011loss=-3.874805\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:01 INFO 140511108945280] processed a total of 737 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846816.2788038, \"EndTime\": 1640846821.0988736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4819.597005844116, \"count\": 1, \"min\": 4819.597005844116, \"max\": 4819.597005844116}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:01 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.91297732776735 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:01 INFO 140511108945280] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=69, train loss <loss>=-4.166584451993306\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:01 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:01 INFO 140511108945280] Epoch[70] Batch[0] avg_epoch_loss=-3.649621\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=-3.6496214866638184\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:03 INFO 140511108945280] Epoch[70] Batch[5] avg_epoch_loss=-4.165368\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=-4.16536819934845\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:03 INFO 140511108945280] Epoch[70] Batch [5]#011Speed: 165.91 samples/sec#011loss=-4.165368\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:05 INFO 140511108945280] Epoch[70] Batch[10] avg_epoch_loss=-4.244637\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=-4.339758777618409\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:05 INFO 140511108945280] Epoch[70] Batch [10]#011Speed: 150.00 samples/sec#011loss=-4.339759\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:06 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846821.0989697, \"EndTime\": 1640846826.2846363, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5185.091733932495, \"count\": 1, \"min\": 5185.091733932495, \"max\": 5185.091733932495}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:06 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.14884952754943 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:06 INFO 140511108945280] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=70, train loss <loss>=-4.3157374660174055\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:06 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:06 INFO 140511108945280] Epoch[71] Batch[0] avg_epoch_loss=-3.953809\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=-3.9538090229034424\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:08 INFO 140511108945280] Epoch[71] Batch[5] avg_epoch_loss=-3.962137\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=-3.9621372620264688\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:08 INFO 140511108945280] Epoch[71] Batch [5]#011Speed: 170.43 samples/sec#011loss=-3.962137\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:10 INFO 140511108945280] Epoch[71] Batch[10] avg_epoch_loss=-4.052543\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=-4.16103081703186\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:10 INFO 140511108945280] Epoch[71] Batch [10]#011Speed: 167.24 samples/sec#011loss=-4.161031\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:11 INFO 140511108945280] processed a total of 733 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846826.2847319, \"EndTime\": 1640846831.1655886, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4880.349159240723, \"count\": 1, \"min\": 4880.349159240723, \"max\": 4880.349159240723}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:11 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.19067192733613 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:11 INFO 140511108945280] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=71, train loss <loss>=-4.192739804585774\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:11 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:11 INFO 140511108945280] Epoch[72] Batch[0] avg_epoch_loss=-4.047898\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=-4.047898292541504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:13 INFO 140511108945280] Epoch[72] Batch[5] avg_epoch_loss=-4.254846\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=-4.254846175511678\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:13 INFO 140511108945280] Epoch[72] Batch [5]#011Speed: 164.12 samples/sec#011loss=-4.254846\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] Epoch[72] Batch[10] avg_epoch_loss=-4.229737\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=-4.199606847763062\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] Epoch[72] Batch [10]#011Speed: 167.74 samples/sec#011loss=-4.199607\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] processed a total of 740 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846831.1656666, \"EndTime\": 1640846835.988713, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4822.347164154053, \"count\": 1, \"min\": 4822.347164154053, \"max\": 4822.347164154053}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.4461306450978 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=72, train loss <loss>=-4.175702353318532\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:15 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:16 INFO 140511108945280] Epoch[73] Batch[0] avg_epoch_loss=-3.721491\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=-3.7214906215667725\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:18 INFO 140511108945280] Epoch[73] Batch[5] avg_epoch_loss=-4.068831\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=-4.068830529848735\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:18 INFO 140511108945280] Epoch[73] Batch [5]#011Speed: 166.48 samples/sec#011loss=-4.068831\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:20 INFO 140511108945280] Epoch[73] Batch[10] avg_epoch_loss=-4.079919\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=-4.093225145339966\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:20 INFO 140511108945280] Epoch[73] Batch [10]#011Speed: 166.65 samples/sec#011loss=-4.093225\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:21 INFO 140511108945280] processed a total of 774 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846835.9888098, \"EndTime\": 1640846841.1624691, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5173.058032989502, \"count\": 1, \"min\": 5173.058032989502, \"max\": 5173.058032989502}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:21 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.61719234456032 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:21 INFO 140511108945280] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=73, train loss <loss>=-4.019852363146269\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:21 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:21 INFO 140511108945280] Epoch[74] Batch[0] avg_epoch_loss=-4.165540\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=-4.1655402183532715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:23 INFO 140511108945280] Epoch[74] Batch[5] avg_epoch_loss=-4.273293\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=-4.273293177286784\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:23 INFO 140511108945280] Epoch[74] Batch [5]#011Speed: 169.69 samples/sec#011loss=-4.273293\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] Epoch[74] Batch[10] avg_epoch_loss=-4.315702\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=-4.366593074798584\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] Epoch[74] Batch [10]#011Speed: 167.92 samples/sec#011loss=-4.366593\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846841.1625702, \"EndTime\": 1640846845.5404036, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4377.211809158325, \"count\": 1, \"min\": 4377.211809158325, \"max\": 4377.211809158325}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.54409846438347 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=74, train loss <loss>=-4.315702221610329\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:25 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:26 INFO 140511108945280] Epoch[75] Batch[0] avg_epoch_loss=-3.539573\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=-3.5395727157592773\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:28 INFO 140511108945280] Epoch[75] Batch[5] avg_epoch_loss=-4.167927\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=-4.167927026748657\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:28 INFO 140511108945280] Epoch[75] Batch [5]#011Speed: 157.81 samples/sec#011loss=-4.167927\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] Epoch[75] Batch[10] avg_epoch_loss=-4.138170\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=-4.102461576461792\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] Epoch[75] Batch [10]#011Speed: 166.08 samples/sec#011loss=-4.102462\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] processed a total of 767 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846845.540485, \"EndTime\": 1640846850.4839716, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4943.018436431885, \"count\": 1, \"min\": 4943.018436431885, \"max\": 4943.018436431885}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.1640436544526 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=75, train loss <loss>=-4.13627701997757\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:30 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:31 INFO 140511108945280] Epoch[76] Batch[0] avg_epoch_loss=-4.101866\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=-4.101865768432617\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:33 INFO 140511108945280] Epoch[76] Batch[5] avg_epoch_loss=-4.355865\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=-4.355865478515625\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:33 INFO 140511108945280] Epoch[76] Batch [5]#011Speed: 169.56 samples/sec#011loss=-4.355865\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] Epoch[76] Batch[10] avg_epoch_loss=-4.343466\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=-4.328587484359741\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] Epoch[76] Batch [10]#011Speed: 167.42 samples/sec#011loss=-4.328587\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846850.484068, \"EndTime\": 1640846854.918788, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4434.146881103516, \"count\": 1, \"min\": 4434.146881103516, \"max\": 4434.146881103516}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.28255520197055 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=76, train loss <loss>=-4.34346639026295\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:34 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:35 INFO 140511108945280] Epoch[77] Batch[0] avg_epoch_loss=-4.708960\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=-4.708960056304932\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:37 INFO 140511108945280] Epoch[77] Batch[5] avg_epoch_loss=-4.119749\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=-4.1197489102681475\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:37 INFO 140511108945280] Epoch[77] Batch [5]#011Speed: 170.69 samples/sec#011loss=-4.119749\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] Epoch[77] Batch[10] avg_epoch_loss=-4.199144\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=-4.294418239593506\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] Epoch[77] Batch [10]#011Speed: 164.47 samples/sec#011loss=-4.294418\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846854.9188812, \"EndTime\": 1640846859.3489752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4429.51512336731, \"count\": 1, \"min\": 4429.51512336731, \"max\": 4429.51512336731}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.12340140244598 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=77, train loss <loss>=-4.199144059961492\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] Epoch[78] Batch[0] avg_epoch_loss=-3.930783\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=-3.9307825565338135\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:41 INFO 140511108945280] Epoch[78] Batch[5] avg_epoch_loss=-4.200284\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=-4.200283606847127\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:41 INFO 140511108945280] Epoch[78] Batch [5]#011Speed: 170.47 samples/sec#011loss=-4.200284\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:43 INFO 140511108945280] Epoch[78] Batch[10] avg_epoch_loss=-4.069780\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=-3.913176393508911\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:43 INFO 140511108945280] Epoch[78] Batch [10]#011Speed: 164.95 samples/sec#011loss=-3.913176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:44 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846859.3490572, \"EndTime\": 1640846864.1344533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4784.7959995269775, \"count\": 1, \"min\": 4784.7959995269775, \"max\": 4784.7959995269775}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:44 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.11505121562303 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:44 INFO 140511108945280] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=78, train loss <loss>=-4.139486412207286\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:44 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:44 INFO 140511108945280] Epoch[79] Batch[0] avg_epoch_loss=-4.278588\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=-4.27858829498291\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:46 INFO 140511108945280] Epoch[79] Batch[5] avg_epoch_loss=-4.246499\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=-4.246499141057332\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:46 INFO 140511108945280] Epoch[79] Batch [5]#011Speed: 170.41 samples/sec#011loss=-4.246499\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] Epoch[79] Batch[10] avg_epoch_loss=-4.319387\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=-4.406853151321411\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] Epoch[79] Batch [10]#011Speed: 165.22 samples/sec#011loss=-4.406853\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846864.134545, \"EndTime\": 1640846868.9267757, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4791.654586791992, \"count\": 1, \"min\": 4791.654586791992, \"max\": 4791.654586791992}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.89201944952714 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=79, train loss <loss>=-4.154329637686412\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:48 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:49 INFO 140511108945280] Epoch[80] Batch[0] avg_epoch_loss=-4.476250\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=-4.476249694824219\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:51 INFO 140511108945280] Epoch[80] Batch[5] avg_epoch_loss=-4.277016\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=-4.277016480763753\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:51 INFO 140511108945280] Epoch[80] Batch [5]#011Speed: 169.25 samples/sec#011loss=-4.277016\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] Epoch[80] Batch[10] avg_epoch_loss=-4.264339\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=-4.249126148223877\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] Epoch[80] Batch [10]#011Speed: 166.19 samples/sec#011loss=-4.249126\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846868.9268653, \"EndTime\": 1640846873.3360422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4408.593416213989, \"count\": 1, \"min\": 4408.593416213989, \"max\": 4408.593416213989}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.4145459876622 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=80, train loss <loss>=-4.264339056881991\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] Epoch[81] Batch[0] avg_epoch_loss=-4.116987\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=-4.116987228393555\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:55 INFO 140511108945280] Epoch[81] Batch[5] avg_epoch_loss=-4.170274\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=-4.170274178187053\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:55 INFO 140511108945280] Epoch[81] Batch [5]#011Speed: 170.03 samples/sec#011loss=-4.170274\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:57 INFO 140511108945280] Epoch[81] Batch[10] avg_epoch_loss=-4.260245\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=-4.368210411071777\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:57 INFO 140511108945280] Epoch[81] Batch [10]#011Speed: 160.94 samples/sec#011loss=-4.368210\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:58 INFO 140511108945280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846873.336151, \"EndTime\": 1640846878.1908941, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4854.257583618164, \"count\": 1, \"min\": 4854.257583618164, \"max\": 4854.257583618164}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:58 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.14359000930958 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:58 INFO 140511108945280] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=81, train loss <loss>=-4.22832848628362\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:58 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:58 INFO 140511108945280] Epoch[82] Batch[0] avg_epoch_loss=-4.097119\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:47:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=-4.097118854522705\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:00 INFO 140511108945280] Epoch[82] Batch[5] avg_epoch_loss=-4.193225\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=-4.193224827448527\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:00 INFO 140511108945280] Epoch[82] Batch [5]#011Speed: 168.69 samples/sec#011loss=-4.193225\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:02 INFO 140511108945280] Epoch[82] Batch[10] avg_epoch_loss=-4.203193\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=-4.215154647827148\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:02 INFO 140511108945280] Epoch[82] Batch [10]#011Speed: 157.28 samples/sec#011loss=-4.215155\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] processed a total of 722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846878.190978, \"EndTime\": 1640846883.120946, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4929.395914077759, \"count\": 1, \"min\": 4929.395914077759, \"max\": 4929.395914077759}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=146.4641072704207 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=82, train loss <loss>=-4.407394051551819\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_93ce6fe0-5c51-498a-babc-98cbb5f18811-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846883.1210437, \"EndTime\": 1640846883.2290435, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 107.41376876831055, \"count\": 1, \"min\": 107.41376876831055, \"max\": 107.41376876831055}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] Epoch[83] Batch[0] avg_epoch_loss=-3.660547\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=-3.6605470180511475\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:06 INFO 140511108945280] Epoch[83] Batch[5] avg_epoch_loss=-4.235795\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=-4.23579474290212\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:06 INFO 140511108945280] Epoch[83] Batch [5]#011Speed: 141.92 samples/sec#011loss=-4.235795\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] Epoch[83] Batch[10] avg_epoch_loss=-4.315640\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=-4.411454582214356\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] Epoch[83] Batch [10]#011Speed: 150.05 samples/sec#011loss=-4.411455\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846883.229124, \"EndTime\": 1640846888.6083033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5379.1046142578125, \"count\": 1, \"min\": 5379.1046142578125, \"max\": 5379.1046142578125}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=134.59145500859492 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=83, train loss <loss>=-4.239540159702301\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:08 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:09 INFO 140511108945280] Epoch[84] Batch[0] avg_epoch_loss=-4.154793\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=-4.1547932624816895\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:11 INFO 140511108945280] Epoch[84] Batch[5] avg_epoch_loss=-4.320430\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=-4.320429881413777\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:11 INFO 140511108945280] Epoch[84] Batch [5]#011Speed: 171.41 samples/sec#011loss=-4.320430\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] Epoch[84] Batch[10] avg_epoch_loss=-4.265061\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=-4.198619318008423\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] Epoch[84] Batch [10]#011Speed: 165.14 samples/sec#011loss=-4.198619\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] processed a total of 746 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846888.6083999, \"EndTime\": 1640846893.3978806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4788.9697551727295, \"count\": 1, \"min\": 4788.9697551727295, \"max\": 4788.9697551727295}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.77044402244545 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=84, train loss <loss>=-4.309962173302968\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:13 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:14 INFO 140511108945280] Epoch[85] Batch[0] avg_epoch_loss=-4.707465\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=-4.707465171813965\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:15 INFO 140511108945280] Epoch[85] Batch[5] avg_epoch_loss=-4.408534\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=-4.4085341691970825\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:15 INFO 140511108945280] Epoch[85] Batch [5]#011Speed: 171.49 samples/sec#011loss=-4.408534\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:17 INFO 140511108945280] Epoch[85] Batch[10] avg_epoch_loss=-4.283451\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=-4.133351278305054\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:17 INFO 140511108945280] Epoch[85] Batch [10]#011Speed: 166.54 samples/sec#011loss=-4.133351\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:18 INFO 140511108945280] processed a total of 741 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846893.3979733, \"EndTime\": 1640846898.2123632, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4813.907146453857, \"count\": 1, \"min\": 4813.907146453857, \"max\": 4813.907146453857}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.92458120645585 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=85, train loss <loss>=-4.238649765650432\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:18 INFO 140511108945280] Epoch[86] Batch[0] avg_epoch_loss=-3.688766\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=-3.68876576423645\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:20 INFO 140511108945280] Epoch[86] Batch[5] avg_epoch_loss=-4.226057\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=-4.226056694984436\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:20 INFO 140511108945280] Epoch[86] Batch [5]#011Speed: 167.76 samples/sec#011loss=-4.226057\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] Epoch[86] Batch[10] avg_epoch_loss=-4.212401\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=-4.1960142135620115\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] Epoch[86] Batch [10]#011Speed: 168.57 samples/sec#011loss=-4.196014\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] processed a total of 737 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846898.212463, \"EndTime\": 1640846902.9926665, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4779.67095375061, \"count\": 1, \"min\": 4779.67095375061, \"max\": 4779.67095375061}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.1903616223477 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=86, train loss <loss>=-3.9389295081297555\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:22 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:23 INFO 140511108945280] Epoch[87] Batch[0] avg_epoch_loss=-4.223327\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=-4.223327159881592\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:25 INFO 140511108945280] Epoch[87] Batch[5] avg_epoch_loss=-4.234286\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=-4.234285791714986\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:25 INFO 140511108945280] Epoch[87] Batch [5]#011Speed: 171.48 samples/sec#011loss=-4.234286\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] Epoch[87] Batch[10] avg_epoch_loss=-4.289268\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=-4.355246257781983\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] Epoch[87] Batch [10]#011Speed: 163.05 samples/sec#011loss=-4.355246\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] processed a total of 711 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846902.9927628, \"EndTime\": 1640846907.9251838, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4931.901693344116, \"count\": 1, \"min\": 4931.901693344116, \"max\": 4931.901693344116}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=144.15947121371667 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=87, train loss <loss>=-4.423954904079437\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:27 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:28 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_3273f8b7-3e97-481e-a4c1-0fcd73da9a7b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846907.9252808, \"EndTime\": 1640846908.0400577, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 114.20106887817383, \"count\": 1, \"min\": 114.20106887817383, \"max\": 114.20106887817383}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:28 INFO 140511108945280] Epoch[88] Batch[0] avg_epoch_loss=-4.196176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=-4.196176052093506\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:30 INFO 140511108945280] Epoch[88] Batch[5] avg_epoch_loss=-4.362030\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=-4.3620303471883135\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:30 INFO 140511108945280] Epoch[88] Batch [5]#011Speed: 166.80 samples/sec#011loss=-4.362030\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:32 INFO 140511108945280] Epoch[88] Batch[10] avg_epoch_loss=-4.436176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=-4.5251518249511715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:32 INFO 140511108945280] Epoch[88] Batch [10]#011Speed: 163.05 samples/sec#011loss=-4.525152\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:33 INFO 140511108945280] processed a total of 811 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846908.04014, \"EndTime\": 1640846913.2905974, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5250.380277633667, \"count\": 1, \"min\": 5250.380277633667, \"max\": 5250.380277633667}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:33 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.46117932802102 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:33 INFO 140511108945280] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=88, train loss <loss>=-4.26743142421429\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:33 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:33 INFO 140511108945280] Epoch[89] Batch[0] avg_epoch_loss=-4.034919\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=-4.034918785095215\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:35 INFO 140511108945280] Epoch[89] Batch[5] avg_epoch_loss=-4.096480\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=-4.096479574839274\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:35 INFO 140511108945280] Epoch[89] Batch [5]#011Speed: 171.08 samples/sec#011loss=-4.096480\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:37 INFO 140511108945280] Epoch[89] Batch[10] avg_epoch_loss=-4.162827\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=-4.242443943023682\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:37 INFO 140511108945280] Epoch[89] Batch [10]#011Speed: 166.40 samples/sec#011loss=-4.242444\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:38 INFO 140511108945280] processed a total of 785 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846913.2906852, \"EndTime\": 1640846918.4687538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5177.334308624268, \"count\": 1, \"min\": 5177.334308624268, \"max\": 5177.334308624268}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:38 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.6184345414228 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:38 INFO 140511108945280] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=89, train loss <loss>=-4.093151496006892\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:38 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:39 INFO 140511108945280] Epoch[90] Batch[0] avg_epoch_loss=-4.837161\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=-4.837161064147949\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:40 INFO 140511108945280] Epoch[90] Batch[5] avg_epoch_loss=-4.614514\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=-4.614514430363973\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:40 INFO 140511108945280] Epoch[90] Batch [5]#011Speed: 170.30 samples/sec#011loss=-4.614514\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:42 INFO 140511108945280] Epoch[90] Batch[10] avg_epoch_loss=-4.484471\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=-4.328419494628906\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:42 INFO 140511108945280] Epoch[90] Batch [10]#011Speed: 162.60 samples/sec#011loss=-4.328419\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:43 INFO 140511108945280] processed a total of 787 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846918.4688394, \"EndTime\": 1640846923.663478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5193.984746932983, \"count\": 1, \"min\": 5193.984746932983, \"max\": 5193.984746932983}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:43 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.51738982192566 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:43 INFO 140511108945280] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=90, train loss <loss>=-4.557507790051973\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:43 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:43 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_5d33491c-71a1-4a8d-b811-0a7951137d51-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846923.6635773, \"EndTime\": 1640846923.7734098, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 109.22408103942871, \"count\": 1, \"min\": 109.22408103942871, \"max\": 109.22408103942871}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:44 INFO 140511108945280] Epoch[91] Batch[0] avg_epoch_loss=-4.765137\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=-4.76513671875\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:46 INFO 140511108945280] Epoch[91] Batch[5] avg_epoch_loss=-4.134697\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=-4.134697238604228\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:46 INFO 140511108945280] Epoch[91] Batch [5]#011Speed: 169.84 samples/sec#011loss=-4.134697\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] Epoch[91] Batch[10] avg_epoch_loss=-4.283699\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=-4.4625016212463375\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] Epoch[91] Batch [10]#011Speed: 168.65 samples/sec#011loss=-4.462502\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] processed a total of 718 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846923.773505, \"EndTime\": 1640846928.562519, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4788.95115852356, \"count\": 1, \"min\": 4788.95115852356, \"max\": 4788.95115852356}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.92528016366617 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=91, train loss <loss>=-4.268448015054067\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:48 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:49 INFO 140511108945280] Epoch[92] Batch[0] avg_epoch_loss=-4.369412\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=-4.369412422180176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:51 INFO 140511108945280] Epoch[92] Batch[5] avg_epoch_loss=-4.220377\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=-4.2203770478566485\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:51 INFO 140511108945280] Epoch[92] Batch [5]#011Speed: 170.48 samples/sec#011loss=-4.220377\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:52 INFO 140511108945280] Epoch[92] Batch[10] avg_epoch_loss=-4.271749\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=-4.3333944320678714\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:52 INFO 140511108945280] Epoch[92] Batch [10]#011Speed: 164.10 samples/sec#011loss=-4.333394\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:53 INFO 140511108945280] processed a total of 786 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846928.5625882, \"EndTime\": 1640846933.7752788, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5212.137699127197, \"count\": 1, \"min\": 5212.137699127197, \"max\": 5212.137699127197}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:53 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.79815126147284 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:53 INFO 140511108945280] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=92, train loss <loss>=-4.215381255516639\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:53 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:54 INFO 140511108945280] Epoch[93] Batch[0] avg_epoch_loss=-4.376364\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=-4.376363754272461\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:56 INFO 140511108945280] Epoch[93] Batch[5] avg_epoch_loss=-4.223166\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=-4.2231660683949785\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:56 INFO 140511108945280] Epoch[93] Batch [5]#011Speed: 172.36 samples/sec#011loss=-4.223166\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] Epoch[93] Batch[10] avg_epoch_loss=-4.340422\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=-4.481128454208374\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] Epoch[93] Batch [10]#011Speed: 160.91 samples/sec#011loss=-4.481128\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846933.775367, \"EndTime\": 1640846938.6137638, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4837.857484817505, \"count\": 1, \"min\": 4837.857484817505, \"max\": 4837.857484817505}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.16846712053655 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=93, train loss <loss>=-3.9406194984912872\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:58 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:59 INFO 140511108945280] Epoch[94] Batch[0] avg_epoch_loss=-3.822218\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:48:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=-3.822218179702759\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:01 INFO 140511108945280] Epoch[94] Batch[5] avg_epoch_loss=-4.248577\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=-4.2485766013463335\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:01 INFO 140511108945280] Epoch[94] Batch [5]#011Speed: 170.36 samples/sec#011loss=-4.248577\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] Epoch[94] Batch[10] avg_epoch_loss=-4.355985\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=-4.484875202178955\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] Epoch[94] Batch [10]#011Speed: 158.20 samples/sec#011loss=-4.484875\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846938.6138594, \"EndTime\": 1640846943.5575314, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4943.132400512695, \"count\": 1, \"min\": 4943.132400512695, \"max\": 4943.132400512695}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=146.46280856209594 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=94, train loss <loss>=-4.068425253033638\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:03 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:04 INFO 140511108945280] Epoch[95] Batch[0] avg_epoch_loss=-4.026098\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=-4.026097774505615\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:06 INFO 140511108945280] Epoch[95] Batch[5] avg_epoch_loss=-4.285212\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=-4.285212238629659\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:06 INFO 140511108945280] Epoch[95] Batch [5]#011Speed: 142.94 samples/sec#011loss=-4.285212\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] Epoch[95] Batch[10] avg_epoch_loss=-4.304340\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=-4.327293109893799\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] Epoch[95] Batch [10]#011Speed: 162.23 samples/sec#011loss=-4.327293\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] processed a total of 720 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846943.5576015, \"EndTime\": 1640846948.7652426, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5207.01003074646, \"count\": 1, \"min\": 5207.01003074646, \"max\": 5207.01003074646}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=138.2693202017646 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=95, train loss <loss>=-4.399315377076467\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:08 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:09 INFO 140511108945280] Epoch[96] Batch[0] avg_epoch_loss=-4.786738\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=-4.786738395690918\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:11 INFO 140511108945280] Epoch[96] Batch[5] avg_epoch_loss=-4.295927\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=-4.295927127202352\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:11 INFO 140511108945280] Epoch[96] Batch [5]#011Speed: 170.66 samples/sec#011loss=-4.295927\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] Epoch[96] Batch[10] avg_epoch_loss=-4.350433\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=-4.415839815139771\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] Epoch[96] Batch [10]#011Speed: 167.15 samples/sec#011loss=-4.415840\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] processed a total of 774 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846948.7654164, \"EndTime\": 1640846953.9367235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5170.411825180054, \"count\": 1, \"min\": 5170.411825180054, \"max\": 5170.411825180054}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.69431623027532 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=96, train loss <loss>=-4.4684642645028925\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:13 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:14 INFO 140511108945280] Epoch[97] Batch[0] avg_epoch_loss=-4.431488\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=-4.431487560272217\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:16 INFO 140511108945280] Epoch[97] Batch[5] avg_epoch_loss=-4.257990\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=-4.257989724477132\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:16 INFO 140511108945280] Epoch[97] Batch [5]#011Speed: 160.61 samples/sec#011loss=-4.257990\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] Epoch[97] Batch[10] avg_epoch_loss=-4.447434\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=-4.6747664451599125\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] Epoch[97] Batch [10]#011Speed: 160.54 samples/sec#011loss=-4.674766\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] processed a total of 737 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846953.93681, \"EndTime\": 1640846958.906253, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4968.958854675293, \"count\": 1, \"min\": 4968.958854675293, \"max\": 4968.958854675293}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.31675317032128 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=97, train loss <loss>=-4.4546042283376055\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:19 INFO 140511108945280] Epoch[98] Batch[0] avg_epoch_loss=-4.799637\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=-4.799637317657471\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:21 INFO 140511108945280] Epoch[98] Batch[5] avg_epoch_loss=-4.438570\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=-4.438570261001587\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:21 INFO 140511108945280] Epoch[98] Batch [5]#011Speed: 169.94 samples/sec#011loss=-4.438570\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] Epoch[98] Batch[10] avg_epoch_loss=-4.482870\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=-4.536030292510986\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] Epoch[98] Batch [10]#011Speed: 166.83 samples/sec#011loss=-4.536030\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] processed a total of 741 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846958.9063497, \"EndTime\": 1640846963.7108436, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4803.94172668457, \"count\": 1, \"min\": 4803.94172668457, \"max\": 4803.94172668457}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.2442521672132 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=98, train loss <loss>=-4.349804878234863\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:23 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:24 INFO 140511108945280] Epoch[99] Batch[0] avg_epoch_loss=-4.849669\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=-4.849669456481934\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:26 INFO 140511108945280] Epoch[99] Batch[5] avg_epoch_loss=-4.746807\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=-4.746806780497233\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:26 INFO 140511108945280] Epoch[99] Batch [5]#011Speed: 170.46 samples/sec#011loss=-4.746807\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] Epoch[99] Batch[10] avg_epoch_loss=-4.514860\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=-4.2365234851837155\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] Epoch[99] Batch [10]#011Speed: 163.99 samples/sec#011loss=-4.236523\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] processed a total of 780 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846963.710931, \"EndTime\": 1640846968.9564428, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5244.896173477173, \"count\": 1, \"min\": 5244.896173477173, \"max\": 5244.896173477173}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.7131309015445 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=99, train loss <loss>=-4.461127207829402\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:28 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:29 INFO 140511108945280] Epoch[100] Batch[0] avg_epoch_loss=-4.384639\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=-4.384638786315918\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:31 INFO 140511108945280] Epoch[100] Batch[5] avg_epoch_loss=-4.292740\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=-4.292739510536194\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:31 INFO 140511108945280] Epoch[100] Batch [5]#011Speed: 169.16 samples/sec#011loss=-4.292740\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] Epoch[100] Batch[10] avg_epoch_loss=-4.231601\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=-4.158233976364135\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] Epoch[100] Batch [10]#011Speed: 170.65 samples/sec#011loss=-4.158234\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846968.9565096, \"EndTime\": 1640846973.3299787, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4372.873783111572, \"count\": 1, \"min\": 4372.873783111572, \"max\": 4372.873783111572}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.64319388967408 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=100, train loss <loss>=-4.231600631367076\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] Epoch[101] Batch[0] avg_epoch_loss=-5.063204\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=-5.063203811645508\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:35 INFO 140511108945280] Epoch[101] Batch[5] avg_epoch_loss=-4.504063\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=-4.504062652587891\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:35 INFO 140511108945280] Epoch[101] Batch [5]#011Speed: 168.05 samples/sec#011loss=-4.504063\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:37 INFO 140511108945280] Epoch[101] Batch[10] avg_epoch_loss=-4.460199\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=-4.407562637329102\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:37 INFO 140511108945280] Epoch[101] Batch [10]#011Speed: 167.14 samples/sec#011loss=-4.407563\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:38 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846973.33006, \"EndTime\": 1640846978.1018813, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4771.262168884277, \"count\": 1, \"min\": 4771.262168884277, \"max\": 4771.262168884277}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:38 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=159.91103416003276 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:38 INFO 140511108945280] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=101, train loss <loss>=-4.372603138287862\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:38 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:38 INFO 140511108945280] Epoch[102] Batch[0] avg_epoch_loss=-4.567161\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=-4.5671610832214355\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:40 INFO 140511108945280] Epoch[102] Batch[5] avg_epoch_loss=-4.272341\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=-4.272341211636861\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:40 INFO 140511108945280] Epoch[102] Batch [5]#011Speed: 171.18 samples/sec#011loss=-4.272341\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:42 INFO 140511108945280] Epoch[102] Batch[10] avg_epoch_loss=-4.204392\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=-4.1228536605834964\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:42 INFO 140511108945280] Epoch[102] Batch [10]#011Speed: 164.27 samples/sec#011loss=-4.122854\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:43 INFO 140511108945280] processed a total of 814 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846978.101981, \"EndTime\": 1640846983.2424893, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5139.960527420044, \"count\": 1, \"min\": 5139.960527420044, \"max\": 5139.960527420044}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:43 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.36278788097496 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:43 INFO 140511108945280] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=102, train loss <loss>=-4.234343620447012\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:43 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:43 INFO 140511108945280] Epoch[103] Batch[0] avg_epoch_loss=-4.143275\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=-4.143275260925293\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:45 INFO 140511108945280] Epoch[103] Batch[5] avg_epoch_loss=-4.289971\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=-4.289971431096395\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:45 INFO 140511108945280] Epoch[103] Batch [5]#011Speed: 168.86 samples/sec#011loss=-4.289971\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:47 INFO 140511108945280] Epoch[103] Batch[10] avg_epoch_loss=-4.276363\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=-4.260032272338867\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:47 INFO 140511108945280] Epoch[103] Batch [10]#011Speed: 165.85 samples/sec#011loss=-4.260032\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:48 INFO 140511108945280] processed a total of 753 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846983.2425854, \"EndTime\": 1640846988.0947495, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4851.6528606414795, \"count\": 1, \"min\": 4851.6528606414795, \"max\": 4851.6528606414795}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:48 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.2006211344943 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:48 INFO 140511108945280] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=103, train loss <loss>=-4.332011381785075\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:48 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:48 INFO 140511108945280] Epoch[104] Batch[0] avg_epoch_loss=-4.222309\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=-4.222309112548828\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:50 INFO 140511108945280] Epoch[104] Batch[5] avg_epoch_loss=-4.190750\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=-4.190749764442444\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:50 INFO 140511108945280] Epoch[104] Batch [5]#011Speed: 169.71 samples/sec#011loss=-4.190750\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:52 INFO 140511108945280] Epoch[104] Batch[10] avg_epoch_loss=-4.266094\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=-4.35650668144226\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:52 INFO 140511108945280] Epoch[104] Batch [10]#011Speed: 166.34 samples/sec#011loss=-4.356507\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:53 INFO 140511108945280] processed a total of 774 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846988.0948436, \"EndTime\": 1640846993.2764397, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5181.117534637451, \"count\": 1, \"min\": 5181.117534637451, \"max\": 5181.117534637451}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:53 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.3856574751262 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:53 INFO 140511108945280] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=104, train loss <loss>=-4.211524083064153\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:53 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:53 INFO 140511108945280] Epoch[105] Batch[0] avg_epoch_loss=-4.247344\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=-4.24734354019165\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:55 INFO 140511108945280] Epoch[105] Batch[5] avg_epoch_loss=-4.422175\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=-4.422175327936809\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:55 INFO 140511108945280] Epoch[105] Batch [5]#011Speed: 167.86 samples/sec#011loss=-4.422175\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:57 INFO 140511108945280] Epoch[105] Batch[10] avg_epoch_loss=-4.400740\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=-4.375016784667968\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:57 INFO 140511108945280] Epoch[105] Batch [10]#011Speed: 167.38 samples/sec#011loss=-4.375017\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:58 INFO 140511108945280] processed a total of 710 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846993.2765112, \"EndTime\": 1640846998.0781724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4801.064014434814, \"count\": 1, \"min\": 4801.064014434814, \"max\": 4801.064014434814}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:58 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.8796333305376 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:58 INFO 140511108945280] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=105, train loss <loss>=-4.367996136347453\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:58 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:58 INFO 140511108945280] Epoch[106] Batch[0] avg_epoch_loss=-4.546947\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:49:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=-4.546947002410889\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:00 INFO 140511108945280] Epoch[106] Batch[5] avg_epoch_loss=-4.389429\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=-4.389428536097209\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:00 INFO 140511108945280] Epoch[106] Batch [5]#011Speed: 164.17 samples/sec#011loss=-4.389429\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:02 INFO 140511108945280] Epoch[106] Batch[10] avg_epoch_loss=-4.352850\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=-4.308955955505371\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:02 INFO 140511108945280] Epoch[106] Batch [10]#011Speed: 159.30 samples/sec#011loss=-4.308956\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:03 INFO 140511108945280] processed a total of 753 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640846998.0782702, \"EndTime\": 1640847003.0437224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4964.900732040405, \"count\": 1, \"min\": 4964.900732040405, \"max\": 4964.900732040405}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:03 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.66048337284298 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:03 INFO 140511108945280] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=106, train loss <loss>=-4.367336829503377\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:03 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:03 INFO 140511108945280] Epoch[107] Batch[0] avg_epoch_loss=-4.078859\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=-4.078859329223633\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:05 INFO 140511108945280] Epoch[107] Batch[5] avg_epoch_loss=-4.267691\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=-4.267691294352214\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:05 INFO 140511108945280] Epoch[107] Batch [5]#011Speed: 156.14 samples/sec#011loss=-4.267691\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:07 INFO 140511108945280] Epoch[107] Batch[10] avg_epoch_loss=-4.208504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=-4.137479686737061\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:07 INFO 140511108945280] Epoch[107] Batch [10]#011Speed: 144.04 samples/sec#011loss=-4.137480\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:08 INFO 140511108945280] processed a total of 725 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847003.0438206, \"EndTime\": 1640847008.2945144, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5250.131130218506, \"count\": 1, \"min\": 5250.131130218506, \"max\": 5250.131130218506}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=138.08813931720792 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=107, train loss <loss>=-4.007108290990193\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:08 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:08 INFO 140511108945280] Epoch[108] Batch[0] avg_epoch_loss=-4.340449\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=-4.340449333190918\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:10 INFO 140511108945280] Epoch[108] Batch[5] avg_epoch_loss=-4.226832\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=-4.226831912994385\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:10 INFO 140511108945280] Epoch[108] Batch [5]#011Speed: 170.06 samples/sec#011loss=-4.226832\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:12 INFO 140511108945280] Epoch[108] Batch[10] avg_epoch_loss=-4.255287\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=-4.289432764053345\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:12 INFO 140511108945280] Epoch[108] Batch [10]#011Speed: 163.07 samples/sec#011loss=-4.289433\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:13 INFO 140511108945280] processed a total of 761 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847008.294609, \"EndTime\": 1640847013.1209705, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4825.839042663574, \"count\": 1, \"min\": 4825.839042663574, \"max\": 4825.839042663574}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:13 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.68837585088374 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:13 INFO 140511108945280] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=108, train loss <loss>=-4.267816364765167\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:13 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:13 INFO 140511108945280] Epoch[109] Batch[0] avg_epoch_loss=-4.290187\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=-4.290187358856201\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:15 INFO 140511108945280] Epoch[109] Batch[5] avg_epoch_loss=-4.610650\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=-4.610650459925334\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:15 INFO 140511108945280] Epoch[109] Batch [5]#011Speed: 171.54 samples/sec#011loss=-4.610650\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:17 INFO 140511108945280] Epoch[109] Batch[10] avg_epoch_loss=-4.476963\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=-4.316539001464844\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:17 INFO 140511108945280] Epoch[109] Batch [10]#011Speed: 166.12 samples/sec#011loss=-4.316539\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] processed a total of 781 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847013.121067, \"EndTime\": 1640847018.2849352, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5163.364887237549, \"count\": 1, \"min\": 5163.364887237549, \"max\": 5163.364887237549}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.2540062843513 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=109, train loss <loss>=-4.580869711362398\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_865405a6-4d51-4f68-b8ff-8182286355b5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847018.2850313, \"EndTime\": 1640847018.3933408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 107.72490501403809, \"count\": 1, \"min\": 107.72490501403809, \"max\": 107.72490501403809}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] Epoch[110] Batch[0] avg_epoch_loss=-4.626978\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=-4.626977920532227\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:20 INFO 140511108945280] Epoch[110] Batch[5] avg_epoch_loss=-4.479432\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=-4.4794321457544966\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:20 INFO 140511108945280] Epoch[110] Batch [5]#011Speed: 169.32 samples/sec#011loss=-4.479432\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:22 INFO 140511108945280] Epoch[110] Batch[10] avg_epoch_loss=-4.544464\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=-4.622501468658447\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:22 INFO 140511108945280] Epoch[110] Batch [10]#011Speed: 166.23 samples/sec#011loss=-4.622501\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847018.3933978, \"EndTime\": 1640847023.1765337, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4783.074617385864, \"count\": 1, \"min\": 4783.074617385864, \"max\": 4783.074617385864}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.1713584524469 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=110, train loss <loss>=-4.5858383774757385\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_083cb12d-10a9-417b-b06b-d1615f41a5a3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847023.176629, \"EndTime\": 1640847023.3056927, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 128.51405143737793, \"count\": 1, \"min\": 128.51405143737793, \"max\": 128.51405143737793}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] Epoch[111] Batch[0] avg_epoch_loss=-4.931457\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=-4.931456565856934\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:25 INFO 140511108945280] Epoch[111] Batch[5] avg_epoch_loss=-4.416714\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=-4.41671363512675\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:25 INFO 140511108945280] Epoch[111] Batch [5]#011Speed: 171.03 samples/sec#011loss=-4.416714\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] Epoch[111] Batch[10] avg_epoch_loss=-4.430295\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=-4.446593189239502\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] Epoch[111] Batch [10]#011Speed: 168.46 samples/sec#011loss=-4.446593\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847023.3057528, \"EndTime\": 1640847027.6715724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4365.761518478394, \"count\": 1, \"min\": 4365.761518478394, \"max\": 4365.761518478394}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=161.02118972256707 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=111, train loss <loss>=-4.430295250632546\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:27 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:28 INFO 140511108945280] Epoch[112] Batch[0] avg_epoch_loss=-4.020068\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=-4.020068168640137\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:30 INFO 140511108945280] Epoch[112] Batch[5] avg_epoch_loss=-4.437173\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=-4.437172571818034\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:30 INFO 140511108945280] Epoch[112] Batch [5]#011Speed: 164.84 samples/sec#011loss=-4.437173\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] Epoch[112] Batch[10] avg_epoch_loss=-4.490250\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=-4.553942775726318\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] Epoch[112] Batch [10]#011Speed: 165.58 samples/sec#011loss=-4.553943\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] processed a total of 748 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847027.6716592, \"EndTime\": 1640847032.5057623, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4833.508014678955, \"count\": 1, \"min\": 4833.508014678955, \"max\": 4833.508014678955}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.74886005291177 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] #progress_metric: host=algo-1, completed 28.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=112, train loss <loss>=-4.581551750500997\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:32 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:33 INFO 140511108945280] Epoch[113] Batch[0] avg_epoch_loss=-4.207789\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=-4.207789421081543\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:34 INFO 140511108945280] Epoch[113] Batch[5] avg_epoch_loss=-4.537920\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=-4.537919759750366\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:34 INFO 140511108945280] Epoch[113] Batch [5]#011Speed: 171.82 samples/sec#011loss=-4.537920\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:36 INFO 140511108945280] Epoch[113] Batch[10] avg_epoch_loss=-4.320335\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=-4.059232473373413\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:36 INFO 140511108945280] Epoch[113] Batch [10]#011Speed: 165.52 samples/sec#011loss=-4.059232\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:37 INFO 140511108945280] processed a total of 777 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847032.505854, \"EndTime\": 1640847037.6387541, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5132.402420043945, \"count\": 1, \"min\": 5132.402420043945, \"max\": 5132.402420043945}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:37 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.38700477906403 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:37 INFO 140511108945280] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=113, train loss <loss>=-4.5531388612893915\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:37 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:38 INFO 140511108945280] Epoch[114] Batch[0] avg_epoch_loss=-3.959429\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=-3.9594292640686035\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:40 INFO 140511108945280] Epoch[114] Batch[5] avg_epoch_loss=-4.269348\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=-4.269348343213399\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:40 INFO 140511108945280] Epoch[114] Batch [5]#011Speed: 170.88 samples/sec#011loss=-4.269348\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] Epoch[114] Batch[10] avg_epoch_loss=-4.324865\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=-4.39148530960083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] Epoch[114] Batch [10]#011Speed: 166.96 samples/sec#011loss=-4.391485\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] processed a total of 746 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847037.638853, \"EndTime\": 1640847042.4543352, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4814.962863922119, \"count\": 1, \"min\": 4814.962863922119, \"max\": 4814.962863922119}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.92933244609438 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] #progress_metric: host=algo-1, completed 28.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=114, train loss <loss>=-4.385342180728912\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:42 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:43 INFO 140511108945280] Epoch[115] Batch[0] avg_epoch_loss=-4.910809\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=-4.910808563232422\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:44 INFO 140511108945280] Epoch[115] Batch[5] avg_epoch_loss=-4.402417\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=-4.4024174610773725\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:44 INFO 140511108945280] Epoch[115] Batch [5]#011Speed: 172.49 samples/sec#011loss=-4.402417\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:46 INFO 140511108945280] Epoch[115] Batch[10] avg_epoch_loss=-4.427893\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=-4.4584641456604\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:46 INFO 140511108945280] Epoch[115] Batch [10]#011Speed: 167.20 samples/sec#011loss=-4.458464\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:47 INFO 140511108945280] processed a total of 739 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847042.4544318, \"EndTime\": 1640847047.1998177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4744.847536087036, \"count\": 1, \"min\": 4744.847536087036, \"max\": 4744.847536087036}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:47 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.74336836296774 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:47 INFO 140511108945280] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=115, train loss <loss>=-4.498567720254262\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:47 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:47 INFO 140511108945280] Epoch[116] Batch[0] avg_epoch_loss=-4.363422\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=-4.36342191696167\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:49 INFO 140511108945280] Epoch[116] Batch[5] avg_epoch_loss=-4.255745\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=-4.25574517250061\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:49 INFO 140511108945280] Epoch[116] Batch [5]#011Speed: 171.27 samples/sec#011loss=-4.255745\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] Epoch[116] Batch[10] avg_epoch_loss=-4.353863\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=-4.471604681015014\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] Epoch[116] Batch [10]#011Speed: 167.51 samples/sec#011loss=-4.471605\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] processed a total of 736 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847047.1999137, \"EndTime\": 1640847051.9459803, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4745.453119277954, \"count\": 1, \"min\": 4745.453119277954, \"max\": 4745.453119277954}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.09142955957861 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] #progress_metric: host=algo-1, completed 29.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=116, train loss <loss>=-4.188950002193451\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:51 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:52 INFO 140511108945280] Epoch[117] Batch[0] avg_epoch_loss=-4.717410\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=-4.717409610748291\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:54 INFO 140511108945280] Epoch[117] Batch[5] avg_epoch_loss=-4.710570\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=-4.710570017496745\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:54 INFO 140511108945280] Epoch[117] Batch [5]#011Speed: 173.44 samples/sec#011loss=-4.710570\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] Epoch[117] Batch[10] avg_epoch_loss=-4.486845\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=-4.218374443054199\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] Epoch[117] Batch [10]#011Speed: 165.76 samples/sec#011loss=-4.218374\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847051.9460769, \"EndTime\": 1640847056.7587874, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4812.188625335693, \"count\": 1, \"min\": 4812.188625335693, \"max\": 4812.188625335693}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.2654303558801 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=117, train loss <loss>=-4.466228087743123\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:56 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:57 INFO 140511108945280] Epoch[118] Batch[0] avg_epoch_loss=-4.689112\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=-4.689111709594727\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:59 INFO 140511108945280] Epoch[118] Batch[5] avg_epoch_loss=-4.599299\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=-4.599299271901448\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:50:59 INFO 140511108945280] Epoch[118] Batch [5]#011Speed: 172.22 samples/sec#011loss=-4.599299\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] Epoch[118] Batch[10] avg_epoch_loss=-4.534765\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=-4.45732479095459\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] Epoch[118] Batch [10]#011Speed: 159.80 samples/sec#011loss=-4.457325\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] processed a total of 744 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847056.758885, \"EndTime\": 1640847061.5951912, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4835.730791091919, \"count\": 1, \"min\": 4835.730791091919, \"max\": 4835.730791091919}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.85022804926876 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] #progress_metric: host=algo-1, completed 29.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=118, train loss <loss>=-4.507266163825989\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:01 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:02 INFO 140511108945280] Epoch[119] Batch[0] avg_epoch_loss=-4.171479\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=-4.171478748321533\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:04 INFO 140511108945280] Epoch[119] Batch[5] avg_epoch_loss=-4.402178\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=-4.402178088823955\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:04 INFO 140511108945280] Epoch[119] Batch [5]#011Speed: 165.82 samples/sec#011loss=-4.402178\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] Epoch[119] Batch[10] avg_epoch_loss=-4.508892\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=-4.636949729919434\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] Epoch[119] Batch [10]#011Speed: 139.69 samples/sec#011loss=-4.636950\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] processed a total of 758 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847061.5952902, \"EndTime\": 1640847066.941487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5345.59965133667, \"count\": 1, \"min\": 5345.59965133667, \"max\": 5345.59965133667}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=141.79621568414944 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=119, train loss <loss>=-4.47288856903712\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:06 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:07 INFO 140511108945280] Epoch[120] Batch[0] avg_epoch_loss=-5.270559\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=-5.270558834075928\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:09 INFO 140511108945280] Epoch[120] Batch[5] avg_epoch_loss=-4.558052\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=-4.5580519040425616\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:09 INFO 140511108945280] Epoch[120] Batch [5]#011Speed: 169.49 samples/sec#011loss=-4.558052\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] Epoch[120] Batch[10] avg_epoch_loss=-4.717144\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=-4.908054542541504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] Epoch[120] Batch [10]#011Speed: 166.54 samples/sec#011loss=-4.908055\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] processed a total of 737 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847066.9415567, \"EndTime\": 1640847071.7216702, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4779.44540977478, \"count\": 1, \"min\": 4779.44540977478, \"max\": 4779.44540977478}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.19792232005466 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] #progress_metric: host=algo-1, completed 30.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=120, train loss <loss>=-4.810565233230591\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:11 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_750ebfa3-6d70-4638-b34d-d2dea0b6e5be-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847071.7217555, \"EndTime\": 1640847071.8217108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 99.3194580078125, \"count\": 1, \"min\": 99.3194580078125, \"max\": 99.3194580078125}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:12 INFO 140511108945280] Epoch[121] Batch[0] avg_epoch_loss=-3.771636\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=-3.7716360092163086\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:14 INFO 140511108945280] Epoch[121] Batch[5] avg_epoch_loss=-4.195174\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=-4.195174177487691\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:14 INFO 140511108945280] Epoch[121] Batch [5]#011Speed: 171.65 samples/sec#011loss=-4.195174\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:16 INFO 140511108945280] Epoch[121] Batch[10] avg_epoch_loss=-4.360316\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=-4.558486461639404\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:16 INFO 140511108945280] Epoch[121] Batch [10]#011Speed: 161.40 samples/sec#011loss=-4.558486\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:17 INFO 140511108945280] processed a total of 806 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847071.8217878, \"EndTime\": 1640847077.0293531, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5207.504034042358, \"count\": 1, \"min\": 5207.504034042358, \"max\": 5207.504034042358}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:17 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.77309425778063 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:17 INFO 140511108945280] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=121, train loss <loss>=-4.428776796047504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:17 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:17 INFO 140511108945280] Epoch[122] Batch[0] avg_epoch_loss=-4.011684\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=-4.011683940887451\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:19 INFO 140511108945280] Epoch[122] Batch[5] avg_epoch_loss=-4.357578\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=-4.3575780391693115\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:19 INFO 140511108945280] Epoch[122] Batch [5]#011Speed: 167.38 samples/sec#011loss=-4.357578\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] Epoch[122] Batch[10] avg_epoch_loss=-4.426721\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=-4.509691905975342\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] Epoch[122] Batch [10]#011Speed: 162.76 samples/sec#011loss=-4.509692\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847077.0294368, \"EndTime\": 1640847081.49391, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4463.906049728394, \"count\": 1, \"min\": 4463.906049728394, \"max\": 4463.906049728394}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.48092702703713 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] #progress_metric: host=algo-1, completed 30.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=122, train loss <loss>=-4.426720705899325\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:21 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:22 INFO 140511108945280] Epoch[123] Batch[0] avg_epoch_loss=-4.883933\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=-4.883932590484619\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:23 INFO 140511108945280] Epoch[123] Batch[5] avg_epoch_loss=-4.125145\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=-4.125145077705383\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:23 INFO 140511108945280] Epoch[123] Batch [5]#011Speed: 172.10 samples/sec#011loss=-4.125145\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:25 INFO 140511108945280] Epoch[123] Batch[10] avg_epoch_loss=-4.164687\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=-4.212138032913208\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:25 INFO 140511108945280] Epoch[123] Batch [10]#011Speed: 161.38 samples/sec#011loss=-4.212138\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:26 INFO 140511108945280] processed a total of 732 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847081.4939985, \"EndTime\": 1640847086.3270068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4832.516670227051, \"count\": 1, \"min\": 4832.516670227051, \"max\": 4832.516670227051}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:26 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.46958760986948 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:26 INFO 140511108945280] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=123, train loss <loss>=-4.172958334287007\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:26 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:26 INFO 140511108945280] Epoch[124] Batch[0] avg_epoch_loss=-4.249768\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=-4.249767780303955\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:28 INFO 140511108945280] Epoch[124] Batch[5] avg_epoch_loss=-4.452906\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=-4.452906052271525\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:28 INFO 140511108945280] Epoch[124] Batch [5]#011Speed: 172.07 samples/sec#011loss=-4.452906\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:30 INFO 140511108945280] Epoch[124] Batch[10] avg_epoch_loss=-4.446846\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=-4.439573574066162\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:30 INFO 140511108945280] Epoch[124] Batch [10]#011Speed: 160.44 samples/sec#011loss=-4.439574\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:31 INFO 140511108945280] processed a total of 725 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847086.327103, \"EndTime\": 1640847091.1858099, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4858.1836223602295, \"count\": 1, \"min\": 4858.1836223602295, \"max\": 4858.1836223602295}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:31 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.22851800491077 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:31 INFO 140511108945280] #progress_metric: host=algo-1, completed 31.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=124, train loss <loss>=-4.377406179904938\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:31 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:31 INFO 140511108945280] Epoch[125] Batch[0] avg_epoch_loss=-4.649418\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=-4.649418354034424\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:33 INFO 140511108945280] Epoch[125] Batch[5] avg_epoch_loss=-4.371929\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=-4.371928811073303\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:33 INFO 140511108945280] Epoch[125] Batch [5]#011Speed: 170.44 samples/sec#011loss=-4.371929\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:35 INFO 140511108945280] Epoch[125] Batch[10] avg_epoch_loss=-4.479880\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=-4.609422302246093\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:35 INFO 140511108945280] Epoch[125] Batch [10]#011Speed: 164.39 samples/sec#011loss=-4.609422\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:36 INFO 140511108945280] processed a total of 774 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847091.1859093, \"EndTime\": 1640847096.415884, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5229.3994426727295, \"count\": 1, \"min\": 5229.3994426727295, \"max\": 5229.3994426727295}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:36 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.00462415391428 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:36 INFO 140511108945280] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=125, train loss <loss>=-4.6186894453488865\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:36 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:36 INFO 140511108945280] Epoch[126] Batch[0] avg_epoch_loss=-4.779357\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=-4.779356956481934\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:38 INFO 140511108945280] Epoch[126] Batch[5] avg_epoch_loss=-4.543246\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=-4.543245633443196\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:38 INFO 140511108945280] Epoch[126] Batch [5]#011Speed: 171.60 samples/sec#011loss=-4.543246\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:40 INFO 140511108945280] Epoch[126] Batch[10] avg_epoch_loss=-4.499325\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=-4.446620607376099\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:40 INFO 140511108945280] Epoch[126] Batch [10]#011Speed: 162.51 samples/sec#011loss=-4.446621\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:41 INFO 140511108945280] processed a total of 777 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847096.4160097, \"EndTime\": 1640847101.614416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5197.854518890381, \"count\": 1, \"min\": 5197.854518890381, \"max\": 5197.854518890381}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:41 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.479247515788 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:41 INFO 140511108945280] #progress_metric: host=algo-1, completed 31.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=126, train loss <loss>=-4.238549122443566\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:41 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:42 INFO 140511108945280] Epoch[127] Batch[0] avg_epoch_loss=-4.181251\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=-4.181251049041748\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:44 INFO 140511108945280] Epoch[127] Batch[5] avg_epoch_loss=-4.393715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=-4.3937151829401655\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:44 INFO 140511108945280] Epoch[127] Batch [5]#011Speed: 171.51 samples/sec#011loss=-4.393715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] Epoch[127] Batch[10] avg_epoch_loss=-4.608579\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=-4.866415023803711\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] Epoch[127] Batch [10]#011Speed: 164.32 samples/sec#011loss=-4.866415\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] processed a total of 749 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847101.6145663, \"EndTime\": 1640847106.4139237, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4798.83599281311, \"count\": 1, \"min\": 4798.83599281311, \"max\": 4798.83599281311}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.07514280486404 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=127, train loss <loss>=-4.658187886079152\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:46 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:47 INFO 140511108945280] Epoch[128] Batch[0] avg_epoch_loss=-4.829346\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=-4.829345703125\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:48 INFO 140511108945280] Epoch[128] Batch[5] avg_epoch_loss=-4.536507\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=-4.536507209142049\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:48 INFO 140511108945280] Epoch[128] Batch [5]#011Speed: 170.71 samples/sec#011loss=-4.536507\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:50 INFO 140511108945280] Epoch[128] Batch[10] avg_epoch_loss=-4.499844\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=-4.455847215652466\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:50 INFO 140511108945280] Epoch[128] Batch [10]#011Speed: 164.65 samples/sec#011loss=-4.455847\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:51 INFO 140511108945280] processed a total of 760 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847106.414019, \"EndTime\": 1640847111.260326, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4845.791816711426, \"count\": 1, \"min\": 4845.791816711426, \"max\": 4845.791816711426}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:51 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.83269179289238 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:51 INFO 140511108945280] #progress_metric: host=algo-1, completed 32.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=128, train loss <loss>=-4.368889590104421\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:51 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:51 INFO 140511108945280] Epoch[129] Batch[0] avg_epoch_loss=-4.695161\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=-4.69516134262085\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:53 INFO 140511108945280] Epoch[129] Batch[5] avg_epoch_loss=-4.493604\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=-4.49360438187917\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:53 INFO 140511108945280] Epoch[129] Batch [5]#011Speed: 169.84 samples/sec#011loss=-4.493604\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:55 INFO 140511108945280] Epoch[129] Batch[10] avg_epoch_loss=-4.517617\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=-4.546431159973144\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:55 INFO 140511108945280] Epoch[129] Batch [10]#011Speed: 166.25 samples/sec#011loss=-4.546431\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:56 INFO 140511108945280] processed a total of 777 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847111.260424, \"EndTime\": 1640847116.4901202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5229.101896286011, \"count\": 1, \"min\": 5229.101896286011, \"max\": 5229.101896286011}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:56 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.5878709756487 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:56 INFO 140511108945280] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=129, train loss <loss>=-4.264688317592327\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:56 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:57 INFO 140511108945280] Epoch[130] Batch[0] avg_epoch_loss=-4.049521\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=-4.049520969390869\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:58 INFO 140511108945280] Epoch[130] Batch[5] avg_epoch_loss=-4.460070\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=-4.460069894790649\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:51:58 INFO 140511108945280] Epoch[130] Batch [5]#011Speed: 170.17 samples/sec#011loss=-4.460070\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:00 INFO 140511108945280] Epoch[130] Batch[10] avg_epoch_loss=-4.394322\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=-4.315424156188965\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:00 INFO 140511108945280] Epoch[130] Batch [10]#011Speed: 162.78 samples/sec#011loss=-4.315424\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:01 INFO 140511108945280] processed a total of 725 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847116.4902048, \"EndTime\": 1640847121.3518422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4861.1109256744385, \"count\": 1, \"min\": 4861.1109256744385, \"max\": 4861.1109256744385}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:01 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.1387300411162 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:01 INFO 140511108945280] #progress_metric: host=algo-1, completed 32.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=130, train loss <loss>=-4.577621301015218\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:01 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:02 INFO 140511108945280] Epoch[131] Batch[0] avg_epoch_loss=-4.485569\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=-4.485569000244141\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:03 INFO 140511108945280] Epoch[131] Batch[5] avg_epoch_loss=-4.559908\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=-4.559908231099446\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:03 INFO 140511108945280] Epoch[131] Batch [5]#011Speed: 167.06 samples/sec#011loss=-4.559908\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] Epoch[131] Batch[10] avg_epoch_loss=-4.472263\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=-4.367088365554809\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] Epoch[131] Batch [10]#011Speed: 147.60 samples/sec#011loss=-4.367088\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] processed a total of 720 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847121.3519394, \"EndTime\": 1640847126.6739762, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5321.475505828857, \"count\": 1, \"min\": 5321.475505828857, \"max\": 5321.475505828857}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=135.29723153778338 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=131, train loss <loss>=-4.591318666934967\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:06 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:07 INFO 140511108945280] Epoch[132] Batch[0] avg_epoch_loss=-4.466825\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=-4.466825485229492\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:09 INFO 140511108945280] Epoch[132] Batch[5] avg_epoch_loss=-4.363218\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=-4.363218108812968\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:09 INFO 140511108945280] Epoch[132] Batch [5]#011Speed: 170.98 samples/sec#011loss=-4.363218\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] Epoch[132] Batch[10] avg_epoch_loss=-4.412013\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=-4.470566272735596\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] Epoch[132] Batch [10]#011Speed: 165.31 samples/sec#011loss=-4.470566\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] processed a total of 755 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847126.674074, \"EndTime\": 1640847131.5681813, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4893.498659133911, \"count\": 1, \"min\": 4893.498659133911, \"max\": 4893.498659133911}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.2817777051957 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] #progress_metric: host=algo-1, completed 33.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=132, train loss <loss>=-4.359131912390391\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:11 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:12 INFO 140511108945280] Epoch[133] Batch[0] avg_epoch_loss=-4.495555\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=-4.495555400848389\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:14 INFO 140511108945280] Epoch[133] Batch[5] avg_epoch_loss=-4.508783\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=-4.508783340454102\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:14 INFO 140511108945280] Epoch[133] Batch [5]#011Speed: 170.42 samples/sec#011loss=-4.508783\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] Epoch[133] Batch[10] avg_epoch_loss=-4.444220\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=-4.366743755340576\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] Epoch[133] Batch [10]#011Speed: 166.18 samples/sec#011loss=-4.366744\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847131.5682678, \"EndTime\": 1640847136.404239, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4835.464715957642, \"count\": 1, \"min\": 4835.464715957642, \"max\": 4835.464715957642}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.78491658425523 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=133, train loss <loss>=-4.453425765037537\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:16 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:17 INFO 140511108945280] Epoch[134] Batch[0] avg_epoch_loss=-4.718996\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=-4.718996047973633\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:18 INFO 140511108945280] Epoch[134] Batch[5] avg_epoch_loss=-4.702958\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=-4.702958106994629\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:18 INFO 140511108945280] Epoch[134] Batch [5]#011Speed: 172.19 samples/sec#011loss=-4.702958\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:20 INFO 140511108945280] Epoch[134] Batch[10] avg_epoch_loss=-4.587169\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=-4.448222827911377\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:20 INFO 140511108945280] Epoch[134] Batch [10]#011Speed: 165.95 samples/sec#011loss=-4.448223\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:21 INFO 140511108945280] processed a total of 721 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847136.4044297, \"EndTime\": 1640847141.200354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4795.362710952759, \"count\": 1, \"min\": 4795.362710952759, \"max\": 4795.362710952759}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:21 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.34935180743784 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:21 INFO 140511108945280] #progress_metric: host=algo-1, completed 33.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=134, train loss <loss>=-4.755740841229756\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:21 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:21 INFO 140511108945280] Epoch[135] Batch[0] avg_epoch_loss=-4.570909\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=-4.570908546447754\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:23 INFO 140511108945280] Epoch[135] Batch[5] avg_epoch_loss=-4.666920\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=-4.666919628779094\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:23 INFO 140511108945280] Epoch[135] Batch [5]#011Speed: 172.15 samples/sec#011loss=-4.666920\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] Epoch[135] Batch[10] avg_epoch_loss=-4.527372\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=-4.359915924072266\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] Epoch[135] Batch [10]#011Speed: 166.78 samples/sec#011loss=-4.359916\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] processed a total of 725 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847141.2004514, \"EndTime\": 1640847145.9875407, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4786.555528640747, \"count\": 1, \"min\": 4786.555528640747, \"max\": 4786.555528640747}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.46158188316758 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=135, train loss <loss>=-4.330306688944499\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:25 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:26 INFO 140511108945280] Epoch[136] Batch[0] avg_epoch_loss=-4.423075\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=-4.423074722290039\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:28 INFO 140511108945280] Epoch[136] Batch[5] avg_epoch_loss=-4.472599\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=-4.472599426905314\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:28 INFO 140511108945280] Epoch[136] Batch [5]#011Speed: 172.48 samples/sec#011loss=-4.472599\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:30 INFO 140511108945280] Epoch[136] Batch[10] avg_epoch_loss=-4.573704\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=-4.695030307769775\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:30 INFO 140511108945280] Epoch[136] Batch [10]#011Speed: 164.39 samples/sec#011loss=-4.695030\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:31 INFO 140511108945280] processed a total of 770 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847145.987637, \"EndTime\": 1640847151.2616932, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5273.490428924561, \"count\": 1, \"min\": 5273.490428924561, \"max\": 5273.490428924561}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:31 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=146.00950079354118 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:31 INFO 140511108945280] #progress_metric: host=algo-1, completed 34.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=136, train loss <loss>=-4.472845536011916\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:31 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:31 INFO 140511108945280] Epoch[137] Batch[0] avg_epoch_loss=-4.654159\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=-4.654159069061279\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:33 INFO 140511108945280] Epoch[137] Batch[5] avg_epoch_loss=-4.482487\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=-4.482486883799235\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:33 INFO 140511108945280] Epoch[137] Batch [5]#011Speed: 171.63 samples/sec#011loss=-4.482487\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:35 INFO 140511108945280] Epoch[137] Batch[10] avg_epoch_loss=-4.515947\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=-4.556098461151123\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:35 INFO 140511108945280] Epoch[137] Batch [10]#011Speed: 166.23 samples/sec#011loss=-4.556098\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:36 INFO 140511108945280] processed a total of 773 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847151.261792, \"EndTime\": 1640847156.427982, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5165.6317710876465, \"count\": 1, \"min\": 5165.6317710876465, \"max\": 5165.6317710876465}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:36 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.6393546975687 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:36 INFO 140511108945280] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=137, train loss <loss>=-4.412581810584435\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:36 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:37 INFO 140511108945280] Epoch[138] Batch[0] avg_epoch_loss=-5.342969\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=-5.342968940734863\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:38 INFO 140511108945280] Epoch[138] Batch[5] avg_epoch_loss=-4.668426\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=-4.668425559997559\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:38 INFO 140511108945280] Epoch[138] Batch [5]#011Speed: 171.67 samples/sec#011loss=-4.668426\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:40 INFO 140511108945280] Epoch[138] Batch[10] avg_epoch_loss=-4.680808\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=-4.695666313171387\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:40 INFO 140511108945280] Epoch[138] Batch [10]#011Speed: 167.14 samples/sec#011loss=-4.695666\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:41 INFO 140511108945280] processed a total of 782 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847156.4280713, \"EndTime\": 1640847161.57458, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5145.920038223267, \"count\": 1, \"min\": 5145.920038223267, \"max\": 5145.920038223267}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:41 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.96091092992768 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:41 INFO 140511108945280] #progress_metric: host=algo-1, completed 34.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=138, train loss <loss>=-4.732131811288687\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:41 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:42 INFO 140511108945280] Epoch[139] Batch[0] avg_epoch_loss=-4.737759\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=-4.737758636474609\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:44 INFO 140511108945280] Epoch[139] Batch[5] avg_epoch_loss=-4.425379\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=-4.425378640492757\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:44 INFO 140511108945280] Epoch[139] Batch [5]#011Speed: 172.64 samples/sec#011loss=-4.425379\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:45 INFO 140511108945280] Epoch[139] Batch[10] avg_epoch_loss=-4.507302\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=-4.605609655380249\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:45 INFO 140511108945280] Epoch[139] Batch [10]#011Speed: 165.29 samples/sec#011loss=-4.605610\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:46 INFO 140511108945280] processed a total of 734 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847161.5746794, \"EndTime\": 1640847166.3678944, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4792.670726776123, \"count\": 1, \"min\": 4792.670726776123, \"max\": 4792.670726776123}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:46 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.14533653809323 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:46 INFO 140511108945280] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=139, train loss <loss>=-4.584983805815379\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:46 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:46 INFO 140511108945280] Epoch[140] Batch[0] avg_epoch_loss=-5.189027\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=-5.189026832580566\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:48 INFO 140511108945280] Epoch[140] Batch[5] avg_epoch_loss=-4.450181\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=-4.450180768966675\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:48 INFO 140511108945280] Epoch[140] Batch [5]#011Speed: 170.56 samples/sec#011loss=-4.450181\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:50 INFO 140511108945280] Epoch[140] Batch[10] avg_epoch_loss=-4.394539\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=-4.327769088745117\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:50 INFO 140511108945280] Epoch[140] Batch [10]#011Speed: 168.60 samples/sec#011loss=-4.327769\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:51 INFO 140511108945280] processed a total of 727 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847166.3680131, \"EndTime\": 1640847171.129605, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4760.957479476929, \"count\": 1, \"min\": 4760.957479476929, \"max\": 4760.957479476929}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:51 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.69583445191435 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:51 INFO 140511108945280] #progress_metric: host=algo-1, completed 35.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=140, train loss <loss>=-4.361680706342061\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:51 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:51 INFO 140511108945280] Epoch[141] Batch[0] avg_epoch_loss=-4.926982\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=-4.9269819259643555\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:53 INFO 140511108945280] Epoch[141] Batch[5] avg_epoch_loss=-4.573438\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=-4.573438008626302\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:53 INFO 140511108945280] Epoch[141] Batch [5]#011Speed: 169.84 samples/sec#011loss=-4.573438\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] Epoch[141] Batch[10] avg_epoch_loss=-4.521477\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=-4.459123516082764\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] Epoch[141] Batch [10]#011Speed: 169.02 samples/sec#011loss=-4.459124\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] processed a total of 738 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847171.1297057, \"EndTime\": 1640847175.881434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4751.126050949097, \"count\": 1, \"min\": 4751.126050949097, \"max\": 4751.126050949097}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.32624279958094 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=141, train loss <loss>=-4.64934515953064\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:55 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:56 INFO 140511108945280] Epoch[142] Batch[0] avg_epoch_loss=-3.973632\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=-3.9736316204071045\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:58 INFO 140511108945280] Epoch[142] Batch[5] avg_epoch_loss=-4.537074\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=-4.537073969841003\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:52:58 INFO 140511108945280] Epoch[142] Batch [5]#011Speed: 166.60 samples/sec#011loss=-4.537074\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] Epoch[142] Batch[10] avg_epoch_loss=-4.460192\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=-4.367933654785157\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] Epoch[142] Batch [10]#011Speed: 167.56 samples/sec#011loss=-4.367934\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847175.8815546, \"EndTime\": 1640847180.2926636, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4410.4955196380615, \"count\": 1, \"min\": 4410.4955196380615, \"max\": 4410.4955196380615}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=159.38726529250494 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] #progress_metric: host=algo-1, completed 35.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=142, train loss <loss>=-4.460192008451982\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] Epoch[143] Batch[0] avg_epoch_loss=-4.594356\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=-4.594356060028076\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:03 INFO 140511108945280] Epoch[143] Batch[5] avg_epoch_loss=-4.436944\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=-4.436943769454956\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:03 INFO 140511108945280] Epoch[143] Batch [5]#011Speed: 143.49 samples/sec#011loss=-4.436944\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] Epoch[143] Batch[10] avg_epoch_loss=-4.516093\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=-4.611072158813476\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] Epoch[143] Batch [10]#011Speed: 142.90 samples/sec#011loss=-4.611072\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] processed a total of 753 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847180.2927654, \"EndTime\": 1640847185.7289128, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5435.534477233887, \"count\": 1, \"min\": 5435.534477233887, \"max\": 5435.534477233887}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=138.52856929689767 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=143, train loss <loss>=-4.584715167681376\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:05 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:06 INFO 140511108945280] Epoch[144] Batch[0] avg_epoch_loss=-4.693972\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=-4.693971633911133\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:08 INFO 140511108945280] Epoch[144] Batch[5] avg_epoch_loss=-4.595567\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=-4.595566987991333\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:08 INFO 140511108945280] Epoch[144] Batch [5]#011Speed: 166.10 samples/sec#011loss=-4.595567\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] Epoch[144] Batch[10] avg_epoch_loss=-4.692885\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=-4.809666728973388\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] Epoch[144] Batch [10]#011Speed: 165.46 samples/sec#011loss=-4.809667\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] processed a total of 768 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847185.729014, \"EndTime\": 1640847190.5703614, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4840.602397918701, \"count\": 1, \"min\": 4840.602397918701, \"max\": 4840.602397918701}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.65366457663345 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] #progress_metric: host=algo-1, completed 36.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=144, train loss <loss>=-4.7265119552612305\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:10 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:11 INFO 140511108945280] Epoch[145] Batch[0] avg_epoch_loss=-4.312870\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=-4.312869548797607\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:13 INFO 140511108945280] Epoch[145] Batch[5] avg_epoch_loss=-4.398686\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=-4.398685852686564\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:13 INFO 140511108945280] Epoch[145] Batch [5]#011Speed: 169.59 samples/sec#011loss=-4.398686\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] Epoch[145] Batch[10] avg_epoch_loss=-4.499731\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=-4.620985221862793\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] Epoch[145] Batch [10]#011Speed: 164.92 samples/sec#011loss=-4.620985\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] processed a total of 790 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847190.5704534, \"EndTime\": 1640847195.7795594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5208.550930023193, \"count\": 1, \"min\": 5208.550930023193, \"max\": 5208.550930023193}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.66993511127015 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=145, train loss <loss>=-4.605918187361497\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:15 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:16 INFO 140511108945280] Epoch[146] Batch[0] avg_epoch_loss=-4.623382\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=-4.623381614685059\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:18 INFO 140511108945280] Epoch[146] Batch[5] avg_epoch_loss=-4.453883\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=-4.453883250554402\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:18 INFO 140511108945280] Epoch[146] Batch [5]#011Speed: 168.65 samples/sec#011loss=-4.453883\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] Epoch[146] Batch[10] avg_epoch_loss=-4.502007\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=-4.559755134582519\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] Epoch[146] Batch [10]#011Speed: 165.57 samples/sec#011loss=-4.559755\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] processed a total of 771 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847195.779645, \"EndTime\": 1640847200.9528913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5172.580242156982, \"count\": 1, \"min\": 5172.580242156982, \"max\": 5172.580242156982}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.05204307886942 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] #progress_metric: host=algo-1, completed 36.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=146, train loss <loss>=-4.231951016646165\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:20 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:21 INFO 140511108945280] Epoch[147] Batch[0] avg_epoch_loss=-4.107191\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=-4.1071906089782715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:23 INFO 140511108945280] Epoch[147] Batch[5] avg_epoch_loss=-4.600589\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=-4.60058871905009\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:23 INFO 140511108945280] Epoch[147] Batch [5]#011Speed: 166.00 samples/sec#011loss=-4.600589\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:25 INFO 140511108945280] Epoch[147] Batch[10] avg_epoch_loss=-4.541661\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=-4.470947360992431\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:25 INFO 140511108945280] Epoch[147] Batch [10]#011Speed: 162.34 samples/sec#011loss=-4.470947\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:26 INFO 140511108945280] processed a total of 806 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847200.9529634, \"EndTime\": 1640847206.18222, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5228.646039962769, \"count\": 1, \"min\": 5228.646039962769, \"max\": 5228.646039962769}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:26 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.14677820140747 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:26 INFO 140511108945280] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=147, train loss <loss>=-4.470194229712853\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:26 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:26 INFO 140511108945280] Epoch[148] Batch[0] avg_epoch_loss=-4.445261\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=-4.445260524749756\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:28 INFO 140511108945280] Epoch[148] Batch[5] avg_epoch_loss=-4.548039\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=-4.548038641611735\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:28 INFO 140511108945280] Epoch[148] Batch [5]#011Speed: 166.68 samples/sec#011loss=-4.548039\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:30 INFO 140511108945280] Epoch[148] Batch[10] avg_epoch_loss=-4.533294\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=-4.5155998229980465\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:30 INFO 140511108945280] Epoch[148] Batch [10]#011Speed: 162.02 samples/sec#011loss=-4.515600\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:31 INFO 140511108945280] processed a total of 795 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847206.1823168, \"EndTime\": 1640847211.4274683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5244.6160316467285, \"count\": 1, \"min\": 5244.6160316467285, \"max\": 5244.6160316467285}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:31 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.58004755958143 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:31 INFO 140511108945280] #progress_metric: host=algo-1, completed 37.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=148, train loss <loss>=-4.664713639479417\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:31 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:32 INFO 140511108945280] Epoch[149] Batch[0] avg_epoch_loss=-4.613553\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=-4.613552570343018\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:34 INFO 140511108945280] Epoch[149] Batch[5] avg_epoch_loss=-4.710189\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=-4.7101891040802\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:34 INFO 140511108945280] Epoch[149] Batch [5]#011Speed: 163.11 samples/sec#011loss=-4.710189\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:35 INFO 140511108945280] Epoch[149] Batch[10] avg_epoch_loss=-4.717665\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=-4.726635265350342\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:35 INFO 140511108945280] Epoch[149] Batch [10]#011Speed: 165.56 samples/sec#011loss=-4.726635\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:36 INFO 140511108945280] processed a total of 759 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847211.4275658, \"EndTime\": 1640847216.3813627, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4953.259468078613, \"count\": 1, \"min\": 4953.259468078613, \"max\": 4953.259468078613}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:36 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.22819221034834 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:36 INFO 140511108945280] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=149, train loss <loss>=-4.748848597208659\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:36 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:36 INFO 140511108945280] Epoch[150] Batch[0] avg_epoch_loss=-4.474428\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=-4.474428176879883\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:38 INFO 140511108945280] Epoch[150] Batch[5] avg_epoch_loss=-4.637265\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=-4.637265125910441\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:38 INFO 140511108945280] Epoch[150] Batch [5]#011Speed: 169.69 samples/sec#011loss=-4.637265\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:40 INFO 140511108945280] Epoch[150] Batch[10] avg_epoch_loss=-4.620883\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=-4.601225280761719\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:40 INFO 140511108945280] Epoch[150] Batch [10]#011Speed: 165.59 samples/sec#011loss=-4.601225\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:41 INFO 140511108945280] processed a total of 767 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847216.3814595, \"EndTime\": 1640847221.1628122, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4780.833959579468, \"count\": 1, \"min\": 4780.833959579468, \"max\": 4780.833959579468}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:41 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=160.42772439492106 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:41 INFO 140511108945280] #progress_metric: host=algo-1, completed 37.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=150, train loss <loss>=-4.612720449765523\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:41 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:41 INFO 140511108945280] Epoch[151] Batch[0] avg_epoch_loss=-5.007421\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=-5.007420539855957\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:43 INFO 140511108945280] Epoch[151] Batch[5] avg_epoch_loss=-4.803417\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=-4.803416649500529\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:43 INFO 140511108945280] Epoch[151] Batch [5]#011Speed: 168.21 samples/sec#011loss=-4.803417\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:45 INFO 140511108945280] Epoch[151] Batch[10] avg_epoch_loss=-4.723494\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=-4.627585792541504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:45 INFO 140511108945280] Epoch[151] Batch [10]#011Speed: 155.57 samples/sec#011loss=-4.627586\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:46 INFO 140511108945280] processed a total of 710 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847221.162909, \"EndTime\": 1640847226.1909063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5027.433633804321, \"count\": 1, \"min\": 5027.433633804321, \"max\": 5027.433633804321}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:46 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=141.22136573423774 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:46 INFO 140511108945280] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=151, train loss <loss>=-4.871140877405803\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:46 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:46 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_45726b99-5f5d-405c-81ec-dad73408ab70-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847226.191001, \"EndTime\": 1640847226.3622482, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 170.64905166625977, \"count\": 1, \"min\": 170.64905166625977, \"max\": 170.64905166625977}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:47 INFO 140511108945280] Epoch[152] Batch[0] avg_epoch_loss=-4.280877\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=-4.280876636505127\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:49 INFO 140511108945280] Epoch[152] Batch[5] avg_epoch_loss=-4.363833\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=-4.3638332684834795\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:49 INFO 140511108945280] Epoch[152] Batch [5]#011Speed: 123.18 samples/sec#011loss=-4.363833\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:51 INFO 140511108945280] Epoch[152] Batch[10] avg_epoch_loss=-4.510583\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=-4.686683273315429\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:51 INFO 140511108945280] Epoch[152] Batch [10]#011Speed: 154.32 samples/sec#011loss=-4.686683\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:52 INFO 140511108945280] processed a total of 757 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847226.362323, \"EndTime\": 1640847232.113176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5750.781536102295, \"count\": 1, \"min\": 5750.781536102295, \"max\": 5750.781536102295}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:52 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=131.63118802286448 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:52 INFO 140511108945280] #progress_metric: host=algo-1, completed 38.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=152, train loss <loss>=-4.641369779904683\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:52 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:52 INFO 140511108945280] Epoch[153] Batch[0] avg_epoch_loss=-4.494882\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=-4.494881629943848\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:54 INFO 140511108945280] Epoch[153] Batch[5] avg_epoch_loss=-4.665595\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=-4.665595134099324\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:54 INFO 140511108945280] Epoch[153] Batch [5]#011Speed: 170.41 samples/sec#011loss=-4.665595\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:56 INFO 140511108945280] Epoch[153] Batch[10] avg_epoch_loss=-4.624401\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=-4.574967384338379\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:56 INFO 140511108945280] Epoch[153] Batch [10]#011Speed: 164.89 samples/sec#011loss=-4.574967\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:57 INFO 140511108945280] processed a total of 773 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847232.1132712, \"EndTime\": 1640847237.3118005, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5198.008060455322, \"count\": 1, \"min\": 5198.008060455322, \"max\": 5198.008060455322}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:57 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.70695154044068 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:57 INFO 140511108945280] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=153, train loss <loss>=-4.21254314826085\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:57 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:57 INFO 140511108945280] Epoch[154] Batch[0] avg_epoch_loss=-4.445343\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=-4.445342540740967\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:59 INFO 140511108945280] Epoch[154] Batch[5] avg_epoch_loss=-4.524776\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=-4.524776379267375\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:53:59 INFO 140511108945280] Epoch[154] Batch [5]#011Speed: 170.14 samples/sec#011loss=-4.524776\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:01 INFO 140511108945280] Epoch[154] Batch[10] avg_epoch_loss=-4.471427\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=-4.407407760620117\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:01 INFO 140511108945280] Epoch[154] Batch [10]#011Speed: 161.80 samples/sec#011loss=-4.407408\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:02 INFO 140511108945280] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847237.3118963, \"EndTime\": 1640847242.3728714, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5060.402870178223, \"count\": 1, \"min\": 5060.402870178223, \"max\": 5060.402870178223}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:02 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=139.90589377983858 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:02 INFO 140511108945280] #progress_metric: host=algo-1, completed 38.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=154, train loss <loss>=-4.720438361167908\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:02 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:03 INFO 140511108945280] Epoch[155] Batch[0] avg_epoch_loss=-4.651860\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=-4.651859760284424\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:05 INFO 140511108945280] Epoch[155] Batch[5] avg_epoch_loss=-4.697504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=-4.697504083315532\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:05 INFO 140511108945280] Epoch[155] Batch [5]#011Speed: 148.09 samples/sec#011loss=-4.697504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] Epoch[155] Batch[10] avg_epoch_loss=-4.621027\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=-4.529255151748657\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] Epoch[155] Batch [10]#011Speed: 168.01 samples/sec#011loss=-4.529255\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] processed a total of 701 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847242.3729708, \"EndTime\": 1640847247.19175, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4818.213939666748, \"count\": 1, \"min\": 4818.213939666748, \"max\": 4818.213939666748}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=145.48596057934762 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=155, train loss <loss>=-4.62102729623968\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] Epoch[156] Batch[0] avg_epoch_loss=-4.516516\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=-4.516516208648682\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:09 INFO 140511108945280] Epoch[156] Batch[5] avg_epoch_loss=-4.433955\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=-4.433955192565918\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:09 INFO 140511108945280] Epoch[156] Batch [5]#011Speed: 170.28 samples/sec#011loss=-4.433955\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] Epoch[156] Batch[10] avg_epoch_loss=-4.548981\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=-4.6870111465454105\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] Epoch[156] Batch [10]#011Speed: 166.71 samples/sec#011loss=-4.687011\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847247.1918323, \"EndTime\": 1640847251.9973314, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4804.97670173645, \"count\": 1, \"min\": 4804.97670173645, \"max\": 4804.97670173645}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.78991994249085 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] #progress_metric: host=algo-1, completed 39.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=156, train loss <loss>=-4.56043819586436\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:11 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:12 INFO 140511108945280] Epoch[157] Batch[0] avg_epoch_loss=-4.198635\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=-4.198634624481201\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:14 INFO 140511108945280] Epoch[157] Batch[5] avg_epoch_loss=-4.529036\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=-4.529036204020183\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:14 INFO 140511108945280] Epoch[157] Batch [5]#011Speed: 166.21 samples/sec#011loss=-4.529036\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] Epoch[157] Batch[10] avg_epoch_loss=-4.643521\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=-4.7809038162231445\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] Epoch[157] Batch [10]#011Speed: 166.67 samples/sec#011loss=-4.780904\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847251.9974108, \"EndTime\": 1640847256.8006365, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4802.528381347656, \"count\": 1, \"min\": 4802.528381347656, \"max\": 4802.528381347656}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.5387441828324 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=157, train loss <loss>=-4.496512373288472\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:16 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:17 INFO 140511108945280] Epoch[158] Batch[0] avg_epoch_loss=-5.241220\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=-5.241219997406006\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:19 INFO 140511108945280] Epoch[158] Batch[5] avg_epoch_loss=-4.653805\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=-4.653805096944173\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:19 INFO 140511108945280] Epoch[158] Batch [5]#011Speed: 167.65 samples/sec#011loss=-4.653805\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] Epoch[158] Batch[10] avg_epoch_loss=-4.704763\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=-4.765913200378418\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] Epoch[158] Batch [10]#011Speed: 163.67 samples/sec#011loss=-4.765913\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] processed a total of 766 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847256.800728, \"EndTime\": 1640847261.6717243, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4870.398759841919, \"count\": 1, \"min\": 4870.398759841919, \"max\": 4870.398759841919}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.27130677945217 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] #progress_metric: host=algo-1, completed 39.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=158, train loss <loss>=-4.735811948776245\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:21 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:22 INFO 140511108945280] Epoch[159] Batch[0] avg_epoch_loss=-4.428871\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=-4.428871154785156\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:24 INFO 140511108945280] Epoch[159] Batch[5] avg_epoch_loss=-4.659197\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=-4.659196694691976\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:24 INFO 140511108945280] Epoch[159] Batch [5]#011Speed: 168.67 samples/sec#011loss=-4.659197\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] Epoch[159] Batch[10] avg_epoch_loss=-4.573198\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=-4.469998836517334\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] Epoch[159] Batch [10]#011Speed: 165.76 samples/sec#011loss=-4.469999\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847261.6718228, \"EndTime\": 1640847266.4591153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4786.696195602417, \"count\": 1, \"min\": 4786.696195602417, \"max\": 4786.696195602417}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.05325806731338 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=159, train loss <loss>=-4.427516182263692\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:26 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:27 INFO 140511108945280] Epoch[160] Batch[0] avg_epoch_loss=-4.332129\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=-4.33212947845459\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:28 INFO 140511108945280] Epoch[160] Batch[5] avg_epoch_loss=-4.572685\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=-4.572685321172078\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:28 INFO 140511108945280] Epoch[160] Batch [5]#011Speed: 168.43 samples/sec#011loss=-4.572685\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:30 INFO 140511108945280] Epoch[160] Batch[10] avg_epoch_loss=-4.617951\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=-4.6722687721252445\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:30 INFO 140511108945280] Epoch[160] Batch [10]#011Speed: 165.71 samples/sec#011loss=-4.672269\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:31 INFO 140511108945280] processed a total of 755 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847266.4592078, \"EndTime\": 1640847271.2691295, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4809.4322681427, \"count\": 1, \"min\": 4809.4322681427, \"max\": 4809.4322681427}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:31 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.9786568579488 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:31 INFO 140511108945280] #progress_metric: host=algo-1, completed 40.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=160, train loss <loss>=-4.618604063987732\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:31 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:31 INFO 140511108945280] Epoch[161] Batch[0] avg_epoch_loss=-4.306963\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=-4.306962966918945\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:33 INFO 140511108945280] Epoch[161] Batch[5] avg_epoch_loss=-4.655119\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=-4.65511949857076\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:33 INFO 140511108945280] Epoch[161] Batch [5]#011Speed: 162.54 samples/sec#011loss=-4.655119\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:35 INFO 140511108945280] Epoch[161] Batch[10] avg_epoch_loss=-4.621239\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=161, batch=10 train loss <loss>=-4.580582618713379\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:35 INFO 140511108945280] Epoch[161] Batch [10]#011Speed: 166.19 samples/sec#011loss=-4.580583\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:36 INFO 140511108945280] processed a total of 749 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847271.2692287, \"EndTime\": 1640847276.164049, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4894.32168006897, \"count\": 1, \"min\": 4894.32168006897, \"max\": 4894.32168006897}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:36 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.03007979399152 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:36 INFO 140511108945280] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=161, train loss <loss>=-4.633931517601013\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:36 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:36 INFO 140511108945280] Epoch[162] Batch[0] avg_epoch_loss=-4.880265\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=-4.880265235900879\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:38 INFO 140511108945280] Epoch[162] Batch[5] avg_epoch_loss=-4.737936\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=-4.737935860951741\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:38 INFO 140511108945280] Epoch[162] Batch [5]#011Speed: 164.49 samples/sec#011loss=-4.737936\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:40 INFO 140511108945280] Epoch[162] Batch[10] avg_epoch_loss=-4.640147\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=-4.522799396514893\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:40 INFO 140511108945280] Epoch[162] Batch [10]#011Speed: 160.74 samples/sec#011loss=-4.522799\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:41 INFO 140511108945280] processed a total of 730 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847276.1641564, \"EndTime\": 1640847281.0702786, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4905.543565750122, \"count\": 1, \"min\": 4905.543565750122, \"max\": 4905.543565750122}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:41 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.80723627367053 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:41 INFO 140511108945280] #progress_metric: host=algo-1, completed 40.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=162, train loss <loss>=-4.7179306745529175\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:41 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:41 INFO 140511108945280] Epoch[163] Batch[0] avg_epoch_loss=-4.654387\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=-4.654387474060059\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:43 INFO 140511108945280] Epoch[163] Batch[5] avg_epoch_loss=-4.409160\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=-4.409160017967224\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:43 INFO 140511108945280] Epoch[163] Batch [5]#011Speed: 166.78 samples/sec#011loss=-4.409160\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:45 INFO 140511108945280] Epoch[163] Batch[10] avg_epoch_loss=-4.522631\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=-4.658795547485352\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:45 INFO 140511108945280] Epoch[163] Batch [10]#011Speed: 163.68 samples/sec#011loss=-4.658796\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:46 INFO 140511108945280] processed a total of 789 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847281.0703723, \"EndTime\": 1640847286.2881057, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5217.171907424927, \"count\": 1, \"min\": 5217.171907424927, \"max\": 5217.171907424927}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:46 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.22668077558728 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:46 INFO 140511108945280] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=163, train loss <loss>=-4.660181760787964\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:46 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:46 INFO 140511108945280] Epoch[164] Batch[0] avg_epoch_loss=-4.723544\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=-4.723544120788574\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:48 INFO 140511108945280] Epoch[164] Batch[5] avg_epoch_loss=-4.459701\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=-4.459701100985209\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:48 INFO 140511108945280] Epoch[164] Batch [5]#011Speed: 168.67 samples/sec#011loss=-4.459701\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:50 INFO 140511108945280] Epoch[164] Batch[10] avg_epoch_loss=-4.618624\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=-4.809330940246582\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:50 INFO 140511108945280] Epoch[164] Batch [10]#011Speed: 165.63 samples/sec#011loss=-4.809331\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:51 INFO 140511108945280] processed a total of 754 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847286.28823, \"EndTime\": 1640847291.1368418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4848.102807998657, \"count\": 1, \"min\": 4848.102807998657, \"max\": 4848.102807998657}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:51 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.52039526752586 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:51 INFO 140511108945280] #progress_metric: host=algo-1, completed 41.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=164, train loss <loss>=-4.602980315685272\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:51 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:51 INFO 140511108945280] Epoch[165] Batch[0] avg_epoch_loss=-4.525965\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=-4.525964736938477\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:53 INFO 140511108945280] Epoch[165] Batch[5] avg_epoch_loss=-4.668741\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=-4.66874114672343\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:53 INFO 140511108945280] Epoch[165] Batch [5]#011Speed: 166.00 samples/sec#011loss=-4.668741\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:55 INFO 140511108945280] Epoch[165] Batch[10] avg_epoch_loss=-4.691332\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=-4.718441390991211\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:55 INFO 140511108945280] Epoch[165] Batch [10]#011Speed: 162.76 samples/sec#011loss=-4.718441\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:56 INFO 140511108945280] processed a total of 814 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847291.136939, \"EndTime\": 1640847296.4135475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5276.091575622559, \"count\": 1, \"min\": 5276.091575622559, \"max\": 5276.091575622559}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:56 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.2768965095597 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:56 INFO 140511108945280] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=165, train loss <loss>=-4.689137642200176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:56 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:57 INFO 140511108945280] Epoch[166] Batch[0] avg_epoch_loss=-4.286112\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=-4.286111831665039\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:58 INFO 140511108945280] Epoch[166] Batch[5] avg_epoch_loss=-4.534326\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=-4.534326473871867\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:54:58 INFO 140511108945280] Epoch[166] Batch [5]#011Speed: 169.11 samples/sec#011loss=-4.534326\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:00 INFO 140511108945280] Epoch[166] Batch[10] avg_epoch_loss=-4.590718\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=-4.658386898040772\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:00 INFO 140511108945280] Epoch[166] Batch [10]#011Speed: 163.31 samples/sec#011loss=-4.658387\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:01 INFO 140511108945280] processed a total of 750 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847296.4136436, \"EndTime\": 1640847301.2837343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4869.5807456970215, \"count\": 1, \"min\": 4869.5807456970215, \"max\": 4869.5807456970215}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:01 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.01303769934566 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:01 INFO 140511108945280] #progress_metric: host=algo-1, completed 41.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=166, train loss <loss>=-4.667935132980347\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:01 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:01 INFO 140511108945280] Epoch[167] Batch[0] avg_epoch_loss=-4.405050\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=-4.405050277709961\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:04 INFO 140511108945280] Epoch[167] Batch[5] avg_epoch_loss=-4.442067\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=-4.442066510518392\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:04 INFO 140511108945280] Epoch[167] Batch [5]#011Speed: 137.67 samples/sec#011loss=-4.442067\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] Epoch[167] Batch[10] avg_epoch_loss=-4.542478\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=-4.6629714012146\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] Epoch[167] Batch [10]#011Speed: 154.51 samples/sec#011loss=-4.662971\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847301.2838311, \"EndTime\": 1640847306.3331056, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5048.7306118011475, \"count\": 1, \"min\": 5048.7306118011475, \"max\": 5048.7306118011475}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=136.4664478764953 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=167, train loss <loss>=-4.542477824471214\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] Epoch[168] Batch[0] avg_epoch_loss=-5.049914\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=-5.049914360046387\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:08 INFO 140511108945280] Epoch[168] Batch[5] avg_epoch_loss=-4.497257\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=-4.497257312138875\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:08 INFO 140511108945280] Epoch[168] Batch [5]#011Speed: 171.40 samples/sec#011loss=-4.497257\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:10 INFO 140511108945280] Epoch[168] Batch[10] avg_epoch_loss=-4.698156\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=168, batch=10 train loss <loss>=-4.93923511505127\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:10 INFO 140511108945280] Epoch[168] Batch [10]#011Speed: 164.72 samples/sec#011loss=-4.939235\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:11 INFO 140511108945280] processed a total of 773 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847306.3331969, \"EndTime\": 1640847311.502451, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5168.745517730713, \"count\": 1, \"min\": 5168.745517730713, \"max\": 5168.745517730713}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:11 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.5498735591593 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:11 INFO 140511108945280] #progress_metric: host=algo-1, completed 42.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=168, train loss <loss>=-4.840521335601807\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:11 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:12 INFO 140511108945280] Epoch[169] Batch[0] avg_epoch_loss=-4.492555\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=-4.492554664611816\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:13 INFO 140511108945280] Epoch[169] Batch[5] avg_epoch_loss=-4.543846\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=-4.543845574061076\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:13 INFO 140511108945280] Epoch[169] Batch [5]#011Speed: 168.22 samples/sec#011loss=-4.543846\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:15 INFO 140511108945280] Epoch[169] Batch[10] avg_epoch_loss=-4.644066\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=-4.76432991027832\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:15 INFO 140511108945280] Epoch[169] Batch [10]#011Speed: 160.55 samples/sec#011loss=-4.764330\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:16 INFO 140511108945280] processed a total of 745 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847311.5025158, \"EndTime\": 1640847316.3578453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4854.725122451782, \"count\": 1, \"min\": 4854.725122451782, \"max\": 4854.725122451782}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:16 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.4528843287595 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:16 INFO 140511108945280] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=169, train loss <loss>=-4.714524865150452\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:16 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:17 INFO 140511108945280] Epoch[170] Batch[0] avg_epoch_loss=-4.737128\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=-4.737128257751465\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:18 INFO 140511108945280] Epoch[170] Batch[5] avg_epoch_loss=-4.508554\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=-4.5085539023081465\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:18 INFO 140511108945280] Epoch[170] Batch [5]#011Speed: 160.50 samples/sec#011loss=-4.508554\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:20 INFO 140511108945280] Epoch[170] Batch[10] avg_epoch_loss=-4.505600\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=-4.502055931091308\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:20 INFO 140511108945280] Epoch[170] Batch [10]#011Speed: 166.37 samples/sec#011loss=-4.502056\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:21 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847316.3579886, \"EndTime\": 1640847321.2990744, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4940.535545349121, \"count\": 1, \"min\": 4940.535545349121, \"max\": 4940.535545349121}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:21 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.2060401752148 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:21 INFO 140511108945280] #progress_metric: host=algo-1, completed 42.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=170, train loss <loss>=-4.4584347407023115\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:21 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:21 INFO 140511108945280] Epoch[171] Batch[0] avg_epoch_loss=-5.014015\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=-5.014014720916748\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:23 INFO 140511108945280] Epoch[171] Batch[5] avg_epoch_loss=-4.568439\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=-4.56843916575114\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:23 INFO 140511108945280] Epoch[171] Batch [5]#011Speed: 170.38 samples/sec#011loss=-4.568439\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:25 INFO 140511108945280] Epoch[171] Batch[10] avg_epoch_loss=-4.664065\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=-4.7788159370422365\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:25 INFO 140511108945280] Epoch[171] Batch [10]#011Speed: 164.47 samples/sec#011loss=-4.778816\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:26 INFO 140511108945280] processed a total of 733 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847321.2991693, \"EndTime\": 1640847326.122445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4822.734117507935, \"count\": 1, \"min\": 4822.734117507935, \"max\": 4822.734117507935}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:26 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.98341760804433 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:26 INFO 140511108945280] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=171, train loss <loss>=-4.778629501660665\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:26 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:26 INFO 140511108945280] Epoch[172] Batch[0] avg_epoch_loss=-4.350480\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=-4.350480079650879\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:28 INFO 140511108945280] Epoch[172] Batch[5] avg_epoch_loss=-4.588837\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=-4.588836749394734\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:28 INFO 140511108945280] Epoch[172] Batch [5]#011Speed: 171.74 samples/sec#011loss=-4.588837\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] Epoch[172] Batch[10] avg_epoch_loss=-4.595488\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=-4.603470325469971\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] Epoch[172] Batch [10]#011Speed: 165.58 samples/sec#011loss=-4.603470\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] processed a total of 750 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847326.1225421, \"EndTime\": 1640847330.929464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4806.299209594727, \"count\": 1, \"min\": 4806.299209594727, \"max\": 4806.299209594727}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.04072687733822 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] #progress_metric: host=algo-1, completed 43.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=172, train loss <loss>=-4.587910413742065\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:30 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:31 INFO 140511108945280] Epoch[173] Batch[0] avg_epoch_loss=-4.531904\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=-4.531904220581055\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:33 INFO 140511108945280] Epoch[173] Batch[5] avg_epoch_loss=-4.787855\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=-4.78785514831543\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:33 INFO 140511108945280] Epoch[173] Batch [5]#011Speed: 170.53 samples/sec#011loss=-4.787855\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] Epoch[173] Batch[10] avg_epoch_loss=-4.735233\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=-4.672086238861084\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] Epoch[173] Batch [10]#011Speed: 156.05 samples/sec#011loss=-4.672086\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847330.9295619, \"EndTime\": 1640847335.8218992, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4891.820907592773, \"count\": 1, \"min\": 4891.820907592773, \"max\": 4891.820907592773}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.9702527896843 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=173, train loss <loss>=-4.751145243644714\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:35 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:36 INFO 140511108945280] Epoch[174] Batch[0] avg_epoch_loss=-3.874435\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=-3.87443470954895\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:38 INFO 140511108945280] Epoch[174] Batch[5] avg_epoch_loss=-4.379836\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=-4.379836201667786\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:38 INFO 140511108945280] Epoch[174] Batch [5]#011Speed: 171.99 samples/sec#011loss=-4.379836\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] Epoch[174] Batch[10] avg_epoch_loss=-4.526961\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=-4.703509998321533\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] Epoch[174] Batch [10]#011Speed: 165.46 samples/sec#011loss=-4.703510\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] processed a total of 734 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847335.8219981, \"EndTime\": 1640847340.601979, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4779.462814331055, \"count\": 1, \"min\": 4779.462814331055, \"max\": 4779.462814331055}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.56924747296455 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] #progress_metric: host=algo-1, completed 43.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=174, train loss <loss>=-4.644115308920543\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:40 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:41 INFO 140511108945280] Epoch[175] Batch[0] avg_epoch_loss=-3.725426\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=-3.725426435470581\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:43 INFO 140511108945280] Epoch[175] Batch[5] avg_epoch_loss=-4.547669\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=-4.547668655713399\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:43 INFO 140511108945280] Epoch[175] Batch [5]#011Speed: 170.88 samples/sec#011loss=-4.547669\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] Epoch[175] Batch[10] avg_epoch_loss=-4.643199\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=-4.757835388183594\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] Epoch[175] Batch [10]#011Speed: 164.29 samples/sec#011loss=-4.757835\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] processed a total of 746 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847340.6020782, \"EndTime\": 1640847345.4105773, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4807.969331741333, \"count\": 1, \"min\": 4807.969331741333, \"max\": 4807.969331741333}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.15458176603997 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=175, train loss <loss>=-4.7264999349912005\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:45 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:46 INFO 140511108945280] Epoch[176] Batch[0] avg_epoch_loss=-4.594697\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=-4.59469747543335\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:47 INFO 140511108945280] Epoch[176] Batch[5] avg_epoch_loss=-4.649338\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=-4.649338483810425\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:47 INFO 140511108945280] Epoch[176] Batch [5]#011Speed: 169.62 samples/sec#011loss=-4.649338\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:49 INFO 140511108945280] Epoch[176] Batch[10] avg_epoch_loss=-4.639639\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=-4.627999019622803\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:49 INFO 140511108945280] Epoch[176] Batch [10]#011Speed: 164.97 samples/sec#011loss=-4.627999\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:50 INFO 140511108945280] processed a total of 750 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847345.4106586, \"EndTime\": 1640847350.217069, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4805.845260620117, \"count\": 1, \"min\": 4805.845260620117, \"max\": 4805.845260620117}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:50 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.0556127902651 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:50 INFO 140511108945280] #progress_metric: host=algo-1, completed 44.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=176, train loss <loss>=-4.642869313557942\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:50 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:50 INFO 140511108945280] Epoch[177] Batch[0] avg_epoch_loss=-4.641683\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=-4.6416826248168945\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:52 INFO 140511108945280] Epoch[177] Batch[5] avg_epoch_loss=-4.210807\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=-4.2108068863550825\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:52 INFO 140511108945280] Epoch[177] Batch [5]#011Speed: 171.14 samples/sec#011loss=-4.210807\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:54 INFO 140511108945280] Epoch[177] Batch[10] avg_epoch_loss=-4.354194\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=-4.526257705688477\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:54 INFO 140511108945280] Epoch[177] Batch [10]#011Speed: 163.88 samples/sec#011loss=-4.526258\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:55 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847350.2171643, \"EndTime\": 1640847355.0184698, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4800.790548324585, \"count\": 1, \"min\": 4800.790548324585, \"max\": 4800.790548324585}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:55 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.92760349438177 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:55 INFO 140511108945280] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=177, train loss <loss>=-4.405214011669159\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:55 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:55 INFO 140511108945280] Epoch[178] Batch[0] avg_epoch_loss=-4.484232\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=-4.484232425689697\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:57 INFO 140511108945280] Epoch[178] Batch[5] avg_epoch_loss=-4.676857\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=-4.676856676737468\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:57 INFO 140511108945280] Epoch[178] Batch [5]#011Speed: 172.13 samples/sec#011loss=-4.676857\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:59 INFO 140511108945280] Epoch[178] Batch[10] avg_epoch_loss=-4.765433\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=-4.871724224090576\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:55:59 INFO 140511108945280] Epoch[178] Batch [10]#011Speed: 163.78 samples/sec#011loss=-4.871724\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] processed a total of 770 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847355.0185664, \"EndTime\": 1640847360.2216074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5202.495813369751, \"count\": 1, \"min\": 5202.495813369751, \"max\": 5202.495813369751}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.00178888179445 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] #progress_metric: host=algo-1, completed 44.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=178, train loss <loss>=-4.887959993802584\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_f5f2047f-731f-4d94-bdb9-a177a774e9a6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847360.2217093, \"EndTime\": 1640847360.323733, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 101.35936737060547, \"count\": 1, \"min\": 101.35936737060547, \"max\": 101.35936737060547}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] Epoch[179] Batch[0] avg_epoch_loss=-4.705181\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=-4.705181121826172\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:03 INFO 140511108945280] Epoch[179] Batch[5] avg_epoch_loss=-4.559524\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=-4.559524456659953\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:03 INFO 140511108945280] Epoch[179] Batch [5]#011Speed: 152.23 samples/sec#011loss=-4.559524\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] Epoch[179] Batch[10] avg_epoch_loss=-4.661636\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=-4.784169769287109\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] Epoch[179] Batch [10]#011Speed: 136.71 samples/sec#011loss=-4.784170\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] processed a total of 740 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847360.3238153, \"EndTime\": 1640847365.7518516, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5427.959203720093, \"count\": 1, \"min\": 5427.959203720093, \"max\": 5427.959203720093}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=136.3282843864486 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=179, train loss <loss>=-4.797880212465922\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:05 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:06 INFO 140511108945280] Epoch[180] Batch[0] avg_epoch_loss=-4.850407\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=-4.850407123565674\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:08 INFO 140511108945280] Epoch[180] Batch[5] avg_epoch_loss=-4.488173\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=-4.488172690073649\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:08 INFO 140511108945280] Epoch[180] Batch [5]#011Speed: 168.76 samples/sec#011loss=-4.488173\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] Epoch[180] Batch[10] avg_epoch_loss=-4.633515\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=-4.807924747467041\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] Epoch[180] Batch [10]#011Speed: 166.04 samples/sec#011loss=-4.807925\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] processed a total of 758 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847365.7519238, \"EndTime\": 1640847370.5699613, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4817.448854446411, \"count\": 1, \"min\": 4817.448854446411, \"max\": 4817.448854446411}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.3402210455994 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] #progress_metric: host=algo-1, completed 45.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=180, train loss <loss>=-4.526602586110433\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:10 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:11 INFO 140511108945280] Epoch[181] Batch[0] avg_epoch_loss=-5.226446\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=-5.226446151733398\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:13 INFO 140511108945280] Epoch[181] Batch[5] avg_epoch_loss=-4.762910\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=-4.762909571329753\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:13 INFO 140511108945280] Epoch[181] Batch [5]#011Speed: 170.86 samples/sec#011loss=-4.762910\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:14 INFO 140511108945280] Epoch[181] Batch[10] avg_epoch_loss=-4.643286\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=181, batch=10 train loss <loss>=-4.499736976623535\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:14 INFO 140511108945280] Epoch[181] Batch [10]#011Speed: 165.17 samples/sec#011loss=-4.499737\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:15 INFO 140511108945280] processed a total of 774 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847370.5700567, \"EndTime\": 1640847375.7342892, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5163.728952407837, \"count\": 1, \"min\": 5163.728952407837, \"max\": 5163.728952407837}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:15 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.88763688237498 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:15 INFO 140511108945280] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=181, train loss <loss>=-4.7998355351961575\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:15 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:16 INFO 140511108945280] Epoch[182] Batch[0] avg_epoch_loss=-4.763215\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=-4.763215065002441\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:18 INFO 140511108945280] Epoch[182] Batch[5] avg_epoch_loss=-4.647064\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=-4.647063970565796\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:18 INFO 140511108945280] Epoch[182] Batch [5]#011Speed: 171.16 samples/sec#011loss=-4.647064\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] Epoch[182] Batch[10] avg_epoch_loss=-4.655658\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=-4.665970039367676\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] Epoch[182] Batch [10]#011Speed: 159.10 samples/sec#011loss=-4.665970\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] processed a total of 758 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847375.734388, \"EndTime\": 1640847380.6213267, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4886.360883712769, \"count\": 1, \"min\": 4886.360883712769, \"max\": 4886.360883712769}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.12137728129903 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] #progress_metric: host=algo-1, completed 45.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=182, train loss <loss>=-4.66523806254069\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:20 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:21 INFO 140511108945280] Epoch[183] Batch[0] avg_epoch_loss=-4.771673\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=-4.77167272567749\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:23 INFO 140511108945280] Epoch[183] Batch[5] avg_epoch_loss=-4.680092\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=-4.680092255274455\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:23 INFO 140511108945280] Epoch[183] Batch [5]#011Speed: 171.93 samples/sec#011loss=-4.680092\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] Epoch[183] Batch[10] avg_epoch_loss=-4.621753\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=-4.551745319366455\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] Epoch[183] Batch [10]#011Speed: 163.55 samples/sec#011loss=-4.551745\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] processed a total of 766 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847380.6214232, \"EndTime\": 1640847385.4540558, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4832.156181335449, \"count\": 1, \"min\": 4832.156181335449, \"max\": 4832.156181335449}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.51686908895834 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=183, train loss <loss>=-4.589641571044922\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:25 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:26 INFO 140511108945280] Epoch[184] Batch[0] avg_epoch_loss=-4.930294\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=-4.930294036865234\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:27 INFO 140511108945280] Epoch[184] Batch[5] avg_epoch_loss=-4.841028\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=-4.841027736663818\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:27 INFO 140511108945280] Epoch[184] Batch [5]#011Speed: 173.13 samples/sec#011loss=-4.841028\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:29 INFO 140511108945280] Epoch[184] Batch[10] avg_epoch_loss=-4.740508\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=184, batch=10 train loss <loss>=-4.619884872436524\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:29 INFO 140511108945280] Epoch[184] Batch [10]#011Speed: 165.64 samples/sec#011loss=-4.619885\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] processed a total of 726 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847385.4541547, \"EndTime\": 1640847390.2294486, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4774.681568145752, \"count\": 1, \"min\": 4774.681568145752, \"max\": 4774.681568145752}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.04603446653073 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] #progress_metric: host=algo-1, completed 46.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=184, train loss <loss>=-4.91073493162791\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_d16a07d1-57cb-4da5-b788-75d7dc45051b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847390.229561, \"EndTime\": 1640847390.3575487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 127.43711471557617, \"count\": 1, \"min\": 127.43711471557617, \"max\": 127.43711471557617}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] Epoch[185] Batch[0] avg_epoch_loss=-4.470938\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=-4.470938205718994\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:32 INFO 140511108945280] Epoch[185] Batch[5] avg_epoch_loss=-4.503752\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=-4.50375231107076\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:32 INFO 140511108945280] Epoch[185] Batch [5]#011Speed: 172.48 samples/sec#011loss=-4.503752\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:34 INFO 140511108945280] Epoch[185] Batch[10] avg_epoch_loss=-4.589883\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=-4.693239974975586\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:34 INFO 140511108945280] Epoch[185] Batch [10]#011Speed: 161.26 samples/sec#011loss=-4.693240\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:35 INFO 140511108945280] processed a total of 786 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847390.3576176, \"EndTime\": 1640847395.5974016, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5239.707946777344, \"count\": 1, \"min\": 5239.707946777344, \"max\": 5239.707946777344}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:35 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.0043951295845 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:35 INFO 140511108945280] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=185, train loss <loss>=-4.720212569603553\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:35 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:36 INFO 140511108945280] Epoch[186] Batch[0] avg_epoch_loss=-4.795743\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=-4.795743465423584\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:38 INFO 140511108945280] Epoch[186] Batch[5] avg_epoch_loss=-4.741199\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=-4.741198698679606\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:38 INFO 140511108945280] Epoch[186] Batch [5]#011Speed: 172.77 samples/sec#011loss=-4.741199\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] Epoch[186] Batch[10] avg_epoch_loss=-4.685166\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=-4.6179274559021\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] Epoch[186] Batch [10]#011Speed: 161.00 samples/sec#011loss=-4.617927\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] processed a total of 769 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847395.5974987, \"EndTime\": 1640847400.8098483, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5211.832523345947, \"count\": 1, \"min\": 5211.832523345947, \"max\": 5211.832523345947}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.54499587019268 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] #progress_metric: host=algo-1, completed 46.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=186, train loss <loss>=-4.946698738978459\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:40 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_35df6e92-ae82-4f39-aad9-a0f56fa1ce56-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847400.8099463, \"EndTime\": 1640847400.9172132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 106.70089721679688, \"count\": 1, \"min\": 106.70089721679688, \"max\": 106.70089721679688}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:41 INFO 140511108945280] Epoch[187] Batch[0] avg_epoch_loss=-3.988148\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=-3.9881479740142822\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:43 INFO 140511108945280] Epoch[187] Batch[5] avg_epoch_loss=-4.477051\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=-4.477050503094991\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:43 INFO 140511108945280] Epoch[187] Batch [5]#011Speed: 170.56 samples/sec#011loss=-4.477051\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] Epoch[187] Batch[10] avg_epoch_loss=-4.612038\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=-4.774021911621094\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] Epoch[187] Batch [10]#011Speed: 165.49 samples/sec#011loss=-4.774022\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] processed a total of 736 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847400.9173079, \"EndTime\": 1640847405.714627, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4797.248363494873, \"count\": 1, \"min\": 4797.248363494873, \"max\": 4797.248363494873}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.4168990651035 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=187, train loss <loss>=-4.786329289277394\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:45 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:46 INFO 140511108945280] Epoch[188] Batch[0] avg_epoch_loss=-4.297187\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=-4.297186851501465\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:48 INFO 140511108945280] Epoch[188] Batch[5] avg_epoch_loss=-4.611860\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=-4.611860275268555\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:48 INFO 140511108945280] Epoch[188] Batch [5]#011Speed: 170.47 samples/sec#011loss=-4.611860\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] Epoch[188] Batch[10] avg_epoch_loss=-4.702079\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=-4.8103409767150875\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] Epoch[188] Batch [10]#011Speed: 166.01 samples/sec#011loss=-4.810341\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] processed a total of 748 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847405.7147262, \"EndTime\": 1640847410.5082507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4792.959928512573, \"count\": 1, \"min\": 4792.959928512573, \"max\": 4792.959928512573}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.05808931148073 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] #progress_metric: host=algo-1, completed 47.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=188, train loss <loss>=-4.761960506439209\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:50 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:51 INFO 140511108945280] Epoch[189] Batch[0] avg_epoch_loss=-4.343342\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=-4.343341827392578\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:53 INFO 140511108945280] Epoch[189] Batch[5] avg_epoch_loss=-4.592176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=-4.592175642649333\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:53 INFO 140511108945280] Epoch[189] Batch [5]#011Speed: 168.32 samples/sec#011loss=-4.592176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:54 INFO 140511108945280] Epoch[189] Batch[10] avg_epoch_loss=-4.572340\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=-4.548536968231201\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:54 INFO 140511108945280] Epoch[189] Batch [10]#011Speed: 165.46 samples/sec#011loss=-4.548537\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:55 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847410.5083392, \"EndTime\": 1640847415.3460708, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4836.95912361145, \"count\": 1, \"min\": 4836.95912361145, \"max\": 4836.95912361145}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:55 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.4316396602898 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:55 INFO 140511108945280] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=189, train loss <loss>=-4.595410704612732\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:55 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:55 INFO 140511108945280] Epoch[190] Batch[0] avg_epoch_loss=-4.939464\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=-4.9394636154174805\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:57 INFO 140511108945280] Epoch[190] Batch[5] avg_epoch_loss=-4.941968\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=-4.941968123118083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:57 INFO 140511108945280] Epoch[190] Batch [5]#011Speed: 172.35 samples/sec#011loss=-4.941968\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:59 INFO 140511108945280] Epoch[190] Batch[10] avg_epoch_loss=-4.808898\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=-4.649213886260986\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:56:59 INFO 140511108945280] Epoch[190] Batch [10]#011Speed: 167.29 samples/sec#011loss=-4.649214\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:00 INFO 140511108945280] processed a total of 735 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847415.3461652, \"EndTime\": 1640847420.1601744, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4813.509941101074, \"count\": 1, \"min\": 4813.509941101074, \"max\": 4813.509941101074}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:00 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.69098560987345 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:00 INFO 140511108945280] #progress_metric: host=algo-1, completed 47.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=190, train loss <loss>=-4.546491851409276\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:00 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:00 INFO 140511108945280] Epoch[191] Batch[0] avg_epoch_loss=-4.587255\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=-4.587254524230957\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:02 INFO 140511108945280] Epoch[191] Batch[5] avg_epoch_loss=-4.790415\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=-4.790415207544963\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:02 INFO 140511108945280] Epoch[191] Batch [5]#011Speed: 159.00 samples/sec#011loss=-4.790415\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] Epoch[191] Batch[10] avg_epoch_loss=-4.695674\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=-4.5819854736328125\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] Epoch[191] Batch [10]#011Speed: 136.04 samples/sec#011loss=-4.581985\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] processed a total of 714 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847420.1602688, \"EndTime\": 1640847425.6502538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5489.408254623413, \"count\": 1, \"min\": 5489.408254623413, \"max\": 5489.408254623413}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=130.06542183324683 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=191, train loss <loss>=-4.621886074542999\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:05 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:06 INFO 140511108945280] Epoch[192] Batch[0] avg_epoch_loss=-5.150981\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=-5.1509809494018555\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:08 INFO 140511108945280] Epoch[192] Batch[5] avg_epoch_loss=-4.694007\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=-4.694007237752278\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:08 INFO 140511108945280] Epoch[192] Batch [5]#011Speed: 169.52 samples/sec#011loss=-4.694007\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] Epoch[192] Batch[10] avg_epoch_loss=-4.675577\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=-4.653460693359375\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] Epoch[192] Batch [10]#011Speed: 165.12 samples/sec#011loss=-4.653461\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] processed a total of 754 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847425.6503513, \"EndTime\": 1640847430.521156, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4870.240688323975, \"count\": 1, \"min\": 4870.240688323975, \"max\": 4870.240688323975}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.8134754517878 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] #progress_metric: host=algo-1, completed 48.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=192, train loss <loss>=-4.687387466430664\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:10 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:11 INFO 140511108945280] Epoch[193] Batch[0] avg_epoch_loss=-4.343574\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=-4.343574047088623\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:13 INFO 140511108945280] Epoch[193] Batch[5] avg_epoch_loss=-4.698120\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=-4.698119759559631\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:13 INFO 140511108945280] Epoch[193] Batch [5]#011Speed: 171.00 samples/sec#011loss=-4.698120\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:14 INFO 140511108945280] Epoch[193] Batch[10] avg_epoch_loss=-4.645978\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=193, batch=10 train loss <loss>=-4.583407974243164\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:14 INFO 140511108945280] Epoch[193] Batch [10]#011Speed: 165.37 samples/sec#011loss=-4.583408\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:15 INFO 140511108945280] processed a total of 755 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847430.5212536, \"EndTime\": 1640847435.3622258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4840.409517288208, \"count\": 1, \"min\": 4840.409517288208, \"max\": 4840.409517288208}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:15 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.97408210492293 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:15 INFO 140511108945280] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=193, train loss <loss>=-4.723785817623138\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:15 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:15 INFO 140511108945280] Epoch[194] Batch[0] avg_epoch_loss=-4.388519\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=-4.388519287109375\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:17 INFO 140511108945280] Epoch[194] Batch[5] avg_epoch_loss=-4.516569\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=-4.516569058100383\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:17 INFO 140511108945280] Epoch[194] Batch [5]#011Speed: 168.68 samples/sec#011loss=-4.516569\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:19 INFO 140511108945280] Epoch[194] Batch[10] avg_epoch_loss=-4.503465\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=-4.487739086151123\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:19 INFO 140511108945280] Epoch[194] Batch [10]#011Speed: 164.52 samples/sec#011loss=-4.487739\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:20 INFO 140511108945280] processed a total of 739 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847435.3623245, \"EndTime\": 1640847440.2061121, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4843.253135681152, \"count\": 1, \"min\": 4843.253135681152, \"max\": 4843.253135681152}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:20 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.57907487556642 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:20 INFO 140511108945280] #progress_metric: host=algo-1, completed 48.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=194, train loss <loss>=-4.32802426815033\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:20 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:20 INFO 140511108945280] Epoch[195] Batch[0] avg_epoch_loss=-4.913496\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=-4.913496494293213\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:22 INFO 140511108945280] Epoch[195] Batch[5] avg_epoch_loss=-4.672415\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=-4.672414859135945\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:22 INFO 140511108945280] Epoch[195] Batch [5]#011Speed: 168.53 samples/sec#011loss=-4.672415\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:24 INFO 140511108945280] Epoch[195] Batch[10] avg_epoch_loss=-4.691750\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=-4.714951229095459\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:24 INFO 140511108945280] Epoch[195] Batch [10]#011Speed: 166.35 samples/sec#011loss=-4.714951\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:25 INFO 140511108945280] processed a total of 739 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847440.2062094, \"EndTime\": 1640847445.0387738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4832.046747207642, \"count\": 1, \"min\": 4832.046747207642, \"max\": 4832.046747207642}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:25 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.9328251543185 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:25 INFO 140511108945280] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=195, train loss <loss>=-4.771443406740825\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:25 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:25 INFO 140511108945280] Epoch[196] Batch[0] avg_epoch_loss=-4.360593\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=-4.360592842102051\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:27 INFO 140511108945280] Epoch[196] Batch[5] avg_epoch_loss=-4.588083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=-4.588083108266194\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:27 INFO 140511108945280] Epoch[196] Batch [5]#011Speed: 169.70 samples/sec#011loss=-4.588083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:29 INFO 140511108945280] Epoch[196] Batch[10] avg_epoch_loss=-4.515012\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=-4.427327299118042\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:29 INFO 140511108945280] Epoch[196] Batch [10]#011Speed: 166.03 samples/sec#011loss=-4.427327\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:30 INFO 140511108945280] processed a total of 784 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847445.0388727, \"EndTime\": 1640847450.2236302, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5184.162378311157, \"count\": 1, \"min\": 5184.162378311157, \"max\": 5184.162378311157}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:30 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.2252129343092 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:30 INFO 140511108945280] #progress_metric: host=algo-1, completed 49.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=196, train loss <loss>=-4.5037366060110235\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:30 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:30 INFO 140511108945280] Epoch[197] Batch[0] avg_epoch_loss=-5.004202\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=-5.004201889038086\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:32 INFO 140511108945280] Epoch[197] Batch[5] avg_epoch_loss=-4.799985\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=-4.799985488255818\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:32 INFO 140511108945280] Epoch[197] Batch [5]#011Speed: 171.26 samples/sec#011loss=-4.799985\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:34 INFO 140511108945280] Epoch[197] Batch[10] avg_epoch_loss=-4.810098\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=-4.8222321510314945\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:34 INFO 140511108945280] Epoch[197] Batch [10]#011Speed: 165.00 samples/sec#011loss=-4.822232\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:35 INFO 140511108945280] processed a total of 728 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847450.2237482, \"EndTime\": 1640847455.0800242, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4855.691194534302, \"count\": 1, \"min\": 4855.691194534302, \"max\": 4855.691194534302}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:35 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.92183971615216 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:35 INFO 140511108945280] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=197, train loss <loss>=-4.781183203061421\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:35 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:35 INFO 140511108945280] Epoch[198] Batch[0] avg_epoch_loss=-4.544227\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=-4.54422664642334\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:37 INFO 140511108945280] Epoch[198] Batch[5] avg_epoch_loss=-4.506413\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=-4.506413300832112\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:37 INFO 140511108945280] Epoch[198] Batch [5]#011Speed: 168.10 samples/sec#011loss=-4.506413\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] Epoch[198] Batch[10] avg_epoch_loss=-4.559712\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=-4.623670959472657\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] Epoch[198] Batch [10]#011Speed: 167.71 samples/sec#011loss=-4.623671\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] processed a total of 729 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847455.0801573, \"EndTime\": 1640847459.9563322, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4875.666379928589, \"count\": 1, \"min\": 4875.666379928589, \"max\": 4875.666379928589}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.51433126531867 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] #progress_metric: host=algo-1, completed 49.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=198, train loss <loss>=-4.695736567179362\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:39 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:40 INFO 140511108945280] Epoch[199] Batch[0] avg_epoch_loss=-4.736114\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=-4.736114025115967\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:42 INFO 140511108945280] Epoch[199] Batch[5] avg_epoch_loss=-4.705579\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=-4.705579201380412\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:42 INFO 140511108945280] Epoch[199] Batch [5]#011Speed: 167.78 samples/sec#011loss=-4.705579\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] Epoch[199] Batch[10] avg_epoch_loss=-4.710614\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=-4.716656112670899\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] Epoch[199] Batch [10]#011Speed: 167.22 samples/sec#011loss=-4.716656\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] processed a total of 743 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847459.9564207, \"EndTime\": 1640847464.7362335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4779.221773147583, \"count\": 1, \"min\": 4779.221773147583, \"max\": 4779.221773147583}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.46038388195825 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=199, train loss <loss>=-4.670080224672954\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:44 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:45 INFO 140511108945280] Epoch[200] Batch[0] avg_epoch_loss=-4.456890\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=-4.456889629364014\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:47 INFO 140511108945280] Epoch[200] Batch[5] avg_epoch_loss=-4.870225\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=-4.870225111643474\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:47 INFO 140511108945280] Epoch[200] Batch [5]#011Speed: 165.67 samples/sec#011loss=-4.870225\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] Epoch[200] Batch[10] avg_epoch_loss=-4.728920\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=-4.559354400634765\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] Epoch[200] Batch [10]#011Speed: 166.53 samples/sec#011loss=-4.559354\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] processed a total of 701 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847464.7363236, \"EndTime\": 1640847469.1850696, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4448.189735412598, \"count\": 1, \"min\": 4448.189735412598, \"max\": 4448.189735412598}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.5883300233182 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] #progress_metric: host=algo-1, completed 50.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=200, train loss <loss>=-4.728920243003151\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] Epoch[201] Batch[0] avg_epoch_loss=-4.599037\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=-4.599037170410156\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:51 INFO 140511108945280] Epoch[201] Batch[5] avg_epoch_loss=-4.669701\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=-4.669701178868611\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:51 INFO 140511108945280] Epoch[201] Batch [5]#011Speed: 167.15 samples/sec#011loss=-4.669701\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] Epoch[201] Batch[10] avg_epoch_loss=-4.722511\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=201, batch=10 train loss <loss>=-4.785882472991943\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] Epoch[201] Batch [10]#011Speed: 165.88 samples/sec#011loss=-4.785882\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] processed a total of 749 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847469.185149, \"EndTime\": 1640847473.9891891, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4803.434133529663, \"count\": 1, \"min\": 4803.434133529663, \"max\": 4803.434133529663}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.9253785821629 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=201, train loss <loss>=-4.772811055183411\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:53 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:54 INFO 140511108945280] Epoch[202] Batch[0] avg_epoch_loss=-4.965270\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=-4.965269565582275\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:56 INFO 140511108945280] Epoch[202] Batch[5] avg_epoch_loss=-4.766505\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=-4.766504764556885\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:56 INFO 140511108945280] Epoch[202] Batch [5]#011Speed: 170.15 samples/sec#011loss=-4.766505\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] Epoch[202] Batch[10] avg_epoch_loss=-4.741953\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=202, batch=10 train loss <loss>=-4.7124913215637205\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] Epoch[202] Batch [10]#011Speed: 166.33 samples/sec#011loss=-4.712491\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] processed a total of 755 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847473.989294, \"EndTime\": 1640847478.7728727, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4782.967090606689, \"count\": 1, \"min\": 4782.967090606689, \"max\": 4782.967090606689}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.8460242629419 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] #progress_metric: host=algo-1, completed 50.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=202, train loss <loss>=-4.815917372703552\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:58 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:59 INFO 140511108945280] Epoch[203] Batch[0] avg_epoch_loss=-4.951574\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:57:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=-4.951574325561523\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:01 INFO 140511108945280] Epoch[203] Batch[5] avg_epoch_loss=-4.918688\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=-4.918687502543132\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:01 INFO 140511108945280] Epoch[203] Batch [5]#011Speed: 165.60 samples/sec#011loss=-4.918688\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:03 INFO 140511108945280] Epoch[203] Batch[10] avg_epoch_loss=-4.729234\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=203, batch=10 train loss <loss>=-4.501889991760254\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:03 INFO 140511108945280] Epoch[203] Batch [10]#011Speed: 151.45 samples/sec#011loss=-4.501890\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:04 INFO 140511108945280] processed a total of 803 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847478.7730057, \"EndTime\": 1640847484.2342954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5460.755109786987, \"count\": 1, \"min\": 5460.755109786987, \"max\": 5460.755109786987}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:04 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.0402276765807 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:04 INFO 140511108945280] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=203, train loss <loss>=-4.792529582977295\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:04 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:04 INFO 140511108945280] Epoch[204] Batch[0] avg_epoch_loss=-4.076090\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=-4.076090335845947\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:07 INFO 140511108945280] Epoch[204] Batch[5] avg_epoch_loss=-4.602128\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=-4.602127869923909\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:07 INFO 140511108945280] Epoch[204] Batch [5]#011Speed: 155.77 samples/sec#011loss=-4.602128\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:08 INFO 140511108945280] Epoch[204] Batch[10] avg_epoch_loss=-4.708796\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=204, batch=10 train loss <loss>=-4.836798477172851\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:08 INFO 140511108945280] Epoch[204] Batch [10]#011Speed: 165.22 samples/sec#011loss=-4.836798\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:09 INFO 140511108945280] processed a total of 766 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847484.2345846, \"EndTime\": 1640847489.3161862, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5081.030368804932, \"count\": 1, \"min\": 5081.030368804932, \"max\": 5081.030368804932}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:09 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.75262902614534 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:09 INFO 140511108945280] #progress_metric: host=algo-1, completed 51.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=204, train loss <loss>=-4.70603350798289\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:09 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:09 INFO 140511108945280] Epoch[205] Batch[0] avg_epoch_loss=-5.090145\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=-5.090145111083984\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:11 INFO 140511108945280] Epoch[205] Batch[5] avg_epoch_loss=-4.800083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=-4.800082763036092\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:11 INFO 140511108945280] Epoch[205] Batch [5]#011Speed: 168.85 samples/sec#011loss=-4.800083\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:13 INFO 140511108945280] Epoch[205] Batch[10] avg_epoch_loss=-4.716742\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=205, batch=10 train loss <loss>=-4.616733455657959\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:13 INFO 140511108945280] Epoch[205] Batch [10]#011Speed: 165.86 samples/sec#011loss=-4.616733\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:14 INFO 140511108945280] processed a total of 767 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847489.3162844, \"EndTime\": 1640847494.1405191, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4823.707103729248, \"count\": 1, \"min\": 4823.707103729248, \"max\": 4823.707103729248}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:14 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=159.00167723644418 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:14 INFO 140511108945280] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=205, train loss <loss>=-4.732022960980733\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:14 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:14 INFO 140511108945280] Epoch[206] Batch[0] avg_epoch_loss=-4.910511\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=-4.910510540008545\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:16 INFO 140511108945280] Epoch[206] Batch[5] avg_epoch_loss=-4.792767\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=-4.792766729990642\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:16 INFO 140511108945280] Epoch[206] Batch [5]#011Speed: 168.50 samples/sec#011loss=-4.792767\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] Epoch[206] Batch[10] avg_epoch_loss=-4.751520\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=206, batch=10 train loss <loss>=-4.702023696899414\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] Epoch[206] Batch [10]#011Speed: 166.69 samples/sec#011loss=-4.702024\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] processed a total of 735 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847494.1406195, \"EndTime\": 1640847498.9300468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4788.818836212158, \"count\": 1, \"min\": 4788.818836212158, \"max\": 4788.818836212158}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.47806880744108 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 51.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=206, train loss <loss>=-4.822625557581584\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:19 INFO 140511108945280] Epoch[207] Batch[0] avg_epoch_loss=-5.153329\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=-5.153328895568848\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:21 INFO 140511108945280] Epoch[207] Batch[5] avg_epoch_loss=-4.983993\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=-4.983993212381999\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:21 INFO 140511108945280] Epoch[207] Batch [5]#011Speed: 167.22 samples/sec#011loss=-4.983993\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:23 INFO 140511108945280] Epoch[207] Batch[10] avg_epoch_loss=-4.870004\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=207, batch=10 train loss <loss>=-4.733217716217041\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:23 INFO 140511108945280] Epoch[207] Batch [10]#011Speed: 163.99 samples/sec#011loss=-4.733218\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:24 INFO 140511108945280] processed a total of 792 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847498.9301455, \"EndTime\": 1640847504.1263158, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5195.568799972534, \"count\": 1, \"min\": 5195.568799972534, \"max\": 5195.568799972534}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:24 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.4316399211 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:24 INFO 140511108945280] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=207, train loss <loss>=-4.683484297532302\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:24 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:24 INFO 140511108945280] Epoch[208] Batch[0] avg_epoch_loss=-4.414656\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=-4.414655685424805\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:26 INFO 140511108945280] Epoch[208] Batch[5] avg_epoch_loss=-4.294332\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=-4.294332464536031\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:26 INFO 140511108945280] Epoch[208] Batch [5]#011Speed: 164.81 samples/sec#011loss=-4.294332\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] Epoch[208] Batch[10] avg_epoch_loss=-4.585858\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=208, batch=10 train loss <loss>=-4.93568868637085\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] Epoch[208] Batch [10]#011Speed: 165.80 samples/sec#011loss=-4.935689\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] processed a total of 766 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847504.1264775, \"EndTime\": 1640847508.9707108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4843.724012374878, \"count\": 1, \"min\": 4843.724012374878, \"max\": 4843.724012374878}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.13823814819727 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] #progress_metric: host=algo-1, completed 52.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=208, train loss <loss>=-4.609842320283254\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:28 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:29 INFO 140511108945280] Epoch[209] Batch[0] avg_epoch_loss=-4.403655\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=-4.4036545753479\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:31 INFO 140511108945280] Epoch[209] Batch[5] avg_epoch_loss=-4.664992\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=-4.664992014567058\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:31 INFO 140511108945280] Epoch[209] Batch [5]#011Speed: 166.25 samples/sec#011loss=-4.664992\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:33 INFO 140511108945280] Epoch[209] Batch[10] avg_epoch_loss=-4.533054\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=-4.374728488922119\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:33 INFO 140511108945280] Epoch[209] Batch [10]#011Speed: 164.15 samples/sec#011loss=-4.374728\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:34 INFO 140511108945280] processed a total of 804 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847508.9708111, \"EndTime\": 1640847514.2071092, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5235.678911209106, \"count\": 1, \"min\": 5235.678911209106, \"max\": 5235.678911209106}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:34 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.55796393263017 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:34 INFO 140511108945280] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=209, train loss <loss>=-4.610665321350098\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:34 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:34 INFO 140511108945280] Epoch[210] Batch[0] avg_epoch_loss=-4.522635\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=-4.522635459899902\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:36 INFO 140511108945280] Epoch[210] Batch[5] avg_epoch_loss=-4.783407\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=-4.783406893412272\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:36 INFO 140511108945280] Epoch[210] Batch [5]#011Speed: 163.69 samples/sec#011loss=-4.783407\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:38 INFO 140511108945280] Epoch[210] Batch[10] avg_epoch_loss=-4.766291\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=210, batch=10 train loss <loss>=-4.745751094818115\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:38 INFO 140511108945280] Epoch[210] Batch [10]#011Speed: 160.13 samples/sec#011loss=-4.745751\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:39 INFO 140511108945280] processed a total of 761 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847514.2072, \"EndTime\": 1640847519.1221755, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4914.257049560547, \"count\": 1, \"min\": 4914.257049560547, \"max\": 4914.257049560547}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:39 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.85124963972035 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:39 INFO 140511108945280] #progress_metric: host=algo-1, completed 52.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=210, train loss <loss>=-4.749636729558309\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:39 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:39 INFO 140511108945280] Epoch[211] Batch[0] avg_epoch_loss=-4.126263\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=-4.126262664794922\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:41 INFO 140511108945280] Epoch[211] Batch[5] avg_epoch_loss=-4.768392\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=-4.768391768137614\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:41 INFO 140511108945280] Epoch[211] Batch [5]#011Speed: 167.46 samples/sec#011loss=-4.768392\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] Epoch[211] Batch[10] avg_epoch_loss=-4.770300\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=211, batch=10 train loss <loss>=-4.772589683532715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] Epoch[211] Batch [10]#011Speed: 165.57 samples/sec#011loss=-4.772590\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] processed a total of 755 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847519.1222715, \"EndTime\": 1640847523.9825652, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4859.760284423828, \"count\": 1, \"min\": 4859.760284423828, \"max\": 4859.760284423828}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.3507045015948 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=211, train loss <loss>=-4.827670653661092\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:43 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:44 INFO 140511108945280] Epoch[212] Batch[0] avg_epoch_loss=-5.119088\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=-5.119088172912598\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:46 INFO 140511108945280] Epoch[212] Batch[5] avg_epoch_loss=-4.746015\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=-4.746014992396037\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:46 INFO 140511108945280] Epoch[212] Batch [5]#011Speed: 168.80 samples/sec#011loss=-4.746015\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:48 INFO 140511108945280] Epoch[212] Batch[10] avg_epoch_loss=-4.730942\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=212, batch=10 train loss <loss>=-4.7128533840179445\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:48 INFO 140511108945280] Epoch[212] Batch [10]#011Speed: 159.89 samples/sec#011loss=-4.712853\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:49 INFO 140511108945280] processed a total of 788 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847523.9827049, \"EndTime\": 1640847529.2371738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5253.889799118042, \"count\": 1, \"min\": 5253.889799118042, \"max\": 5253.889799118042}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:49 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.98043068751068 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:49 INFO 140511108945280] #progress_metric: host=algo-1, completed 53.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=212, train loss <loss>=-4.892963024286123\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:49 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:49 INFO 140511108945280] Epoch[213] Batch[0] avg_epoch_loss=-5.497428\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=-5.497427940368652\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:51 INFO 140511108945280] Epoch[213] Batch[5] avg_epoch_loss=-4.821556\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=-4.821556091308594\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:51 INFO 140511108945280] Epoch[213] Batch [5]#011Speed: 165.93 samples/sec#011loss=-4.821556\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:53 INFO 140511108945280] Epoch[213] Batch[10] avg_epoch_loss=-4.681583\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=213, batch=10 train loss <loss>=-4.513614654541016\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:53 INFO 140511108945280] Epoch[213] Batch [10]#011Speed: 165.66 samples/sec#011loss=-4.513615\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:54 INFO 140511108945280] processed a total of 737 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847529.2372608, \"EndTime\": 1640847534.0734174, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4835.578918457031, \"count\": 1, \"min\": 4835.578918457031, \"max\": 4835.578918457031}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:54 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.407844541735 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:54 INFO 140511108945280] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=213, train loss <loss>=-4.750088532765706\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:54 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:54 INFO 140511108945280] Epoch[214] Batch[0] avg_epoch_loss=-4.744128\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=-4.7441277503967285\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:56 INFO 140511108945280] Epoch[214] Batch[5] avg_epoch_loss=-4.742824\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=-4.742823759714763\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:56 INFO 140511108945280] Epoch[214] Batch [5]#011Speed: 167.95 samples/sec#011loss=-4.742824\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:58 INFO 140511108945280] Epoch[214] Batch[10] avg_epoch_loss=-4.765917\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=214, batch=10 train loss <loss>=-4.7936280250549315\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:58 INFO 140511108945280] Epoch[214] Batch [10]#011Speed: 163.13 samples/sec#011loss=-4.793628\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:59 INFO 140511108945280] processed a total of 805 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847534.0735102, \"EndTime\": 1640847539.3266475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5252.671957015991, \"count\": 1, \"min\": 5252.671957015991, \"max\": 5252.671957015991}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:59 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.25199599142746 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:59 INFO 140511108945280] #progress_metric: host=algo-1, completed 53.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=214, train loss <loss>=-4.687072277069092\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:59 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:59 INFO 140511108945280] Epoch[215] Batch[0] avg_epoch_loss=-4.531530\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:58:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=-4.531530380249023\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:01 INFO 140511108945280] Epoch[215] Batch[5] avg_epoch_loss=-4.862823\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=-4.862823327382405\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:01 INFO 140511108945280] Epoch[215] Batch [5]#011Speed: 159.33 samples/sec#011loss=-4.862823\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:04 INFO 140511108945280] Epoch[215] Batch[10] avg_epoch_loss=-4.823188\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=215, batch=10 train loss <loss>=-4.775625801086425\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:04 INFO 140511108945280] Epoch[215] Batch [10]#011Speed: 145.95 samples/sec#011loss=-4.775626\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] processed a total of 785 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847539.3267272, \"EndTime\": 1640847545.1339118, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5806.596040725708, \"count\": 1, \"min\": 5806.596040725708, \"max\": 5806.596040725708}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=135.18799549254092 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=215, train loss <loss>=-5.0139785546522875\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_ce123f05-ccbe-47ad-87c7-06d933c8e830-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847545.1340008, \"EndTime\": 1640847545.2400513, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 105.43298721313477, \"count\": 1, \"min\": 105.43298721313477, \"max\": 105.43298721313477}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] Epoch[216] Batch[0] avg_epoch_loss=-4.525437\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=-4.525437355041504\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:07 INFO 140511108945280] Epoch[216] Batch[5] avg_epoch_loss=-4.700847\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=-4.700846751530965\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:07 INFO 140511108945280] Epoch[216] Batch [5]#011Speed: 159.05 samples/sec#011loss=-4.700847\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:09 INFO 140511108945280] Epoch[216] Batch[10] avg_epoch_loss=-4.640688\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=216, batch=10 train loss <loss>=-4.568496608734131\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:09 INFO 140511108945280] Epoch[216] Batch [10]#011Speed: 168.27 samples/sec#011loss=-4.568497\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:10 INFO 140511108945280] processed a total of 711 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847545.2401419, \"EndTime\": 1640847550.1361592, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4895.954370498657, \"count\": 1, \"min\": 4895.954370498657, \"max\": 4895.954370498657}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:10 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=145.21788252114186 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:10 INFO 140511108945280] #progress_metric: host=algo-1, completed 54.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=216, train loss <loss>=-4.800400614738464\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:10 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:10 INFO 140511108945280] Epoch[217] Batch[0] avg_epoch_loss=-4.425176\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=-4.42517614364624\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:12 INFO 140511108945280] Epoch[217] Batch[5] avg_epoch_loss=-4.813142\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=-4.813141981760661\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:12 INFO 140511108945280] Epoch[217] Batch [5]#011Speed: 166.20 samples/sec#011loss=-4.813142\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:14 INFO 140511108945280] Epoch[217] Batch[10] avg_epoch_loss=-4.839416\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=217, batch=10 train loss <loss>=-4.870944690704346\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:14 INFO 140511108945280] Epoch[217] Batch [10]#011Speed: 163.87 samples/sec#011loss=-4.870945\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:15 INFO 140511108945280] processed a total of 746 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847550.1362565, \"EndTime\": 1640847555.0189266, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4882.164239883423, \"count\": 1, \"min\": 4882.164239883423, \"max\": 4882.164239883423}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:15 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=152.79657206803475 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:15 INFO 140511108945280] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=217, train loss <loss>=-4.8348550001780195\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:15 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:15 INFO 140511108945280] Epoch[218] Batch[0] avg_epoch_loss=-4.882478\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=-4.8824782371521\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:17 INFO 140511108945280] Epoch[218] Batch[5] avg_epoch_loss=-4.592073\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=-4.592073440551758\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:17 INFO 140511108945280] Epoch[218] Batch [5]#011Speed: 168.92 samples/sec#011loss=-4.592073\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] Epoch[218] Batch[10] avg_epoch_loss=-4.638990\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=218, batch=10 train loss <loss>=-4.695290756225586\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] Epoch[218] Batch [10]#011Speed: 163.59 samples/sec#011loss=-4.695291\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] processed a total of 735 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847555.0190308, \"EndTime\": 1640847559.8556519, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4836.044788360596, \"count\": 1, \"min\": 4836.044788360596, \"max\": 4836.044788360596}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.97971798390228 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] #progress_metric: host=algo-1, completed 54.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=218, train loss <loss>=-4.803179621696472\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:19 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:20 INFO 140511108945280] Epoch[219] Batch[0] avg_epoch_loss=-4.395347\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=-4.395346641540527\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:22 INFO 140511108945280] Epoch[219] Batch[5] avg_epoch_loss=-4.691253\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=-4.691253026326497\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:22 INFO 140511108945280] Epoch[219] Batch [5]#011Speed: 166.37 samples/sec#011loss=-4.691253\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] Epoch[219] Batch[10] avg_epoch_loss=-4.796664\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=219, batch=10 train loss <loss>=-4.923158264160156\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] Epoch[219] Batch [10]#011Speed: 164.93 samples/sec#011loss=-4.923158\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] processed a total of 720 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847559.8557374, \"EndTime\": 1640847564.70031, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4843.937873840332, \"count\": 1, \"min\": 4843.937873840332, \"max\": 4843.937873840332}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.63516017475945 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=219, train loss <loss>=-4.763603846232097\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:24 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:25 INFO 140511108945280] Epoch[220] Batch[0] avg_epoch_loss=-5.450326\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=-5.450326442718506\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:27 INFO 140511108945280] Epoch[220] Batch[5] avg_epoch_loss=-4.921682\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=-4.921682039896647\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:27 INFO 140511108945280] Epoch[220] Batch [5]#011Speed: 168.24 samples/sec#011loss=-4.921682\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] Epoch[220] Batch[10] avg_epoch_loss=-4.815710\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=220, batch=10 train loss <loss>=-4.688542652130127\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] Epoch[220] Batch [10]#011Speed: 164.43 samples/sec#011loss=-4.688543\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] processed a total of 758 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847564.700407, \"EndTime\": 1640847569.5624046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4861.470699310303, \"count\": 1, \"min\": 4861.470699310303, \"max\": 4861.470699310303}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.91571981251366 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] #progress_metric: host=algo-1, completed 55.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=220, train loss <loss>=-4.804399013519287\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:29 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:30 INFO 140511108945280] Epoch[221] Batch[0] avg_epoch_loss=-4.761713\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=-4.761713027954102\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:32 INFO 140511108945280] Epoch[221] Batch[5] avg_epoch_loss=-4.572530\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=-4.572529673576355\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:32 INFO 140511108945280] Epoch[221] Batch [5]#011Speed: 171.28 samples/sec#011loss=-4.572530\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:33 INFO 140511108945280] Epoch[221] Batch[10] avg_epoch_loss=-4.543529\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=221, batch=10 train loss <loss>=-4.508728122711181\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:33 INFO 140511108945280] Epoch[221] Batch [10]#011Speed: 161.95 samples/sec#011loss=-4.508728\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:34 INFO 140511108945280] processed a total of 769 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847569.5624902, \"EndTime\": 1640847574.7542443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5191.203355789185, \"count\": 1, \"min\": 5191.203355789185, \"max\": 5191.203355789185}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:34 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.13107374897314 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:34 INFO 140511108945280] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=221, train loss <loss>=-4.761478662490845\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:34 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:35 INFO 140511108945280] Epoch[222] Batch[0] avg_epoch_loss=-5.001715\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=-5.001714706420898\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:37 INFO 140511108945280] Epoch[222] Batch[5] avg_epoch_loss=-4.835997\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=-4.835997263590495\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:37 INFO 140511108945280] Epoch[222] Batch [5]#011Speed: 168.47 samples/sec#011loss=-4.835997\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] Epoch[222] Batch[10] avg_epoch_loss=-4.815466\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=222, batch=10 train loss <loss>=-4.7908275604248045\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] Epoch[222] Batch [10]#011Speed: 158.95 samples/sec#011loss=-4.790828\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] processed a total of 715 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847574.7543492, \"EndTime\": 1640847579.661005, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4906.166076660156, \"count\": 1, \"min\": 4906.166076660156, \"max\": 4906.166076660156}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=145.7308552688933 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] #progress_metric: host=algo-1, completed 55.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=222, train loss <loss>=-4.4285835685829325\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:39 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:40 INFO 140511108945280] Epoch[223] Batch[0] avg_epoch_loss=-4.491643\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=-4.491642951965332\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:42 INFO 140511108945280] Epoch[223] Batch[5] avg_epoch_loss=-4.487683\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=-4.487683375676473\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:42 INFO 140511108945280] Epoch[223] Batch [5]#011Speed: 169.96 samples/sec#011loss=-4.487683\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] Epoch[223] Batch[10] avg_epoch_loss=-4.698039\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=-4.950464820861816\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] Epoch[223] Batch [10]#011Speed: 162.97 samples/sec#011loss=-4.950465\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] processed a total of 723 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847579.6611037, \"EndTime\": 1640847584.5125027, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4850.899696350098, \"count\": 1, \"min\": 4850.899696350098, \"max\": 4850.899696350098}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.04163036647702 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=223, train loss <loss>=-4.681373318036397\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:44 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:45 INFO 140511108945280] Epoch[224] Batch[0] avg_epoch_loss=-4.908873\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=-4.908872604370117\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:47 INFO 140511108945280] Epoch[224] Batch[5] avg_epoch_loss=-4.431792\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=-4.431792418162028\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:47 INFO 140511108945280] Epoch[224] Batch [5]#011Speed: 168.84 samples/sec#011loss=-4.431792\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:48 INFO 140511108945280] Epoch[224] Batch[10] avg_epoch_loss=-4.615568\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=224, batch=10 train loss <loss>=-4.836098957061767\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:48 INFO 140511108945280] Epoch[224] Batch [10]#011Speed: 164.32 samples/sec#011loss=-4.836099\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:49 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847584.5125642, \"EndTime\": 1640847589.3368237, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4823.621034622192, \"count\": 1, \"min\": 4823.621034622192, \"max\": 4823.621034622192}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:49 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.89505477551324 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:49 INFO 140511108945280] #progress_metric: host=algo-1, completed 56.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=224, train loss <loss>=-4.55116202433904\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:49 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:49 INFO 140511108945280] Epoch[225] Batch[0] avg_epoch_loss=-4.628382\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=-4.628382205963135\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:51 INFO 140511108945280] Epoch[225] Batch[5] avg_epoch_loss=-4.764981\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=-4.764981110890706\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:51 INFO 140511108945280] Epoch[225] Batch [5]#011Speed: 169.41 samples/sec#011loss=-4.764981\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:53 INFO 140511108945280] Epoch[225] Batch[10] avg_epoch_loss=-4.826248\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=225, batch=10 train loss <loss>=-4.89976806640625\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:53 INFO 140511108945280] Epoch[225] Batch [10]#011Speed: 162.47 samples/sec#011loss=-4.899768\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:54 INFO 140511108945280] processed a total of 792 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847589.3369215, \"EndTime\": 1640847594.5717201, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5234.237432479858, \"count\": 1, \"min\": 5234.237432479858, \"max\": 5234.237432479858}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:54 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.30735930893675 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:54 INFO 140511108945280] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=225, train loss <loss>=-4.854814492739164\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:54 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:55 INFO 140511108945280] Epoch[226] Batch[0] avg_epoch_loss=-4.830724\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=-4.830724239349365\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:57 INFO 140511108945280] Epoch[226] Batch[5] avg_epoch_loss=-4.995008\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=-4.995007514953613\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:57 INFO 140511108945280] Epoch[226] Batch [5]#011Speed: 168.89 samples/sec#011loss=-4.995008\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] Epoch[226] Batch[10] avg_epoch_loss=-4.935493\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=226, batch=10 train loss <loss>=-4.864075374603272\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] Epoch[226] Batch [10]#011Speed: 162.81 samples/sec#011loss=-4.864075\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] processed a total of 787 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847594.571821, \"EndTime\": 1640847599.78147, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5209.053993225098, \"count\": 1, \"min\": 5209.053993225098, \"max\": 5209.053993225098}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.07914672423044 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] #progress_metric: host=algo-1, completed 56.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=226, train loss <loss>=-4.653371086487403\u001b[0m\n",
      "\u001b[34m[12/30/2021 06:59:59 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:00 INFO 140511108945280] Epoch[227] Batch[0] avg_epoch_loss=-4.115844\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=-4.115843772888184\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:02 INFO 140511108945280] Epoch[227] Batch[5] avg_epoch_loss=-4.723288\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=-4.72328782081604\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:02 INFO 140511108945280] Epoch[227] Batch [5]#011Speed: 158.06 samples/sec#011loss=-4.723288\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:04 INFO 140511108945280] Epoch[227] Batch[10] avg_epoch_loss=-4.727293\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=227, batch=10 train loss <loss>=-4.732098197937011\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:04 INFO 140511108945280] Epoch[227] Batch [10]#011Speed: 148.06 samples/sec#011loss=-4.732098\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:05 INFO 140511108945280] processed a total of 722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847599.7815657, \"EndTime\": 1640847605.0599034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5277.785301208496, \"count\": 1, \"min\": 5277.785301208496, \"max\": 5277.785301208496}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:05 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=136.7963073201392 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:05 INFO 140511108945280] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=227, train loss <loss>=-4.92290997505188\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:05 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:05 INFO 140511108945280] Epoch[228] Batch[0] avg_epoch_loss=-4.787712\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=-4.7877116203308105\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:07 INFO 140511108945280] Epoch[228] Batch[5] avg_epoch_loss=-4.584899\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=-4.584898948669434\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:07 INFO 140511108945280] Epoch[228] Batch [5]#011Speed: 171.99 samples/sec#011loss=-4.584899\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:09 INFO 140511108945280] Epoch[228] Batch[10] avg_epoch_loss=-4.651731\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=228, batch=10 train loss <loss>=-4.731928825378418\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:09 INFO 140511108945280] Epoch[228] Batch [10]#011Speed: 155.44 samples/sec#011loss=-4.731929\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:10 INFO 140511108945280] processed a total of 794 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847605.0599961, \"EndTime\": 1640847610.3858116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5325.071573257446, \"count\": 1, \"min\": 5325.071573257446, \"max\": 5325.071573257446}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:10 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.10209849320927 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:10 INFO 140511108945280] #progress_metric: host=algo-1, completed 57.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=228, train loss <loss>=-4.805338969597449\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:10 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:11 INFO 140511108945280] Epoch[229] Batch[0] avg_epoch_loss=-4.409552\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=-4.409552097320557\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:12 INFO 140511108945280] Epoch[229] Batch[5] avg_epoch_loss=-4.587191\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=-4.587190548578898\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:12 INFO 140511108945280] Epoch[229] Batch [5]#011Speed: 171.53 samples/sec#011loss=-4.587191\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:14 INFO 140511108945280] Epoch[229] Batch[10] avg_epoch_loss=-4.674424\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=-4.779103755950928\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:14 INFO 140511108945280] Epoch[229] Batch [10]#011Speed: 161.14 samples/sec#011loss=-4.779104\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:15 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847610.3859096, \"EndTime\": 1640847615.2671359, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4880.649089813232, \"count\": 1, \"min\": 4880.649089813232, \"max\": 4880.649089813232}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:15 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.32725901952583 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:15 INFO 140511108945280] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=229, train loss <loss>=-4.690995971361796\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:15 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:15 INFO 140511108945280] Epoch[230] Batch[0] avg_epoch_loss=-5.377473\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=-5.3774733543396\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:17 INFO 140511108945280] Epoch[230] Batch[5] avg_epoch_loss=-4.783724\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=-4.783724387486775\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:17 INFO 140511108945280] Epoch[230] Batch [5]#011Speed: 172.27 samples/sec#011loss=-4.783724\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:19 INFO 140511108945280] Epoch[230] Batch[10] avg_epoch_loss=-4.810233\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=-4.842043018341064\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:19 INFO 140511108945280] Epoch[230] Batch [10]#011Speed: 164.32 samples/sec#011loss=-4.842043\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:20 INFO 140511108945280] processed a total of 767 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847615.2672343, \"EndTime\": 1640847620.0968368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4829.104423522949, \"count\": 1, \"min\": 4829.104423522949, \"max\": 4829.104423522949}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:20 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.8252813431522 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:20 INFO 140511108945280] #progress_metric: host=algo-1, completed 57.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=230, train loss <loss>=-4.84484867254893\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:20 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:20 INFO 140511108945280] Epoch[231] Batch[0] avg_epoch_loss=-4.647018\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=-4.6470184326171875\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:22 INFO 140511108945280] Epoch[231] Batch[5] avg_epoch_loss=-4.701694\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=-4.701694488525391\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:22 INFO 140511108945280] Epoch[231] Batch [5]#011Speed: 170.17 samples/sec#011loss=-4.701694\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:24 INFO 140511108945280] Epoch[231] Batch[10] avg_epoch_loss=-4.694581\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=-4.686044597625733\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:24 INFO 140511108945280] Epoch[231] Batch [10]#011Speed: 165.54 samples/sec#011loss=-4.686045\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:25 INFO 140511108945280] processed a total of 770 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847620.0969079, \"EndTime\": 1640847625.2412686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5143.789291381836, \"count\": 1, \"min\": 5143.789291381836, \"max\": 5143.789291381836}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:25 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.69098235344939 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:25 INFO 140511108945280] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=231, train loss <loss>=-4.866774962498591\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:25 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:25 INFO 140511108945280] Epoch[232] Batch[0] avg_epoch_loss=-5.327946\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=-5.327945709228516\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:27 INFO 140511108945280] Epoch[232] Batch[5] avg_epoch_loss=-4.775584\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:27 INFO 140511108945280] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=-4.775583744049072\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:27 INFO 140511108945280] Epoch[232] Batch [5]#011Speed: 172.36 samples/sec#011loss=-4.775584\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] Epoch[232] Batch[10] avg_epoch_loss=-4.767111\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=232, batch=10 train loss <loss>=-4.756943702697754\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] Epoch[232] Batch [10]#011Speed: 168.74 samples/sec#011loss=-4.756944\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847625.2413683, \"EndTime\": 1640847629.5813756, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4339.409828186035, \"count\": 1, \"min\": 4339.409828186035, \"max\": 4339.409828186035}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=160.3859183627386 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] #progress_metric: host=algo-1, completed 58.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=232, train loss <loss>=-4.767110997980291\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:29 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:30 INFO 140511108945280] Epoch[233] Batch[0] avg_epoch_loss=-4.884573\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=-4.884573459625244\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:32 INFO 140511108945280] Epoch[233] Batch[5] avg_epoch_loss=-4.673490\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=-4.673489650090535\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:32 INFO 140511108945280] Epoch[233] Batch [5]#011Speed: 170.81 samples/sec#011loss=-4.673490\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] Epoch[233] Batch[10] avg_epoch_loss=-4.574703\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=233, batch=10 train loss <loss>=-4.456158638000488\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] Epoch[233] Batch [10]#011Speed: 164.45 samples/sec#011loss=-4.456159\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] processed a total of 773 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847629.5814598, \"EndTime\": 1640847634.7704978, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5188.487768173218, \"count\": 1, \"min\": 5188.487768173218, \"max\": 5188.487768173218}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.9795106343403 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] #progress_metric: host=algo-1, completed 58.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=233, train loss <loss>=-4.241279776852864\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:34 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:35 INFO 140511108945280] Epoch[234] Batch[0] avg_epoch_loss=-4.555707\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=-4.555706977844238\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:37 INFO 140511108945280] Epoch[234] Batch[5] avg_epoch_loss=-4.799734\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=-4.799734195073445\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:37 INFO 140511108945280] Epoch[234] Batch [5]#011Speed: 169.56 samples/sec#011loss=-4.799734\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] Epoch[234] Batch[10] avg_epoch_loss=-4.860695\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=234, batch=10 train loss <loss>=-4.933847332000733\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] Epoch[234] Batch [10]#011Speed: 160.69 samples/sec#011loss=-4.933847\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] processed a total of 724 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847634.770601, \"EndTime\": 1640847639.6127293, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4841.4905071258545, \"count\": 1, \"min\": 4841.4905071258545, \"max\": 4841.4905071258545}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.53650537522154 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] #progress_metric: host=algo-1, completed 58.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=234, train loss <loss>=-5.04129962126414\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:39 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_e5e42a99-f91b-477f-990e-22182ac6c878-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847639.6128256, \"EndTime\": 1640847639.75127, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 137.89701461791992, \"count\": 1, \"min\": 137.89701461791992, \"max\": 137.89701461791992}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:40 INFO 140511108945280] Epoch[235] Batch[0] avg_epoch_loss=-4.225965\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=-4.22596549987793\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:42 INFO 140511108945280] Epoch[235] Batch[5] avg_epoch_loss=-4.734769\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=-4.734769423802693\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:42 INFO 140511108945280] Epoch[235] Batch [5]#011Speed: 170.80 samples/sec#011loss=-4.734769\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] Epoch[235] Batch[10] avg_epoch_loss=-4.812681\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=235, batch=10 train loss <loss>=-4.906174564361573\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] Epoch[235] Batch [10]#011Speed: 166.50 samples/sec#011loss=-4.906175\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] processed a total of 706 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847639.7513525, \"EndTime\": 1640847644.5348103, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4783.38885307312, \"count\": 1, \"min\": 4783.38885307312, \"max\": 4783.38885307312}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.59007443105116 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=235, train loss <loss>=-4.376558428009351\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:44 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:45 INFO 140511108945280] Epoch[236] Batch[0] avg_epoch_loss=-4.782614\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=-4.782614231109619\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:46 INFO 140511108945280] Epoch[236] Batch[5] avg_epoch_loss=-4.909966\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=-4.909965991973877\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:46 INFO 140511108945280] Epoch[236] Batch [5]#011Speed: 171.15 samples/sec#011loss=-4.909966\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:48 INFO 140511108945280] Epoch[236] Batch[10] avg_epoch_loss=-4.739610\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=236, batch=10 train loss <loss>=-4.535182857513428\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:48 INFO 140511108945280] Epoch[236] Batch [10]#011Speed: 164.95 samples/sec#011loss=-4.535183\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:49 INFO 140511108945280] processed a total of 741 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847644.534904, \"EndTime\": 1640847649.3107126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4775.1734256744385, \"count\": 1, \"min\": 4775.1734256744385, \"max\": 4775.1734256744385}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:49 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.17180719145244 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:49 INFO 140511108945280] #progress_metric: host=algo-1, completed 59.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=236, train loss <loss>=-4.558650294939677\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:49 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:49 INFO 140511108945280] Epoch[237] Batch[0] avg_epoch_loss=-4.515110\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=-4.515110492706299\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:51 INFO 140511108945280] Epoch[237] Batch[5] avg_epoch_loss=-4.796570\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=-4.796570221583049\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:51 INFO 140511108945280] Epoch[237] Batch [5]#011Speed: 168.30 samples/sec#011loss=-4.796570\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:53 INFO 140511108945280] Epoch[237] Batch[10] avg_epoch_loss=-4.717195\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=237, batch=10 train loss <loss>=-4.6219446659088135\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:53 INFO 140511108945280] Epoch[237] Batch [10]#011Speed: 166.47 samples/sec#011loss=-4.621945\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:54 INFO 140511108945280] processed a total of 786 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847649.3108501, \"EndTime\": 1640847654.5004685, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5189.0904903411865, \"count\": 1, \"min\": 5189.0904903411865, \"max\": 5189.0904903411865}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:54 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.46806780660904 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:54 INFO 140511108945280] #progress_metric: host=algo-1, completed 59.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=237, train loss <loss>=-4.863786715727586\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:54 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:55 INFO 140511108945280] Epoch[238] Batch[0] avg_epoch_loss=-4.749543\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=-4.749543190002441\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:56 INFO 140511108945280] Epoch[238] Batch[5] avg_epoch_loss=-4.792173\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=-4.7921728293101\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:56 INFO 140511108945280] Epoch[238] Batch [5]#011Speed: 170.28 samples/sec#011loss=-4.792173\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:58 INFO 140511108945280] Epoch[238] Batch[10] avg_epoch_loss=-4.767977\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=238, batch=10 train loss <loss>=-4.738941097259522\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:58 INFO 140511108945280] Epoch[238] Batch [10]#011Speed: 166.06 samples/sec#011loss=-4.738941\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:59 INFO 140511108945280] processed a total of 742 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847654.500551, \"EndTime\": 1640847659.2877426, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4786.569118499756, \"count\": 1, \"min\": 4786.569118499756, \"max\": 4786.569118499756}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:59 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.01253892841603 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:59 INFO 140511108945280] #progress_metric: host=algo-1, completed 59.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=238, train loss <loss>=-4.634206374486287\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:59 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:59 INFO 140511108945280] Epoch[239] Batch[0] avg_epoch_loss=-4.909734\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:00:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=239, batch=0 train loss <loss>=-4.90973424911499\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:01 INFO 140511108945280] Epoch[239] Batch[5] avg_epoch_loss=-4.749623\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:01 INFO 140511108945280] #quality_metric: host=algo-1, epoch=239, batch=5 train loss <loss>=-4.74962321917216\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:01 INFO 140511108945280] Epoch[239] Batch [5]#011Speed: 159.39 samples/sec#011loss=-4.749623\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:03 INFO 140511108945280] Epoch[239] Batch[10] avg_epoch_loss=-4.712740\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=239, batch=10 train loss <loss>=-4.66848087310791\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:03 INFO 140511108945280] Epoch[239] Batch [10]#011Speed: 157.13 samples/sec#011loss=-4.668481\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:04 INFO 140511108945280] processed a total of 722 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847659.2878432, \"EndTime\": 1640847664.412678, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5124.224901199341, \"count\": 1, \"min\": 5124.224901199341, \"max\": 5124.224901199341}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:04 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=140.89559534160006 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:04 INFO 140511108945280] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=239, train loss <loss>=-4.397537887096405\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:04 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:05 INFO 140511108945280] Epoch[240] Batch[0] avg_epoch_loss=-5.151717\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=240, batch=0 train loss <loss>=-5.151717185974121\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:07 INFO 140511108945280] Epoch[240] Batch[5] avg_epoch_loss=-4.689814\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=240, batch=5 train loss <loss>=-4.689813693364461\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:07 INFO 140511108945280] Epoch[240] Batch [5]#011Speed: 160.20 samples/sec#011loss=-4.689814\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] Epoch[240] Batch[10] avg_epoch_loss=-4.691468\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=240, batch=10 train loss <loss>=-4.693452501296997\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] Epoch[240] Batch [10]#011Speed: 160.56 samples/sec#011loss=-4.693453\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] processed a total of 731 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847664.412774, \"EndTime\": 1640847669.5471287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5133.775949478149, \"count\": 1, \"min\": 5133.775949478149, \"max\": 5133.775949478149}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=142.38657060655777 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] #progress_metric: host=algo-1, completed 60.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=240, train loss <loss>=-4.8206103046735125\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:09 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:10 INFO 140511108945280] Epoch[241] Batch[0] avg_epoch_loss=-4.559179\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=241, batch=0 train loss <loss>=-4.559179306030273\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:12 INFO 140511108945280] Epoch[241] Batch[5] avg_epoch_loss=-4.958243\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=241, batch=5 train loss <loss>=-4.9582429726918535\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:12 INFO 140511108945280] Epoch[241] Batch [5]#011Speed: 170.57 samples/sec#011loss=-4.958243\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:13 INFO 140511108945280] Epoch[241] Batch[10] avg_epoch_loss=-4.946304\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=241, batch=10 train loss <loss>=-4.931977844238281\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:13 INFO 140511108945280] Epoch[241] Batch [10]#011Speed: 167.51 samples/sec#011loss=-4.931978\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:14 INFO 140511108945280] processed a total of 745 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847669.547226, \"EndTime\": 1640847674.329456, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4781.68249130249, \"count\": 1, \"min\": 4781.68249130249, \"max\": 4781.68249130249}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:14 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.79841016397353 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:14 INFO 140511108945280] #progress_metric: host=algo-1, completed 60.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=241, train loss <loss>=-4.926108082135518\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:14 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:14 INFO 140511108945280] Epoch[242] Batch[0] avg_epoch_loss=-4.984839\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=242, batch=0 train loss <loss>=-4.984838962554932\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:16 INFO 140511108945280] Epoch[242] Batch[5] avg_epoch_loss=-4.884145\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=242, batch=5 train loss <loss>=-4.884145100911458\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:16 INFO 140511108945280] Epoch[242] Batch [5]#011Speed: 170.56 samples/sec#011loss=-4.884145\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] Epoch[242] Batch[10] avg_epoch_loss=-4.932081\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=242, batch=10 train loss <loss>=-4.989604854583741\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] Epoch[242] Batch [10]#011Speed: 167.77 samples/sec#011loss=-4.989605\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847674.3295534, \"EndTime\": 1640847678.7689445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4438.873291015625, \"count\": 1, \"min\": 4438.873291015625, \"max\": 4438.873291015625}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.440510150547 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 60.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=242, train loss <loss>=-4.932081352580678\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:19 INFO 140511108945280] Epoch[243] Batch[0] avg_epoch_loss=-4.877208\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=243, batch=0 train loss <loss>=-4.8772077560424805\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:21 INFO 140511108945280] Epoch[243] Batch[5] avg_epoch_loss=-4.583583\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=243, batch=5 train loss <loss>=-4.583583434422811\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:21 INFO 140511108945280] Epoch[243] Batch [5]#011Speed: 170.56 samples/sec#011loss=-4.583583\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] Epoch[243] Batch[10] avg_epoch_loss=-4.781067\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=243, batch=10 train loss <loss>=-5.018047618865967\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] Epoch[243] Batch [10]#011Speed: 162.91 samples/sec#011loss=-5.018048\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] processed a total of 765 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847678.7690318, \"EndTime\": 1640847683.5896504, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4820.139646530151, \"count\": 1, \"min\": 4820.139646530151, \"max\": 4820.139646530151}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=158.7046281228824 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=243, train loss <loss>=-4.812995036443074\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:23 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:24 INFO 140511108945280] Epoch[244] Batch[0] avg_epoch_loss=-4.662165\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=244, batch=0 train loss <loss>=-4.662164688110352\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:26 INFO 140511108945280] Epoch[244] Batch[5] avg_epoch_loss=-4.543381\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=244, batch=5 train loss <loss>=-4.543380578358968\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:26 INFO 140511108945280] Epoch[244] Batch [5]#011Speed: 170.60 samples/sec#011loss=-4.543381\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] Epoch[244] Batch[10] avg_epoch_loss=-4.635757\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=244, batch=10 train loss <loss>=-4.746608734130859\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] Epoch[244] Batch [10]#011Speed: 166.37 samples/sec#011loss=-4.746609\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] processed a total of 768 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847683.589748, \"EndTime\": 1640847688.4127235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4822.453498840332, \"count\": 1, \"min\": 4822.453498840332, \"max\": 4822.453498840332}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=159.2502636764451 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] #progress_metric: host=algo-1, completed 61.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=244, train loss <loss>=-4.616806268692017\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:28 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:29 INFO 140511108945280] Epoch[245] Batch[0] avg_epoch_loss=-4.568667\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=245, batch=0 train loss <loss>=-4.568666934967041\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:30 INFO 140511108945280] Epoch[245] Batch[5] avg_epoch_loss=-4.730350\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:30 INFO 140511108945280] #quality_metric: host=algo-1, epoch=245, batch=5 train loss <loss>=-4.730350097020467\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:30 INFO 140511108945280] Epoch[245] Batch [5]#011Speed: 169.90 samples/sec#011loss=-4.730350\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:32 INFO 140511108945280] Epoch[245] Batch[10] avg_epoch_loss=-4.850969\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=245, batch=10 train loss <loss>=-4.995711612701416\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:32 INFO 140511108945280] Epoch[245] Batch [10]#011Speed: 165.29 samples/sec#011loss=-4.995712\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:33 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847688.4128244, \"EndTime\": 1640847693.223686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4810.158252716064, \"count\": 1, \"min\": 4810.158252716064, \"max\": 4810.158252716064}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:33 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.33141152925822 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:33 INFO 140511108945280] #progress_metric: host=algo-1, completed 61.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=245, train loss <loss>=-4.90072758992513\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:33 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:33 INFO 140511108945280] Epoch[246] Batch[0] avg_epoch_loss=-4.915885\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=246, batch=0 train loss <loss>=-4.9158854484558105\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:35 INFO 140511108945280] Epoch[246] Batch[5] avg_epoch_loss=-4.845056\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=246, batch=5 train loss <loss>=-4.845056056976318\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:35 INFO 140511108945280] Epoch[246] Batch [5]#011Speed: 166.44 samples/sec#011loss=-4.845056\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:37 INFO 140511108945280] Epoch[246] Batch[10] avg_epoch_loss=-4.844990\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=246, batch=10 train loss <loss>=-4.844911575317383\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:37 INFO 140511108945280] Epoch[246] Batch [10]#011Speed: 167.80 samples/sec#011loss=-4.844912\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:38 INFO 140511108945280] processed a total of 753 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847693.2237825, \"EndTime\": 1640847698.048836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4824.472665786743, \"count\": 1, \"min\": 4824.472665786743, \"max\": 4824.472665786743}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:38 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.0745911710308 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:38 INFO 140511108945280] #progress_metric: host=algo-1, completed 61.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=246, train loss <loss>=-4.775466640790303\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:38 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:38 INFO 140511108945280] Epoch[247] Batch[0] avg_epoch_loss=-4.781696\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=247, batch=0 train loss <loss>=-4.781696319580078\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:40 INFO 140511108945280] Epoch[247] Batch[5] avg_epoch_loss=-4.934433\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=247, batch=5 train loss <loss>=-4.934433221817017\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:40 INFO 140511108945280] Epoch[247] Batch [5]#011Speed: 163.85 samples/sec#011loss=-4.934433\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] Epoch[247] Batch[10] avg_epoch_loss=-4.883785\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=247, batch=10 train loss <loss>=-4.8230077743530275\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] Epoch[247] Batch [10]#011Speed: 167.50 samples/sec#011loss=-4.823008\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] processed a total of 708 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847698.0489368, \"EndTime\": 1640847702.877033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4827.576637268066, \"count\": 1, \"min\": 4827.576637268066, \"max\": 4827.576637268066}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=146.65333418276262 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=247, train loss <loss>=-4.800329466660817\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:42 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:43 INFO 140511108945280] Epoch[248] Batch[0] avg_epoch_loss=-4.670485\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=248, batch=0 train loss <loss>=-4.67048454284668\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:45 INFO 140511108945280] Epoch[248] Batch[5] avg_epoch_loss=-5.099777\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=248, batch=5 train loss <loss>=-5.0997772216796875\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:45 INFO 140511108945280] Epoch[248] Batch [5]#011Speed: 166.83 samples/sec#011loss=-5.099777\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] Epoch[248] Batch[10] avg_epoch_loss=-5.033222\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=248, batch=10 train loss <loss>=-4.953354930877685\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] Epoch[248] Batch [10]#011Speed: 166.81 samples/sec#011loss=-4.953355\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] processed a total of 732 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847702.8771293, \"EndTime\": 1640847707.7159674, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4838.245391845703, \"count\": 1, \"min\": 4838.245391845703, \"max\": 4838.245391845703}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.2891716771077 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] #progress_metric: host=algo-1, completed 62.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=248, train loss <loss>=-5.1384009917577105\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:47 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/state_e4cc0aa4-0787-442c-97d7-d82844739a69-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847707.7160993, \"EndTime\": 1640847707.8229096, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 106.22859001159668, \"count\": 1, \"min\": 106.22859001159668, \"max\": 106.22859001159668}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:48 INFO 140511108945280] Epoch[249] Batch[0] avg_epoch_loss=-5.089055\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=249, batch=0 train loss <loss>=-5.089054584503174\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:50 INFO 140511108945280] Epoch[249] Batch[5] avg_epoch_loss=-5.111082\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=249, batch=5 train loss <loss>=-5.111081679662068\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:50 INFO 140511108945280] Epoch[249] Batch [5]#011Speed: 168.68 samples/sec#011loss=-5.111082\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] Epoch[249] Batch[10] avg_epoch_loss=-4.936371\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=249, batch=10 train loss <loss>=-4.726717758178711\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] Epoch[249] Batch [10]#011Speed: 162.84 samples/sec#011loss=-4.726718\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847707.8229775, \"EndTime\": 1640847712.6708722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4847.816467285156, \"count\": 1, \"min\": 4847.816467285156, \"max\": 4847.816467285156}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.0854980982658 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] #progress_metric: host=algo-1, completed 62.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=249, train loss <loss>=-4.823095679283142\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:52 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:53 INFO 140511108945280] Epoch[250] Batch[0] avg_epoch_loss=-4.577586\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=250, batch=0 train loss <loss>=-4.577585697174072\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:55 INFO 140511108945280] Epoch[250] Batch[5] avg_epoch_loss=-4.725257\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=250, batch=5 train loss <loss>=-4.725257237752278\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:55 INFO 140511108945280] Epoch[250] Batch [5]#011Speed: 169.97 samples/sec#011loss=-4.725257\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] Epoch[250] Batch[10] avg_epoch_loss=-4.698594\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=250, batch=10 train loss <loss>=-4.666598796844482\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] Epoch[250] Batch [10]#011Speed: 168.09 samples/sec#011loss=-4.666599\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] processed a total of 715 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847712.6709712, \"EndTime\": 1640847717.4726882, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4801.192998886108, \"count\": 1, \"min\": 4801.192998886108, \"max\": 4801.192998886108}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.9172973542282 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] #progress_metric: host=algo-1, completed 62.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=250, train loss <loss>=-4.848367253939311\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:57 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:58 INFO 140511108945280] Epoch[251] Batch[0] avg_epoch_loss=-4.972425\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=251, batch=0 train loss <loss>=-4.9724249839782715\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:59 INFO 140511108945280] Epoch[251] Batch[5] avg_epoch_loss=-4.774442\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=251, batch=5 train loss <loss>=-4.774442275365193\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:01:59 INFO 140511108945280] Epoch[251] Batch [5]#011Speed: 166.93 samples/sec#011loss=-4.774442\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] Epoch[251] Batch[10] avg_epoch_loss=-4.740079\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=251, batch=10 train loss <loss>=-4.698843193054199\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] Epoch[251] Batch [10]#011Speed: 158.20 samples/sec#011loss=-4.698843\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] processed a total of 736 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847717.4727736, \"EndTime\": 1640847722.38663, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4913.284778594971, \"count\": 1, \"min\": 4913.284778594971, \"max\": 4913.284778594971}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.79356945146625 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=251, train loss <loss>=-4.725217223167419\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] Epoch[252] Batch[0] avg_epoch_loss=-4.955154\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=252, batch=0 train loss <loss>=-4.955153942108154\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:05 INFO 140511108945280] Epoch[252] Batch[5] avg_epoch_loss=-5.013263\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:05 INFO 140511108945280] #quality_metric: host=algo-1, epoch=252, batch=5 train loss <loss>=-5.013262669245402\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:05 INFO 140511108945280] Epoch[252] Batch [5]#011Speed: 138.93 samples/sec#011loss=-5.013263\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:07 INFO 140511108945280] Epoch[252] Batch[10] avg_epoch_loss=-4.859920\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:07 INFO 140511108945280] #quality_metric: host=algo-1, epoch=252, batch=10 train loss <loss>=-4.675908088684082\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:07 INFO 140511108945280] Epoch[252] Batch [10]#011Speed: 159.27 samples/sec#011loss=-4.675908\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:08 INFO 140511108945280] processed a total of 793 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847722.3867316, \"EndTime\": 1640847728.06306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5675.729036331177, \"count\": 1, \"min\": 5675.729036331177, \"max\": 5675.729036331177}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=139.71447091144697 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 63.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=252, train loss <loss>=-4.833062905531663\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:08 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:08 INFO 140511108945280] Epoch[253] Batch[0] avg_epoch_loss=-4.083679\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=253, batch=0 train loss <loss>=-4.083678722381592\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:10 INFO 140511108945280] Epoch[253] Batch[5] avg_epoch_loss=-4.696481\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:10 INFO 140511108945280] #quality_metric: host=algo-1, epoch=253, batch=5 train loss <loss>=-4.696481227874756\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:10 INFO 140511108945280] Epoch[253] Batch [5]#011Speed: 156.82 samples/sec#011loss=-4.696481\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:12 INFO 140511108945280] Epoch[253] Batch[10] avg_epoch_loss=-4.724028\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:12 INFO 140511108945280] #quality_metric: host=algo-1, epoch=253, batch=10 train loss <loss>=-4.757083797454834\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:12 INFO 140511108945280] Epoch[253] Batch [10]#011Speed: 164.90 samples/sec#011loss=-4.757084\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:13 INFO 140511108945280] processed a total of 713 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847728.063152, \"EndTime\": 1640847733.0782983, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5014.645576477051, \"count\": 1, \"min\": 5014.645576477051, \"max\": 5014.645576477051}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:13 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=142.17946538646427 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:13 INFO 140511108945280] #progress_metric: host=algo-1, completed 63.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=253, train loss <loss>=-4.988827745119731\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:13 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:13 INFO 140511108945280] Epoch[254] Batch[0] avg_epoch_loss=-4.948801\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=254, batch=0 train loss <loss>=-4.948801040649414\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:15 INFO 140511108945280] Epoch[254] Batch[5] avg_epoch_loss=-4.842361\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:15 INFO 140511108945280] #quality_metric: host=algo-1, epoch=254, batch=5 train loss <loss>=-4.842361052831014\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:15 INFO 140511108945280] Epoch[254] Batch [5]#011Speed: 168.11 samples/sec#011loss=-4.842361\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:17 INFO 140511108945280] Epoch[254] Batch[10] avg_epoch_loss=-4.760941\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:17 INFO 140511108945280] #quality_metric: host=algo-1, epoch=254, batch=10 train loss <loss>=-4.6632367134094235\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:17 INFO 140511108945280] Epoch[254] Batch [10]#011Speed: 165.18 samples/sec#011loss=-4.663237\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:18 INFO 140511108945280] processed a total of 824 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847733.0783958, \"EndTime\": 1640847738.2968335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5217.936992645264, \"count\": 1, \"min\": 5217.936992645264, \"max\": 5217.936992645264}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.91246591404993 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 63.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=254, train loss <loss>=-4.7260691569401665\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:18 INFO 140511108945280] Epoch[255] Batch[0] avg_epoch_loss=-5.834309\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=255, batch=0 train loss <loss>=-5.834309101104736\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:20 INFO 140511108945280] Epoch[255] Batch[5] avg_epoch_loss=-4.894579\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:20 INFO 140511108945280] #quality_metric: host=algo-1, epoch=255, batch=5 train loss <loss>=-4.8945794105529785\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:20 INFO 140511108945280] Epoch[255] Batch [5]#011Speed: 167.87 samples/sec#011loss=-4.894579\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:22 INFO 140511108945280] Epoch[255] Batch[10] avg_epoch_loss=-4.793976\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:22 INFO 140511108945280] #quality_metric: host=algo-1, epoch=255, batch=10 train loss <loss>=-4.673251628875732\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:22 INFO 140511108945280] Epoch[255] Batch [10]#011Speed: 162.78 samples/sec#011loss=-4.673252\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:23 INFO 140511108945280] processed a total of 776 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847738.2969346, \"EndTime\": 1640847743.518427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5220.888376235962, \"count\": 1, \"min\": 5220.888376235962, \"max\": 5220.888376235962}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:23 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.62977841660177 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:23 INFO 140511108945280] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=255, train loss <loss>=-4.922939814054049\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:23 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:24 INFO 140511108945280] Epoch[256] Batch[0] avg_epoch_loss=-4.431237\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=256, batch=0 train loss <loss>=-4.43123722076416\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:26 INFO 140511108945280] Epoch[256] Batch[5] avg_epoch_loss=-4.777127\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=256, batch=5 train loss <loss>=-4.777126709620158\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:26 INFO 140511108945280] Epoch[256] Batch [5]#011Speed: 164.46 samples/sec#011loss=-4.777127\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] Epoch[256] Batch[10] avg_epoch_loss=-4.802718\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=256, batch=10 train loss <loss>=-4.833428573608399\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] Epoch[256] Batch [10]#011Speed: 161.82 samples/sec#011loss=-4.833429\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] processed a total of 748 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847743.5185246, \"EndTime\": 1640847748.4733403, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4954.294443130493, \"count\": 1, \"min\": 4954.294443130493, \"max\": 4954.294443130493}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.97605019504456 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] #progress_metric: host=algo-1, completed 64.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=256, train loss <loss>=-4.892054120699565\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:28 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:29 INFO 140511108945280] Epoch[257] Batch[0] avg_epoch_loss=-5.039514\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=257, batch=0 train loss <loss>=-5.039514064788818\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:31 INFO 140511108945280] Epoch[257] Batch[5] avg_epoch_loss=-4.845223\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=257, batch=5 train loss <loss>=-4.845223426818848\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:31 INFO 140511108945280] Epoch[257] Batch [5]#011Speed: 168.04 samples/sec#011loss=-4.845223\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:32 INFO 140511108945280] Epoch[257] Batch[10] avg_epoch_loss=-4.849843\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:32 INFO 140511108945280] #quality_metric: host=algo-1, epoch=257, batch=10 train loss <loss>=-4.855385494232178\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:32 INFO 140511108945280] Epoch[257] Batch [10]#011Speed: 165.26 samples/sec#011loss=-4.855385\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:33 INFO 140511108945280] processed a total of 763 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847748.4734354, \"EndTime\": 1640847753.3282237, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4854.263067245483, \"count\": 1, \"min\": 4854.263067245483, \"max\": 4854.263067245483}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:33 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.17714334547594 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:33 INFO 140511108945280] #progress_metric: host=algo-1, completed 64.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=257, train loss <loss>=-4.853994806607564\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:33 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:33 INFO 140511108945280] Epoch[258] Batch[0] avg_epoch_loss=-4.935699\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=258, batch=0 train loss <loss>=-4.935698509216309\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:35 INFO 140511108945280] Epoch[258] Batch[5] avg_epoch_loss=-5.181772\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:35 INFO 140511108945280] #quality_metric: host=algo-1, epoch=258, batch=5 train loss <loss>=-5.1817716757456465\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:35 INFO 140511108945280] Epoch[258] Batch [5]#011Speed: 166.39 samples/sec#011loss=-5.181772\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:37 INFO 140511108945280] Epoch[258] Batch[10] avg_epoch_loss=-4.950377\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=258, batch=10 train loss <loss>=-4.672703552246094\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:37 INFO 140511108945280] Epoch[258] Batch [10]#011Speed: 164.56 samples/sec#011loss=-4.672704\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:38 INFO 140511108945280] processed a total of 730 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847753.3283172, \"EndTime\": 1640847758.1936865, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4864.875078201294, \"count\": 1, \"min\": 4864.875078201294, \"max\": 4864.875078201294}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:38 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.0511003792735 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:38 INFO 140511108945280] #progress_metric: host=algo-1, completed 64.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=258, train loss <loss>=-4.773241360982259\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:38 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:38 INFO 140511108945280] Epoch[259] Batch[0] avg_epoch_loss=-4.679749\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=259, batch=0 train loss <loss>=-4.67974853515625\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:40 INFO 140511108945280] Epoch[259] Batch[5] avg_epoch_loss=-4.557282\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:40 INFO 140511108945280] #quality_metric: host=algo-1, epoch=259, batch=5 train loss <loss>=-4.557281891504924\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:40 INFO 140511108945280] Epoch[259] Batch [5]#011Speed: 163.22 samples/sec#011loss=-4.557282\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:42 INFO 140511108945280] Epoch[259] Batch[10] avg_epoch_loss=-4.650710\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:42 INFO 140511108945280] #quality_metric: host=algo-1, epoch=259, batch=10 train loss <loss>=-4.762824153900146\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:42 INFO 140511108945280] Epoch[259] Batch [10]#011Speed: 166.29 samples/sec#011loss=-4.762824\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:43 INFO 140511108945280] processed a total of 753 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847758.1937816, \"EndTime\": 1640847763.099223, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4904.92582321167, \"count\": 1, \"min\": 4904.92582321167, \"max\": 4904.92582321167}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:43 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.51492513481662 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:43 INFO 140511108945280] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=259, train loss <loss>=-4.7472660938898725\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:43 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:43 INFO 140511108945280] Epoch[260] Batch[0] avg_epoch_loss=-4.627439\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=260, batch=0 train loss <loss>=-4.627438545227051\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:45 INFO 140511108945280] Epoch[260] Batch[5] avg_epoch_loss=-4.646000\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:45 INFO 140511108945280] #quality_metric: host=algo-1, epoch=260, batch=5 train loss <loss>=-4.646000385284424\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:45 INFO 140511108945280] Epoch[260] Batch [5]#011Speed: 168.79 samples/sec#011loss=-4.646000\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:47 INFO 140511108945280] Epoch[260] Batch[10] avg_epoch_loss=-4.509515\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:47 INFO 140511108945280] #quality_metric: host=algo-1, epoch=260, batch=10 train loss <loss>=-4.345732164382935\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:47 INFO 140511108945280] Epoch[260] Batch [10]#011Speed: 166.19 samples/sec#011loss=-4.345732\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:48 INFO 140511108945280] processed a total of 773 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847763.099318, \"EndTime\": 1640847768.2759695, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5176.166534423828, \"count\": 1, \"min\": 5176.166534423828, \"max\": 5176.166534423828}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:48 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.3344240255922 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:48 INFO 140511108945280] #progress_metric: host=algo-1, completed 65.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=260, train loss <loss>=-4.519568590017466\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:48 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:48 INFO 140511108945280] Epoch[261] Batch[0] avg_epoch_loss=-4.629158\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=261, batch=0 train loss <loss>=-4.6291584968566895\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:50 INFO 140511108945280] Epoch[261] Batch[5] avg_epoch_loss=-4.564155\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:50 INFO 140511108945280] #quality_metric: host=algo-1, epoch=261, batch=5 train loss <loss>=-4.564155022303264\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:50 INFO 140511108945280] Epoch[261] Batch [5]#011Speed: 167.74 samples/sec#011loss=-4.564155\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:52 INFO 140511108945280] Epoch[261] Batch[10] avg_epoch_loss=-4.597386\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:52 INFO 140511108945280] #quality_metric: host=algo-1, epoch=261, batch=10 train loss <loss>=-4.63726315498352\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:52 INFO 140511108945280] Epoch[261] Batch [10]#011Speed: 164.83 samples/sec#011loss=-4.637263\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:53 INFO 140511108945280] processed a total of 761 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847768.2760658, \"EndTime\": 1640847773.1135144, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4836.898565292358, \"count\": 1, \"min\": 4836.898565292358, \"max\": 4836.898565292358}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:53 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=157.3278329360293 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:53 INFO 140511108945280] #progress_metric: host=algo-1, completed 65.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=261, train loss <loss>=-4.646836062272389\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:53 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:53 INFO 140511108945280] Epoch[262] Batch[0] avg_epoch_loss=-4.809034\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=262, batch=0 train loss <loss>=-4.80903434753418\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:55 INFO 140511108945280] Epoch[262] Batch[5] avg_epoch_loss=-5.099126\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:55 INFO 140511108945280] #quality_metric: host=algo-1, epoch=262, batch=5 train loss <loss>=-5.0991264184316\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:55 INFO 140511108945280] Epoch[262] Batch [5]#011Speed: 169.24 samples/sec#011loss=-5.099126\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:57 INFO 140511108945280] Epoch[262] Batch[10] avg_epoch_loss=-4.961359\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=262, batch=10 train loss <loss>=-4.796038627624512\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:57 INFO 140511108945280] Epoch[262] Batch [10]#011Speed: 166.06 samples/sec#011loss=-4.796039\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:58 INFO 140511108945280] processed a total of 778 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847773.1136105, \"EndTime\": 1640847778.3217034, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5207.579612731934, \"count\": 1, \"min\": 5207.579612731934, \"max\": 5207.579612731934}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:58 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.39365980045278 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:58 INFO 140511108945280] #progress_metric: host=algo-1, completed 65.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=262, train loss <loss>=-5.087010346926176\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:58 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:58 INFO 140511108945280] Epoch[263] Batch[0] avg_epoch_loss=-4.280020\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:02:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=263, batch=0 train loss <loss>=-4.280019760131836\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:00 INFO 140511108945280] Epoch[263] Batch[5] avg_epoch_loss=-4.680043\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=263, batch=5 train loss <loss>=-4.680042823155721\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:00 INFO 140511108945280] Epoch[263] Batch [5]#011Speed: 166.01 samples/sec#011loss=-4.680043\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:02 INFO 140511108945280] Epoch[263] Batch[10] avg_epoch_loss=-4.640256\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=263, batch=10 train loss <loss>=-4.592512702941894\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:02 INFO 140511108945280] Epoch[263] Batch [10]#011Speed: 154.51 samples/sec#011loss=-4.592513\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:03 INFO 140511108945280] processed a total of 777 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847778.3218002, \"EndTime\": 1640847783.8070655, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5484.739065170288, \"count\": 1, \"min\": 5484.739065170288, \"max\": 5484.739065170288}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:03 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=141.66234879250257 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:03 INFO 140511108945280] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=263, train loss <loss>=-4.799088147970346\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:03 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:04 INFO 140511108945280] Epoch[264] Batch[0] avg_epoch_loss=-4.104977\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=264, batch=0 train loss <loss>=-4.104977130889893\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:06 INFO 140511108945280] Epoch[264] Batch[5] avg_epoch_loss=-4.607054\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=264, batch=5 train loss <loss>=-4.607054392496745\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:06 INFO 140511108945280] Epoch[264] Batch [5]#011Speed: 147.31 samples/sec#011loss=-4.607054\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] Epoch[264] Batch[10] avg_epoch_loss=-4.691454\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=264, batch=10 train loss <loss>=-4.792733192443848\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] Epoch[264] Batch [10]#011Speed: 164.73 samples/sec#011loss=-4.792733\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] processed a total of 742 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847783.807162, \"EndTime\": 1640847788.945876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5138.168811798096, \"count\": 1, \"min\": 5138.168811798096, \"max\": 5138.168811798096}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=144.4054843183555 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 66.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=264, train loss <loss>=-4.657181779543559\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:08 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:09 INFO 140511108945280] Epoch[265] Batch[0] avg_epoch_loss=-4.074161\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=265, batch=0 train loss <loss>=-4.074160575866699\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:11 INFO 140511108945280] Epoch[265] Batch[5] avg_epoch_loss=-4.730612\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=265, batch=5 train loss <loss>=-4.73061219851176\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:11 INFO 140511108945280] Epoch[265] Batch [5]#011Speed: 157.15 samples/sec#011loss=-4.730612\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:13 INFO 140511108945280] Epoch[265] Batch[10] avg_epoch_loss=-4.792675\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=265, batch=10 train loss <loss>=-4.867150402069091\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:13 INFO 140511108945280] Epoch[265] Batch [10]#011Speed: 162.42 samples/sec#011loss=-4.867150\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:14 INFO 140511108945280] processed a total of 805 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847788.9459732, \"EndTime\": 1640847794.3206563, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5374.116659164429, \"count\": 1, \"min\": 5374.116659164429, \"max\": 5374.116659164429}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:14 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.7882712710374 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:14 INFO 140511108945280] #progress_metric: host=algo-1, completed 66.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=265, train loss <loss>=-4.848596646235539\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:14 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:14 INFO 140511108945280] Epoch[266] Batch[0] avg_epoch_loss=-4.298289\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=266, batch=0 train loss <loss>=-4.2982892990112305\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:16 INFO 140511108945280] Epoch[266] Batch[5] avg_epoch_loss=-4.691451\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=266, batch=5 train loss <loss>=-4.69145147005717\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:16 INFO 140511108945280] Epoch[266] Batch [5]#011Speed: 169.67 samples/sec#011loss=-4.691451\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:18 INFO 140511108945280] Epoch[266] Batch[10] avg_epoch_loss=-4.728734\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=266, batch=10 train loss <loss>=-4.773472118377685\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:18 INFO 140511108945280] Epoch[266] Batch [10]#011Speed: 167.54 samples/sec#011loss=-4.773472\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:19 INFO 140511108945280] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847794.3207538, \"EndTime\": 1640847799.151778, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4830.482482910156, \"count\": 1, \"min\": 4830.482482910156, \"max\": 4830.482482910156}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:19 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.39308671908012 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:19 INFO 140511108945280] #progress_metric: host=algo-1, completed 66.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=266, train loss <loss>=-4.722081383069356\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:19 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:19 INFO 140511108945280] Epoch[267] Batch[0] avg_epoch_loss=-4.474996\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=267, batch=0 train loss <loss>=-4.474996089935303\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:21 INFO 140511108945280] Epoch[267] Batch[5] avg_epoch_loss=-4.445667\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=267, batch=5 train loss <loss>=-4.445666790008545\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:21 INFO 140511108945280] Epoch[267] Batch [5]#011Speed: 168.02 samples/sec#011loss=-4.445667\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:23 INFO 140511108945280] Epoch[267] Batch[10] avg_epoch_loss=-4.581650\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=267, batch=10 train loss <loss>=-4.744830513000489\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:23 INFO 140511108945280] Epoch[267] Batch [10]#011Speed: 166.13 samples/sec#011loss=-4.744831\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:24 INFO 140511108945280] processed a total of 772 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847799.1518764, \"EndTime\": 1640847804.3547013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5202.262401580811, \"count\": 1, \"min\": 5202.262401580811, \"max\": 5202.262401580811}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:24 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.3930978477061 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:24 INFO 140511108945280] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=267, train loss <loss>=-4.849388746114878\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:24 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:25 INFO 140511108945280] Epoch[268] Batch[0] avg_epoch_loss=-4.564283\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:25 INFO 140511108945280] #quality_metric: host=algo-1, epoch=268, batch=0 train loss <loss>=-4.5642828941345215\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:26 INFO 140511108945280] Epoch[268] Batch[5] avg_epoch_loss=-4.664043\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=268, batch=5 train loss <loss>=-4.664042790730794\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:26 INFO 140511108945280] Epoch[268] Batch [5]#011Speed: 169.61 samples/sec#011loss=-4.664043\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] Epoch[268] Batch[10] avg_epoch_loss=-4.433628\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=268, batch=10 train loss <loss>=-4.15713062286377\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] Epoch[268] Batch [10]#011Speed: 170.23 samples/sec#011loss=-4.157131\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847804.3547978, \"EndTime\": 1640847808.780171, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4424.8552322387695, \"count\": 1, \"min\": 4424.8552322387695, \"max\": 4424.8552322387695}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.24930805742872 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] #progress_metric: host=algo-1, completed 67.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=268, train loss <loss>=-4.433628168973056\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:28 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:29 INFO 140511108945280] Epoch[269] Batch[0] avg_epoch_loss=-4.424990\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=269, batch=0 train loss <loss>=-4.424990177154541\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:31 INFO 140511108945280] Epoch[269] Batch[5] avg_epoch_loss=-4.856097\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=269, batch=5 train loss <loss>=-4.856096824010213\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:31 INFO 140511108945280] Epoch[269] Batch [5]#011Speed: 167.97 samples/sec#011loss=-4.856097\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] Epoch[269] Batch[10] avg_epoch_loss=-4.815284\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=269, batch=10 train loss <loss>=-4.7663084983825685\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] Epoch[269] Batch [10]#011Speed: 167.73 samples/sec#011loss=-4.766308\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] processed a total of 743 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847808.780254, \"EndTime\": 1640847813.573401, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4792.63973236084, \"count\": 1, \"min\": 4792.63973236084, \"max\": 4792.63973236084}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.02486009737655 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] #progress_metric: host=algo-1, completed 67.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=269, train loss <loss>=-4.874743898709615\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:33 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:34 INFO 140511108945280] Epoch[270] Batch[0] avg_epoch_loss=-5.428597\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=270, batch=0 train loss <loss>=-5.4285969734191895\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:36 INFO 140511108945280] Epoch[270] Batch[5] avg_epoch_loss=-4.753540\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=270, batch=5 train loss <loss>=-4.7535397211710615\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:36 INFO 140511108945280] Epoch[270] Batch [5]#011Speed: 168.05 samples/sec#011loss=-4.753540\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:37 INFO 140511108945280] Epoch[270] Batch[10] avg_epoch_loss=-4.725918\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:37 INFO 140511108945280] #quality_metric: host=algo-1, epoch=270, batch=10 train loss <loss>=-4.692771625518799\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:37 INFO 140511108945280] Epoch[270] Batch [10]#011Speed: 165.59 samples/sec#011loss=-4.692772\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:38 INFO 140511108945280] processed a total of 779 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847813.573499, \"EndTime\": 1640847818.7435875, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5169.5237159729, \"count\": 1, \"min\": 5169.5237159729, \"max\": 5169.5237159729}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:38 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.68697309805225 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:38 INFO 140511108945280] #progress_metric: host=algo-1, completed 67.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=270, train loss <loss>=-4.742770561805139\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:38 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:39 INFO 140511108945280] Epoch[271] Batch[0] avg_epoch_loss=-4.755582\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=271, batch=0 train loss <loss>=-4.755582332611084\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:41 INFO 140511108945280] Epoch[271] Batch[5] avg_epoch_loss=-4.860350\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=271, batch=5 train loss <loss>=-4.860349734624227\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:41 INFO 140511108945280] Epoch[271] Batch [5]#011Speed: 167.54 samples/sec#011loss=-4.860350\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] Epoch[271] Batch[10] avg_epoch_loss=-4.891754\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=271, batch=10 train loss <loss>=-4.929438781738281\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] Epoch[271] Batch [10]#011Speed: 159.84 samples/sec#011loss=-4.929439\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847818.7436826, \"EndTime\": 1640847823.6383274, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4894.092321395874, \"count\": 1, \"min\": 4894.092321395874, \"max\": 4894.092321395874}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=153.65033730214597 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=271, train loss <loss>=-4.814797143141429\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:43 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:44 INFO 140511108945280] Epoch[272] Batch[0] avg_epoch_loss=-5.427550\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=272, batch=0 train loss <loss>=-5.427549839019775\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:46 INFO 140511108945280] Epoch[272] Batch[5] avg_epoch_loss=-4.810787\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=272, batch=5 train loss <loss>=-4.810787359873454\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:46 INFO 140511108945280] Epoch[272] Batch [5]#011Speed: 169.47 samples/sec#011loss=-4.810787\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] Epoch[272] Batch[10] avg_epoch_loss=-4.843026\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=272, batch=10 train loss <loss>=-4.881713390350342\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] Epoch[272] Batch [10]#011Speed: 165.38 samples/sec#011loss=-4.881713\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] processed a total of 785 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847823.6384256, \"EndTime\": 1640847828.8184657, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5179.508686065674, \"count\": 1, \"min\": 5179.508686065674, \"max\": 5179.508686065674}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.55480715008417 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] #progress_metric: host=algo-1, completed 68.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=272, train loss <loss>=-4.51416906943688\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:48 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:49 INFO 140511108945280] Epoch[273] Batch[0] avg_epoch_loss=-4.621212\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=273, batch=0 train loss <loss>=-4.621211528778076\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:51 INFO 140511108945280] Epoch[273] Batch[5] avg_epoch_loss=-4.816990\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=273, batch=5 train loss <loss>=-4.816990454991658\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:51 INFO 140511108945280] Epoch[273] Batch [5]#011Speed: 169.55 samples/sec#011loss=-4.816990\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] Epoch[273] Batch[10] avg_epoch_loss=-4.820921\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=273, batch=10 train loss <loss>=-4.825637912750244\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] Epoch[273] Batch [10]#011Speed: 168.09 samples/sec#011loss=-4.825638\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] processed a total of 707 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847828.818562, \"EndTime\": 1640847833.6619902, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4842.918395996094, \"count\": 1, \"min\": 4842.918395996094, \"max\": 4842.918395996094}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=145.98226486851607 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] #progress_metric: host=algo-1, completed 68.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=273, train loss <loss>=-4.513242443402608\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:53 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:54 INFO 140511108945280] Epoch[274] Batch[0] avg_epoch_loss=-4.314460\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=274, batch=0 train loss <loss>=-4.314459800720215\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:56 INFO 140511108945280] Epoch[274] Batch[5] avg_epoch_loss=-4.707957\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=274, batch=5 train loss <loss>=-4.70795734723409\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:56 INFO 140511108945280] Epoch[274] Batch [5]#011Speed: 169.38 samples/sec#011loss=-4.707957\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] Epoch[274] Batch[10] avg_epoch_loss=-4.794571\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=274, batch=10 train loss <loss>=-4.898507690429687\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] Epoch[274] Batch [10]#011Speed: 167.46 samples/sec#011loss=-4.898508\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] processed a total of 740 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847833.6620877, \"EndTime\": 1640847838.4526231, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4790.013551712036, \"count\": 1, \"min\": 4790.013551712036, \"max\": 4790.013551712036}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.48172940341883 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] #progress_metric: host=algo-1, completed 68.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=274, train loss <loss>=-4.830398877461751\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:58 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:59 INFO 140511108945280] Epoch[275] Batch[0] avg_epoch_loss=-4.671220\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:03:59 INFO 140511108945280] #quality_metric: host=algo-1, epoch=275, batch=0 train loss <loss>=-4.671219825744629\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:00 INFO 140511108945280] Epoch[275] Batch[5] avg_epoch_loss=-4.800237\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=275, batch=5 train loss <loss>=-4.800236940383911\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:00 INFO 140511108945280] Epoch[275] Batch [5]#011Speed: 169.06 samples/sec#011loss=-4.800237\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] Epoch[275] Batch[10] avg_epoch_loss=-4.693276\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=275, batch=10 train loss <loss>=-4.56492338180542\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] Epoch[275] Batch [10]#011Speed: 155.10 samples/sec#011loss=-4.564923\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] processed a total of 721 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847838.4527795, \"EndTime\": 1640847843.47266, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5019.299268722534, \"count\": 1, \"min\": 5019.299268722534, \"max\": 5019.299268722534}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=143.64175558065938 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=275, train loss <loss>=-4.8617903391520185\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:03 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:04 INFO 140511108945280] Epoch[276] Batch[0] avg_epoch_loss=-4.728245\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:04 INFO 140511108945280] #quality_metric: host=algo-1, epoch=276, batch=0 train loss <loss>=-4.728244781494141\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:06 INFO 140511108945280] Epoch[276] Batch[5] avg_epoch_loss=-4.909004\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=276, batch=5 train loss <loss>=-4.909004370371501\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:06 INFO 140511108945280] Epoch[276] Batch [5]#011Speed: 141.97 samples/sec#011loss=-4.909004\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:08 INFO 140511108945280] Epoch[276] Batch[10] avg_epoch_loss=-4.868760\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=276, batch=10 train loss <loss>=-4.820466423034668\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:08 INFO 140511108945280] Epoch[276] Batch [10]#011Speed: 164.52 samples/sec#011loss=-4.820466\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:09 INFO 140511108945280] processed a total of 778 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847843.4727511, \"EndTime\": 1640847849.084095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5610.764503479004, \"count\": 1, \"min\": 5610.764503479004, \"max\": 5610.764503479004}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:09 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=138.65796524866084 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:09 INFO 140511108945280] #progress_metric: host=algo-1, completed 69.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=276, train loss <loss>=-5.02660608291626\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:09 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:09 INFO 140511108945280] Epoch[277] Batch[0] avg_epoch_loss=-4.827210\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:09 INFO 140511108945280] #quality_metric: host=algo-1, epoch=277, batch=0 train loss <loss>=-4.827209949493408\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:11 INFO 140511108945280] Epoch[277] Batch[5] avg_epoch_loss=-4.876033\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:11 INFO 140511108945280] #quality_metric: host=algo-1, epoch=277, batch=5 train loss <loss>=-4.876032670338948\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:11 INFO 140511108945280] Epoch[277] Batch [5]#011Speed: 163.73 samples/sec#011loss=-4.876033\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:13 INFO 140511108945280] Epoch[277] Batch[10] avg_epoch_loss=-4.917207\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:13 INFO 140511108945280] #quality_metric: host=algo-1, epoch=277, batch=10 train loss <loss>=-4.966615390777588\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:13 INFO 140511108945280] Epoch[277] Batch [10]#011Speed: 157.93 samples/sec#011loss=-4.966615\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:14 INFO 140511108945280] processed a total of 746 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847849.084217, \"EndTime\": 1640847854.0607939, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4975.985050201416, \"count\": 1, \"min\": 4975.985050201416, \"max\": 4975.985050201416}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:14 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=149.91593431173584 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:14 INFO 140511108945280] #progress_metric: host=algo-1, completed 69.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=277, train loss <loss>=-4.740192969640096\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:14 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:14 INFO 140511108945280] Epoch[278] Batch[0] avg_epoch_loss=-4.660515\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:14 INFO 140511108945280] #quality_metric: host=algo-1, epoch=278, batch=0 train loss <loss>=-4.660515308380127\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:16 INFO 140511108945280] Epoch[278] Batch[5] avg_epoch_loss=-4.822649\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:16 INFO 140511108945280] #quality_metric: host=algo-1, epoch=278, batch=5 train loss <loss>=-4.822648843129476\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:16 INFO 140511108945280] Epoch[278] Batch [5]#011Speed: 169.56 samples/sec#011loss=-4.822649\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] Epoch[278] Batch[10] avg_epoch_loss=-4.908337\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=278, batch=10 train loss <loss>=-5.011163139343262\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] Epoch[278] Batch [10]#011Speed: 165.18 samples/sec#011loss=-5.011163\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] processed a total of 756 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847854.060892, \"EndTime\": 1640847858.931547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4870.166540145874, \"count\": 1, \"min\": 4870.166540145874, \"max\": 4870.166540145874}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.226469295142 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] #progress_metric: host=algo-1, completed 69.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] #quality_metric: host=algo-1, epoch=278, train loss <loss>=-4.970738450686137\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:18 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:19 INFO 140511108945280] Epoch[279] Batch[0] avg_epoch_loss=-5.426600\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:19 INFO 140511108945280] #quality_metric: host=algo-1, epoch=279, batch=0 train loss <loss>=-5.426599979400635\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:21 INFO 140511108945280] Epoch[279] Batch[5] avg_epoch_loss=-4.825785\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:21 INFO 140511108945280] #quality_metric: host=algo-1, epoch=279, batch=5 train loss <loss>=-4.825785239537557\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:21 INFO 140511108945280] Epoch[279] Batch [5]#011Speed: 170.68 samples/sec#011loss=-4.825785\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:23 INFO 140511108945280] Epoch[279] Batch[10] avg_epoch_loss=-4.716171\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:23 INFO 140511108945280] #quality_metric: host=algo-1, epoch=279, batch=10 train loss <loss>=-4.584633255004883\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:23 INFO 140511108945280] Epoch[279] Batch [10]#011Speed: 162.93 samples/sec#011loss=-4.584633\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:24 INFO 140511108945280] processed a total of 782 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847858.9316435, \"EndTime\": 1640847864.1264272, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5194.196462631226, \"count\": 1, \"min\": 5194.196462631226, \"max\": 5194.196462631226}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:24 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=150.54867537234907 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:24 INFO 140511108945280] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=279, train loss <loss>=-4.538069413258479\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:24 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:24 INFO 140511108945280] Epoch[280] Batch[0] avg_epoch_loss=-4.887691\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:24 INFO 140511108945280] #quality_metric: host=algo-1, epoch=280, batch=0 train loss <loss>=-4.887691020965576\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:26 INFO 140511108945280] Epoch[280] Batch[5] avg_epoch_loss=-4.585247\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:26 INFO 140511108945280] #quality_metric: host=algo-1, epoch=280, batch=5 train loss <loss>=-4.585247278213501\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:26 INFO 140511108945280] Epoch[280] Batch [5]#011Speed: 169.05 samples/sec#011loss=-4.585247\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] Epoch[280] Batch[10] avg_epoch_loss=-4.749841\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=280, batch=10 train loss <loss>=-4.947353839874268\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] Epoch[280] Batch [10]#011Speed: 166.81 samples/sec#011loss=-4.947354\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] processed a total of 752 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847864.1265235, \"EndTime\": 1640847868.958775, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4831.713676452637, \"count\": 1, \"min\": 4831.713676452637, \"max\": 4831.713676452637}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=155.63477510249484 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] #progress_metric: host=algo-1, completed 70.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] #quality_metric: host=algo-1, epoch=280, train loss <loss>=-4.794337153434753\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:28 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:29 INFO 140511108945280] Epoch[281] Batch[0] avg_epoch_loss=-5.066801\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:29 INFO 140511108945280] #quality_metric: host=algo-1, epoch=281, batch=0 train loss <loss>=-5.066800594329834\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:31 INFO 140511108945280] Epoch[281] Batch[5] avg_epoch_loss=-4.614468\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:31 INFO 140511108945280] #quality_metric: host=algo-1, epoch=281, batch=5 train loss <loss>=-4.614468097686768\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:31 INFO 140511108945280] Epoch[281] Batch [5]#011Speed: 170.26 samples/sec#011loss=-4.614468\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] Epoch[281] Batch[10] avg_epoch_loss=-4.668107\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=281, batch=10 train loss <loss>=-4.732472610473633\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] Epoch[281] Batch [10]#011Speed: 160.73 samples/sec#011loss=-4.732473\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] processed a total of 734 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847868.9588509, \"EndTime\": 1640847873.794855, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4835.385084152222, \"count\": 1, \"min\": 4835.385084152222, \"max\": 4835.385084152222}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=151.79325242608346 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] #progress_metric: host=algo-1, completed 70.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] #quality_metric: host=algo-1, epoch=281, train loss <loss>=-4.781105677286784\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:33 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:34 INFO 140511108945280] Epoch[282] Batch[0] avg_epoch_loss=-3.890297\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:34 INFO 140511108945280] #quality_metric: host=algo-1, epoch=282, batch=0 train loss <loss>=-3.890296697616577\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:36 INFO 140511108945280] Epoch[282] Batch[5] avg_epoch_loss=-4.463982\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:36 INFO 140511108945280] #quality_metric: host=algo-1, epoch=282, batch=5 train loss <loss>=-4.463982462882996\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:36 INFO 140511108945280] Epoch[282] Batch [5]#011Speed: 170.79 samples/sec#011loss=-4.463982\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] Epoch[282] Batch[10] avg_epoch_loss=-4.758868\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=282, batch=10 train loss <loss>=-5.112731647491455\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] Epoch[282] Batch [10]#011Speed: 160.72 samples/sec#011loss=-5.112732\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] processed a total of 748 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847873.7949536, \"EndTime\": 1640847878.6454654, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4849.974632263184, \"count\": 1, \"min\": 4849.974632263184, \"max\": 4849.974632263184}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.22334246398646 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] #progress_metric: host=algo-1, completed 70.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] #quality_metric: host=algo-1, epoch=282, train loss <loss>=-4.6964500943819685\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:38 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:39 INFO 140511108945280] Epoch[283] Batch[0] avg_epoch_loss=-4.333255\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:39 INFO 140511108945280] #quality_metric: host=algo-1, epoch=283, batch=0 train loss <loss>=-4.333254814147949\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:41 INFO 140511108945280] Epoch[283] Batch[5] avg_epoch_loss=-4.777463\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:41 INFO 140511108945280] #quality_metric: host=algo-1, epoch=283, batch=5 train loss <loss>=-4.777462959289551\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:41 INFO 140511108945280] Epoch[283] Batch [5]#011Speed: 172.52 samples/sec#011loss=-4.777463\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] Epoch[283] Batch[10] avg_epoch_loss=-4.884194\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=283, batch=10 train loss <loss>=-5.012270927429199\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] Epoch[283] Batch [10]#011Speed: 154.45 samples/sec#011loss=-5.012271\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] processed a total of 781 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847878.6455615, \"EndTime\": 1640847883.9183552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5272.284984588623, \"count\": 1, \"min\": 5272.284984588623, \"max\": 5272.284984588623}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.12933739649716 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] #quality_metric: host=algo-1, epoch=283, train loss <loss>=-4.989277582902175\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:43 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:44 INFO 140511108945280] Epoch[284] Batch[0] avg_epoch_loss=-5.260285\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:44 INFO 140511108945280] #quality_metric: host=algo-1, epoch=284, batch=0 train loss <loss>=-5.260284900665283\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:46 INFO 140511108945280] Epoch[284] Batch[5] avg_epoch_loss=-5.017100\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:46 INFO 140511108945280] #quality_metric: host=algo-1, epoch=284, batch=5 train loss <loss>=-5.0171003341674805\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:46 INFO 140511108945280] Epoch[284] Batch [5]#011Speed: 171.46 samples/sec#011loss=-5.017100\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] Epoch[284] Batch[10] avg_epoch_loss=-4.957463\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=284, batch=10 train loss <loss>=-4.8858977317810055\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] Epoch[284] Batch [10]#011Speed: 164.95 samples/sec#011loss=-4.885898\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] processed a total of 747 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847883.9184513, \"EndTime\": 1640847888.7063305, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4787.317276000977, \"count\": 1, \"min\": 4787.317276000977, \"max\": 4787.317276000977}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=156.03124145615377 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] #progress_metric: host=algo-1, completed 71.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] #quality_metric: host=algo-1, epoch=284, train loss <loss>=-4.999123652776082\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:48 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:49 INFO 140511108945280] Epoch[285] Batch[0] avg_epoch_loss=-5.447933\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:49 INFO 140511108945280] #quality_metric: host=algo-1, epoch=285, batch=0 train loss <loss>=-5.447933197021484\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:51 INFO 140511108945280] Epoch[285] Batch[5] avg_epoch_loss=-5.032709\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:51 INFO 140511108945280] #quality_metric: host=algo-1, epoch=285, batch=5 train loss <loss>=-5.03270943959554\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:51 INFO 140511108945280] Epoch[285] Batch [5]#011Speed: 172.40 samples/sec#011loss=-5.032709\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] Epoch[285] Batch[10] avg_epoch_loss=-4.907712\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=285, batch=10 train loss <loss>=-4.757714176177979\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] Epoch[285] Batch [10]#011Speed: 163.93 samples/sec#011loss=-4.757714\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] processed a total of 713 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847888.706476, \"EndTime\": 1640847893.5068007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4799.815893173218, \"count\": 1, \"min\": 4799.815893173218, \"max\": 4799.815893173218}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=148.54318805562343 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] #progress_metric: host=algo-1, completed 71.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] #quality_metric: host=algo-1, epoch=285, train loss <loss>=-4.704871515432994\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:53 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:54 INFO 140511108945280] Epoch[286] Batch[0] avg_epoch_loss=-5.033907\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:54 INFO 140511108945280] #quality_metric: host=algo-1, epoch=286, batch=0 train loss <loss>=-5.033906936645508\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:56 INFO 140511108945280] Epoch[286] Batch[5] avg_epoch_loss=-4.856571\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:56 INFO 140511108945280] #quality_metric: host=algo-1, epoch=286, batch=5 train loss <loss>=-4.856571435928345\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:56 INFO 140511108945280] Epoch[286] Batch [5]#011Speed: 168.76 samples/sec#011loss=-4.856571\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:57 INFO 140511108945280] Epoch[286] Batch[10] avg_epoch_loss=-4.906552\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:57 INFO 140511108945280] #quality_metric: host=algo-1, epoch=286, batch=10 train loss <loss>=-4.966529369354248\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:57 INFO 140511108945280] Epoch[286] Batch [10]#011Speed: 165.20 samples/sec#011loss=-4.966529\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:58 INFO 140511108945280] processed a total of 743 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847893.5068967, \"EndTime\": 1640847898.3140836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4806.616544723511, \"count\": 1, \"min\": 4806.616544723511, \"max\": 4806.616544723511}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:58 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=154.57375030516775 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:58 INFO 140511108945280] #progress_metric: host=algo-1, completed 71.75 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=286, train loss <loss>=-5.0090707540512085\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:58 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:58 INFO 140511108945280] Epoch[287] Batch[0] avg_epoch_loss=-4.507805\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:04:58 INFO 140511108945280] #quality_metric: host=algo-1, epoch=287, batch=0 train loss <loss>=-4.507805347442627\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:00 INFO 140511108945280] Epoch[287] Batch[5] avg_epoch_loss=-4.790070\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:00 INFO 140511108945280] #quality_metric: host=algo-1, epoch=287, batch=5 train loss <loss>=-4.790070374806722\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:00 INFO 140511108945280] Epoch[287] Batch [5]#011Speed: 170.75 samples/sec#011loss=-4.790070\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:02 INFO 140511108945280] Epoch[287] Batch[10] avg_epoch_loss=-4.752458\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:02 INFO 140511108945280] #quality_metric: host=algo-1, epoch=287, batch=10 train loss <loss>=-4.707322454452514\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:02 INFO 140511108945280] Epoch[287] Batch [10]#011Speed: 155.56 samples/sec#011loss=-4.707322\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:03 INFO 140511108945280] processed a total of 730 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847898.3141942, \"EndTime\": 1640847903.2799368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4965.239524841309, \"count\": 1, \"min\": 4965.239524841309, \"max\": 4965.239524841309}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:03 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=147.01801015590618 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:03 INFO 140511108945280] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=287, train loss <loss>=-4.889782687028249\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:03 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:03 INFO 140511108945280] Epoch[288] Batch[0] avg_epoch_loss=-4.884202\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:03 INFO 140511108945280] #quality_metric: host=algo-1, epoch=288, batch=0 train loss <loss>=-4.884201526641846\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:06 INFO 140511108945280] Epoch[288] Batch[5] avg_epoch_loss=-4.719654\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:06 INFO 140511108945280] #quality_metric: host=algo-1, epoch=288, batch=5 train loss <loss>=-4.7196541627248125\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:06 INFO 140511108945280] Epoch[288] Batch [5]#011Speed: 149.10 samples/sec#011loss=-4.719654\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] Epoch[288] Batch[10] avg_epoch_loss=-4.868850\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=288, batch=10 train loss <loss>=-5.047885131835938\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] Epoch[288] Batch [10]#011Speed: 165.26 samples/sec#011loss=-5.047885\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] processed a total of 730 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847903.2800364, \"EndTime\": 1640847908.442185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5161.6058349609375, \"count\": 1, \"min\": 5161.6058349609375, \"max\": 5161.6058349609375}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] #throughput_metric: host=algo-1, train throughput=141.42526148164976 records/second\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 72.25 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] #quality_metric: host=algo-1, epoch=288, train loss <loss>=-4.802417556444804\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] Loading parameters from best epoch (248)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847908.442274, \"EndTime\": 1640847908.4937475, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 50.74715614318848, \"count\": 1, \"min\": 50.74715614318848, \"max\": 50.74715614318848}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] stopping training now\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] Final loss: -5.1384009917577105 (occurred at epoch 248)\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] #quality_metric: host=algo-1, train final_loss <loss>=-5.1384009917577105\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.6/site-packages/algorithm/run_worker.py:356: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  \"You are using large values for `context_length` and/or `prediction_length`. \"\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 WARNING 140511108945280] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 WARNING 140511108945280] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:08 INFO 140511108945280] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847908.493827, \"EndTime\": 1640847910.1124656, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 1617.3391342163086, \"count\": 1, \"min\": 1617.3391342163086, \"max\": 1617.3391342163086}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:10 INFO 140511108945280] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847910.1125457, \"EndTime\": 1640847910.5734339, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 2078.348398208618, \"count\": 1, \"min\": 2078.348398208618, \"max\": 2078.348398208618}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:10 INFO 140511108945280] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:10 INFO 140511108945280] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847910.5735269, \"EndTime\": 1640847910.6390073, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 65.43350219726562, \"count\": 1, \"min\": 65.43350219726562, \"max\": 65.43350219726562}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:10 INFO 140511108945280] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:10 INFO 140511108945280] #memory_usage::<batchbuffer> = 18.48388671875 mb\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:10 INFO 140511108945280] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847910.639069, \"EndTime\": 1640847910.6400054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.026702880859375, \"count\": 1, \"min\": 0.026702880859375, \"max\": 0.026702880859375}}}\u001b[0m\n",
      "\n",
      "2021-12-30 07:05:22 Uploading - Uploading generated training model\u001b[34m#metrics {\"StartTime\": 1640847910.64007, \"EndTime\": 1640847917.545518, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 6905.537366867065, \"count\": 1, \"min\": 6905.537366867065, \"max\": 6905.537366867065}}}\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, RMSE): 349.3101382797219\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, mean_absolute_QuantileLoss): 114527.88297991552\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, mean_wQuantileLoss): 0.9380294340937004\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.1]): 0.23577526220085154\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.2]): 0.4413028746461372\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.3]): 0.63235892931976\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.4]): 0.81631774295297\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.5]): 0.9919946405871465\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.6]): 1.1571675313370196\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.7]): 1.304302005997025\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.8]): 1.4248761639363283\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #test_score (algo-1, wQuantileLoss[0.9]): 1.4381697558660655\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #quality_metric: host=algo-1, test RMSE <loss>=349.3101382797219\u001b[0m\n",
      "\u001b[34m[12/30/2021 07:05:17 INFO 140511108945280] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.9380294340937004\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1640847917.5455937, \"EndTime\": 1640847917.6385143, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 6.976604461669922, \"count\": 1, \"min\": 6.976604461669922, \"max\": 6.976604461669922}, \"totaltime\": {\"sum\": 1448404.7615528107, \"count\": 1, \"min\": 1448404.7615528107, \"max\": 1448404.7615528107}}}\u001b[0m\n",
      "\n",
      "2021-12-30 07:05:54 Completed - Training job completed\n",
      "ProfilerReport-1640846283: NoIssuesFound\n",
      "Training seconds: 1519\n",
      "Billable seconds: 1519\n",
      "CPU times: user 3.95 s, sys: 323 ms, total: 4.27 s\n",
      "Wall time: 28min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb0168dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f157d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "924f7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65f968ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-30 01:00:00</th>\n",
       "      <td>-0.014229</td>\n",
       "      <td>-0.005282</td>\n",
       "      <td>0.007760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 02:00:00</th>\n",
       "      <td>-0.010172</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>0.010066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 03:00:00</th>\n",
       "      <td>-0.010866</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.014311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 04:00:00</th>\n",
       "      <td>-0.007570</td>\n",
       "      <td>-0.001272</td>\n",
       "      <td>0.005656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 05:00:00</th>\n",
       "      <td>-0.013403</td>\n",
       "      <td>-0.002611</td>\n",
       "      <td>0.006495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05 20:00:00</th>\n",
       "      <td>-0.014879</td>\n",
       "      <td>-0.005751</td>\n",
       "      <td>0.003340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05 21:00:00</th>\n",
       "      <td>-0.014847</td>\n",
       "      <td>-0.005998</td>\n",
       "      <td>0.001633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05 22:00:00</th>\n",
       "      <td>-0.013741</td>\n",
       "      <td>-0.006907</td>\n",
       "      <td>0.002622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05 23:00:00</th>\n",
       "      <td>-0.016960</td>\n",
       "      <td>-0.007571</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-06 00:00:00</th>\n",
       "      <td>-24.314049</td>\n",
       "      <td>2.299904</td>\n",
       "      <td>16.087177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0.1       0.5        0.9\n",
       "2022-03-30 01:00:00  -0.014229 -0.005282   0.007760\n",
       "2022-03-30 02:00:00  -0.010172 -0.000415   0.010066\n",
       "2022-03-30 03:00:00  -0.010866  0.002125   0.014311\n",
       "2022-03-30 04:00:00  -0.007570 -0.001272   0.005656\n",
       "2022-03-30 05:00:00  -0.013403 -0.002611   0.006495\n",
       "...                        ...       ...        ...\n",
       "2022-04-05 20:00:00  -0.014879 -0.005751   0.003340\n",
       "2022-04-05 21:00:00  -0.014847 -0.005998   0.001633\n",
       "2022-04-05 22:00:00  -0.013741 -0.006907   0.002622\n",
       "2022-04-05 23:00:00  -0.016960 -0.007571   0.000027\n",
       "2022-04-06 00:00:00 -24.314049  2.299904  16.087177\n",
       "\n",
       "[168 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c204f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor,\n",
    "    target_ts,\n",
    "    cat=None,\n",
    "    dynamic_feat=None,\n",
    "    forecast_date=end_training,\n",
    "    show_samples=False,\n",
    "    plot_history=7 * 12,\n",
    "    confidence=80,\n",
    "):\n",
    "    freq = target_ts.index.freq\n",
    "    print(\n",
    "        \"calling served model to generate predictions starting from {}\".format(str(forecast_date))\n",
    "    )\n",
    "    assert confidence > 50 and confidence < 100\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "\n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100,\n",
    "    }\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, \"cat = {}\".format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples:\n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color=\"lightskyblue\", alpha=0.2, label=\"_nolegend_\")\n",
    "\n",
    "    # plot the target\n",
    "    target_section = target_ts[\n",
    "        forecast_date - plot_history * freq : forecast_date + prediction_length * freq\n",
    "    ]\n",
    "    target_section.plot(color=\"black\", label=\"target\")\n",
    "\n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index,\n",
    "        prediction[str(low_quantile)].values,\n",
    "        prediction[str(up_quantile)].values,\n",
    "        color=\"b\",\n",
    "        alpha=0.3,\n",
    "        label=\"{}% confidence interval\".format(confidence),\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label=\"P50\")\n",
    "    ax.legend(loc=2)\n",
    "\n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.date_range(\n",
    "                    start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)\n",
    "                ),\n",
    "                data=f,\n",
    "            )\n",
    "            feat_ts[\n",
    "                forecast_date - plot_history * freq : forecast_date + prediction_length * freq\n",
    "            ].plot(ax=ax, color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f8c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df8e58a4",
   "metadata": {},
   "source": [
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c9a3ca",
   "metadata": {},
   "source": [
    "***************************Second Approach*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c6b6c4",
   "metadata": {},
   "source": [
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da663b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                          datetime64[ns]\n",
       "year                                  object\n",
       "quarter                               object\n",
       "month                                 object\n",
       "week                                  object\n",
       "day                                   object\n",
       "hour                                   int64\n",
       "origin                                object\n",
       "destination                           object\n",
       "flight                                object\n",
       "capacity                               int64\n",
       "price_type                            object\n",
       "promotion                             object\n",
       "roundtrip_or_oneway                   object\n",
       "customer_type                         object\n",
       "product_type                          object\n",
       "location_lifestyle                    object\n",
       "location_economical_status            object\n",
       "location_employment_status            object\n",
       "location_event                        object\n",
       "source_wind                           object\n",
       "source_humidity                       object\n",
       "source_precipitation                  object\n",
       "destination_wind                      object\n",
       "destination_humidity                  object\n",
       "destination_precipitation             object\n",
       "number_of_booking                    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data[['date', 'year',\n",
    "       'quarter', 'month', 'week', 'day', 'hour', 'origin',\n",
    "       'destination', 'flight', 'capacity', 'price_type', 'promotion',\n",
    "       'roundtrip_or_oneway', 'customer_type', 'product_type',\n",
    "       'location_lifestyle', 'location_economical_status',\n",
    "       'location_employment_status', 'location_event', 'source_wind',\n",
    "       'source_humidity', 'source_precipitation', 'destination_wind',\n",
    "       'destination_humidity', 'destination_precipitation',\n",
    "       'number_of_booking']]\n",
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4028da1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>flight</th>\n",
       "      <th>capacity</th>\n",
       "      <th>...</th>\n",
       "      <th>location_economical_status</th>\n",
       "      <th>location_employment_status</th>\n",
       "      <th>location_event</th>\n",
       "      <th>source_wind</th>\n",
       "      <th>source_humidity</th>\n",
       "      <th>source_precipitation</th>\n",
       "      <th>destination_wind</th>\n",
       "      <th>destination_humidity</th>\n",
       "      <th>destination_precipitation</th>\n",
       "      <th>number_of_booking</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-03 06:00:00</th>\n",
       "      <td>2021</td>\n",
       "      <td>Q1</td>\n",
       "      <td>JAN</td>\n",
       "      <td>1</td>\n",
       "      <td>SUNDAY</td>\n",
       "      <td>6</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-26 12:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-28 18:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD101</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-28 18:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>24</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-30 00:00:00</th>\n",
       "      <td>2022</td>\n",
       "      <td>Q1</td>\n",
       "      <td>MAR</td>\n",
       "      <td>4</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>24</td>\n",
       "      <td>MAA</td>\n",
       "      <td>DXB</td>\n",
       "      <td>FD102</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>EMPLOYED</td>\n",
       "      <td>CHRISTMAS</td>\n",
       "      <td>0.45 MPS</td>\n",
       "      <td>30%</td>\n",
       "      <td>RAIN</td>\n",
       "      <td>0.52 MPS</td>\n",
       "      <td>20%</td>\n",
       "      <td>DRIZZLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2404 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     year quarter month week       day  hour origin  \\\n",
       "date                                                                  \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "2021-01-03 06:00:00  2021      Q1   JAN    1    SUNDAY     6    MAA   \n",
       "...                   ...     ...   ...  ...       ...   ...    ...   \n",
       "2022-03-26 12:00:00  2022      Q1   MAR    4   TUESDAY    12    MAA   \n",
       "2022-03-28 18:00:00  2022      Q1   MAR    4  THURSDAY    18    MAA   \n",
       "2022-03-28 18:00:00  2022      Q1   MAR    4  THURSDAY    18    MAA   \n",
       "2022-03-30 00:00:00  2022      Q1   MAR    4    FRIDAY    24    MAA   \n",
       "2022-03-30 00:00:00  2022      Q1   MAR    4    FRIDAY    24    MAA   \n",
       "\n",
       "                    destination flight  capacity  ...  \\\n",
       "date                                              ...   \n",
       "2021-01-03 06:00:00         DXB  FD101       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD101       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD102       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD102       467  ...   \n",
       "2021-01-03 06:00:00         DXB  FD101       467  ...   \n",
       "...                         ...    ...       ...  ...   \n",
       "2022-03-26 12:00:00         DXB  FD102       467  ...   \n",
       "2022-03-28 18:00:00         DXB  FD101       467  ...   \n",
       "2022-03-28 18:00:00         DXB  FD102       467  ...   \n",
       "2022-03-30 00:00:00         DXB  FD102       467  ...   \n",
       "2022-03-30 00:00:00         DXB  FD102       467  ...   \n",
       "\n",
       "                    location_economical_status location_employment_status  \\\n",
       "date                                                                        \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "2021-01-03 06:00:00                       HIGH                   EMPLOYED   \n",
       "...                                        ...                        ...   \n",
       "2022-03-26 12:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-28 18:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-28 18:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-30 00:00:00                       HIGH                   EMPLOYED   \n",
       "2022-03-30 00:00:00                       HIGH                   EMPLOYED   \n",
       "\n",
       "                    location_event source_wind source_humidity  \\\n",
       "date                                                             \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2021-01-03 06:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "...                            ...         ...             ...   \n",
       "2022-03-26 12:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-28 18:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-28 18:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-30 00:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "2022-03-30 00:00:00      CHRISTMAS    0.45 MPS             30%   \n",
       "\n",
       "                    source_precipitation destination_wind  \\\n",
       "date                                                        \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "2021-01-03 06:00:00                 RAIN         0.52 MPS   \n",
       "...                                  ...              ...   \n",
       "2022-03-26 12:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-28 18:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-28 18:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-30 00:00:00                 RAIN         0.52 MPS   \n",
       "2022-03-30 00:00:00                 RAIN         0.52 MPS   \n",
       "\n",
       "                    destination_humidity destination_precipitation  \\\n",
       "date                                                                 \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "2021-01-03 06:00:00                  20%                   DRIZZLE   \n",
       "...                                  ...                       ...   \n",
       "2022-03-26 12:00:00                  20%                   DRIZZLE   \n",
       "2022-03-28 18:00:00                  20%                   DRIZZLE   \n",
       "2022-03-28 18:00:00                  20%                   DRIZZLE   \n",
       "2022-03-30 00:00:00                  20%                   DRIZZLE   \n",
       "2022-03-30 00:00:00                  20%                   DRIZZLE   \n",
       "\n",
       "                    number_of_booking  \n",
       "date                                   \n",
       "2021-01-03 06:00:00              10.0  \n",
       "2021-01-03 06:00:00               5.0  \n",
       "2021-01-03 06:00:00              10.0  \n",
       "2021-01-03 06:00:00              20.0  \n",
       "2021-01-03 06:00:00              20.0  \n",
       "...                               ...  \n",
       "2022-03-26 12:00:00               NaN  \n",
       "2022-03-28 18:00:00               NaN  \n",
       "2022-03-28 18:00:00               NaN  \n",
       "2022-03-30 00:00:00               NaN  \n",
       "2022-03-30 00:00:00               NaN  \n",
       "\n",
       "[2404 rows x 26 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.set_index('date', inplace=True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e82e47f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#!pip install jsonlines\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "#series = pd.read_csv('test.csv', parse_dates=[0], index_col=0)\n",
    "#series.sort_index(inplace=True)\n",
    "\n",
    "target_column = 'number_of_booking'\n",
    "group_column = 'quarter'\n",
    "\n",
    "for col in data1.columns:\n",
    "    if col !=target_column:\n",
    "        data1[col] = le.fit_transform(data1[col])\n",
    "\n",
    "if data1[group_column].nunique()==1:\n",
    "    a = [data1]\n",
    "else:\n",
    "    a = [v for k, v in data1.groupby(group_column)]\n",
    "\n",
    "out = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    dynamic_feat = []\n",
    "    cat = []\n",
    "    for col in a[0].columns:\n",
    "        if col == target_column:\n",
    "            target = a[0][col].values.tolist()\n",
    "            start = str(a[0].index[0])\n",
    "\n",
    "        else:\n",
    "            if a[0][col].nunique()>=2: #if 2 or more values, add as dynamic feature\n",
    "                dynamic_feat.append(a[0][col].values.astype(float).tolist())\n",
    "            elif a[0][col].nunique()==1: #if 1 value, add as category\n",
    "                cat.append(int(a[0][col][0]))\n",
    "    out.append({'start':start, 'target':target, 'cat':cat, 'dynamic_feat':dynamic_feat})\n",
    "    \n",
    "with jsonlines.open('train-data.jsonl', mode='w') as writer:\n",
    "    writer.write_all(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80668d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dbca4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0edb235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = \"deepar-demo-notebook1\"  # prefix used for all data stored within the bucket\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "%%time\n",
    "copy_to_s3(\"train-data.jsonl\", s3_data_path + \"/train/train-data.jsonl\")\n",
    "#copy_to_s3(\"test.json\", s3_data_path + \"/test/test-data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c15548b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "region = session.boto_region_name\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=session,\n",
    "    image_uri=sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\"),\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-poc',\n",
    "    output_path=s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "645ec9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": '1H',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": '10',\n",
    "    \"prediction_length\": '1'\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d06f5d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateTrainingJob operation: No S3 objects found under S3 URL \"s3://sagemaker-us-east-2-363247502029/deepar-demo-notebook1/data/train/\" given in input data source. Please ensure that the bucket exists in the selected region (us-east-2), that objects exist under that S3 prefix, and that the role \"arn:aws:iam::363247502029:role/Sagemaker-Full-Access\" has \"s3:ListBucket\" permissions on bucket \"sagemaker-us-east-2-363247502029\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \"\"\"\n\u001b[1;32m   1470\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateTrainingJob operation: No S3 objects found under S3 URL \"s3://sagemaker-us-east-2-363247502029/deepar-demo-notebook1/data/train/\" given in input data source. Please ensure that the bucket exists in the selected region (us-east-2), that objects exist under that S3 prefix, and that the role \"arn:aws:iam::363247502029:role/Sagemaker-Full-Access\" has \"s3:ListBucket\" permissions on bucket \"sagemaker-us-east-2-363247502029\"."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train-data\": \"{}/train/\".format(s3_data_path), \"test-data\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ba6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16b53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031bdc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
